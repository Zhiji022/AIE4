{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lElF3o5PR6ys"
      },
      "source": [
        "# Your First RAG Application\n",
        "\n",
        "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
        "\n",
        "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
        "\n",
        "> NOTE: This was done with Python 3.11.4.\n",
        "\n",
        "> NOTE: There might be [compatibility issues](https://github.com/wandb/wandb/issues/7683) if you're on NVIDIA driver >552.44 As an interim solution - you can rollback your drivers to the 552.44."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CtcL8P8R6yt"
      },
      "source": [
        "## Table of Contents:\n",
        "\n",
        "- Task 1: Imports and Utilities\n",
        "- Task 2: Documents\n",
        "- Task 3: Embeddings and Vectors\n",
        "- Task 4: Prompts\n",
        "- Task 5: Retrieval Augmented Generation\n",
        "  - üöß Activity #1: Augment RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dz6GYilR6yt"
      },
      "source": [
        "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
        "\n",
        "<img src=\"https://i.imgur.com/vD8b016.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjmC0KFtR6yt"
      },
      "source": [
        "## Task 1: Imports and Utility\n",
        "\n",
        "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7VEzqziR6yt",
        "outputId": "f873dd3b-55a0-4e00-ecf4-e2a0fe3af327"
      },
      "outputs": [],
      "source": [
        "!pip install -qU numpy=='1.26.4' matplotlib plotly pandas scipy scikit-learn openai python-dotenv\n",
        "!pip install -qU langchain_community\n",
        "!pip install -qU pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Z1dyrG4hR6yt"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "import asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9OrFZRnER6yt"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jGnpQsR6yu"
      },
      "source": [
        "## Task 2: Documents\n",
        "\n",
        "We'll be concerning ourselves with this part of the flow in the following section:\n",
        "\n",
        "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFPWvRUR6yu"
      },
      "source": [
        "### Loading Source Documents\n",
        "\n",
        "So, first things first, we need some documents to work with.\n",
        "\n",
        "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
        "\n",
        "In this case, we're going to parse our text file into a single document in memory.\n",
        "\n",
        "Let's look at the relevant bits of the `TextFileLoader` class:\n",
        "\n",
        "```python\n",
        "def load_file(self):\n",
        "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
        "            self.documents.append(f.read())\n",
        "```\n",
        "\n",
        "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.pdf import PyMuPDFLoader\n",
        "\n",
        "documents = PyMuPDFLoader(\"https://arxiv.org/pdf/2205.13147\").load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2sUEuGR6yu",
        "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# text_loader = TextFileLoader(\"data/PMarcaBlogs.txt\")\n",
        "# documents = text_loader.load_documents()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Matryoshka Representation Learning\\nAditya Kusupati‚àó‚Ä†‚ãÑ, Gantavya Bhatt‚àó‚Ä†, Aniket Rege‚àó‚Ä†,\\nMatthew Wallingford‚Ä†, Aditya Sinha‚ãÑ, Vivek Ramanujan‚Ä†, William Howard-Snyder‚Ä†,\\nKaifeng Chen‚ãÑ, Sham Kakade‚Ä°, Prateek Jain‚ãÑand Ali Farhadi‚Ä†\\n‚Ä†University of Washington, ‚ãÑGoogle Research, ‚Ä°Harvard University\\n{kusupati,ali}@cs.washington.edu, prajain@google.com\\nAbstract\\nLearned representations are a central component in modern ML systems, serv-\\ning a multitude of downstream tasks. When training such representations, it\\nis often the case that computational and statistical constraints for each down-\\nstream task are unknown. In this context, rigid fixed-capacity representations\\ncan be either over or under-accommodating to the task at hand. This leads us\\nto ask: can we design a flexible representation that can adapt to multiple down-\\nstream tasks with varying computational resources? Our main contribution is\\nMatryoshka Representation Learning (MRL) which encodes information at\\ndifferent granularities and allows a single embedding to adapt to the computational\\nconstraints of downstream tasks. MRL minimally modifies existing representation\\nlearning pipelines and imposes no additional cost during inference and deployment.\\nMRL learns coarse-to-fine representations that are at least as accurate and rich as\\nindependently trained low-dimensional representations. The flexibility within the\\nlearned Matryoshka Representations offer: (a) up to 14√ó smaller embedding\\nsize for ImageNet-1K classification at the same level of accuracy; (b) up to 14√ó\\nreal-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up\\nto 2% accuracy improvements for long-tail few-shot classification, all while being\\nas robust as the original representations. Finally, we show that MRL extends seam-\\nlessly to web-scale datasets (ImageNet, JFT) across various modalities ‚Äì vision\\n(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\\npretrained models are open-sourced at https://github.com/RAIVNLab/MRL.\\n1\\nIntroduction\\nLearned representations [57] are fundamental building blocks of real-world ML systems [66, 91].\\nTrained once and frozen, d-dimensional representations encode rich information and can be used\\nto perform multiple downstream tasks [4]. The deployment of deep representations has two steps:\\n(1) an expensive yet constant-cost forward pass to compute the representation [29] and (2) utilization\\nof the representation for downstream applications [50, 89]. Compute costs for the latter part of the\\npipeline scale with the embedding dimensionality as well as the data size (N) and label space (L).\\nAt web-scale [15, 85] this utilization cost overshadows the feature computation cost. The rigidity in\\nthese representations forces the use of high-dimensional embedding vectors across multiple tasks\\ndespite the varying resource and accuracy constraints that require flexibility.\\nHuman perception of the natural world has a naturally coarse-to-fine granularity [28, 32]. However,\\nperhaps due to the inductive bias of gradient-based training [84], deep learning models tend to diffuse\\n‚Äúinformation‚Äù across the entire representation vector. The desired elasticity is usually enabled in the\\nexisting flat and fixed representations either through training multiple low-dimensional models [29],\\njointly optimizing sub-networks of varying capacity [9, 100] or post-hoc compression [38, 60]. Each\\nof these techniques struggle to meet the requirements for adaptive large-scale deployment either\\n‚àóEqual contribution ‚Äì AK led the project with extensive support from GB and AR for experimentation.\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\\narXiv:2205.13147v4  [cs.LG]  8 Feb 2024\\n'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV-tj5WFR6yu",
        "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
      },
      "outputs": [],
      "source": [
        "# print(documents[0][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHlTvCzYR6yu"
      },
      "source": [
        "### Splitting Text Into Chunks\n",
        "\n",
        "As we can see, there is one massive document.\n",
        "\n",
        "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
        "\n",
        "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
        "\n",
        "For this toy example, we'll just split blindly on length.\n",
        "\n",
        ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
        ">\n",
        ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
        ">- \"document(s)\" : single (or more) text object(s)\n",
        ">- \"corpus\" : the combination of all of our documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6Voc0jR6yv"
      },
      "source": [
        "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMC4tsEmR6yv",
        "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "373"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# text_splitter = CharacterTextSplitter()\n",
        "# split_documents = text_splitter.split_texts(documents)\n",
        "# len(split_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents = [d.page_content for d in documents]\n",
        "len(split_documents) # 35"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2wKT0WLR6yv"
      },
      "source": [
        "Let's take a look at some of the documents we've managed to split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcYMwWJoR6yv",
        "outputId": "20d69876-feca-4826-b4be-32915276987a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Matryoshka Representation Learning\\nAditya Kusupati‚àó‚Ä†‚ãÑ, Gantavya Bhatt‚àó‚Ä†, Aniket Rege‚àó‚Ä†,\\nMatthew Wallingford‚Ä†, Aditya Sinha‚ãÑ, Vivek Ramanujan‚Ä†, William Howard-Snyder‚Ä†,\\nKaifeng Chen‚ãÑ, Sham Kakade‚Ä°, Prateek Jain‚ãÑand Ali Farhadi‚Ä†\\n‚Ä†University of Washington, ‚ãÑGoogle Research, ‚Ä°Harvard University\\n{kusupati,ali}@cs.washington.edu, prajain@google.com\\nAbstract\\nLearned representations are a central component in modern ML systems, serv-\\ning a multitude of downstream tasks. When training such representations, it\\nis often the case that computational and statistical constraints for each down-\\nstream task are unknown. In this context, rigid fixed-capacity representations\\ncan be either over or under-accommodating to the task at hand. This leads us\\nto ask: can we design a flexible representation that can adapt to multiple down-\\nstream tasks with varying computational resources? Our main contribution is\\nMatryoshka Representation Learning (MRL) which encodes information at\\ndifferent granularities and allows a single embedding to adapt to the computational\\nconstraints of downstream tasks. MRL minimally modifies existing representation\\nlearning pipelines and imposes no additional cost during inference and deployment.\\nMRL learns coarse-to-fine representations that are at least as accurate and rich as\\nindependently trained low-dimensional representations. The flexibility within the\\nlearned Matryoshka Representations offer: (a) up to 14√ó smaller embedding\\nsize for ImageNet-1K classification at the same level of accuracy; (b) up to 14√ó\\nreal-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up\\nto 2% accuracy improvements for long-tail few-shot classification, all while being\\nas robust as the original representations. Finally, we show that MRL extends seam-\\nlessly to web-scale datasets (ImageNet, JFT) across various modalities ‚Äì vision\\n(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\\npretrained models are open-sourced at https://github.com/RAIVNLab/MRL.\\n1\\nIntroduction\\nLearned representations [57] are fundamental building blocks of real-world ML systems [66, 91].\\nTrained once and frozen, d-dimensional representations encode rich information and can be used\\nto perform multiple downstream tasks [4]. The deployment of deep representations has two steps:\\n(1) an expensive yet constant-cost forward pass to compute the representation [29] and (2) utilization\\nof the representation for downstream applications [50, 89]. Compute costs for the latter part of the\\npipeline scale with the embedding dimensionality as well as the data size (N) and label space (L).\\nAt web-scale [15, 85] this utilization cost overshadows the feature computation cost. The rigidity in\\nthese representations forces the use of high-dimensional embedding vectors across multiple tasks\\ndespite the varying resource and accuracy constraints that require flexibility.\\nHuman perception of the natural world has a naturally coarse-to-fine granularity [28, 32]. However,\\nperhaps due to the inductive bias of gradient-based training [84], deep learning models tend to diffuse\\n‚Äúinformation‚Äù across the entire representation vector. The desired elasticity is usually enabled in the\\nexisting flat and fixed representations either through training multiple low-dimensional models [29],\\njointly optimizing sub-networks of varying capacity [9, 100] or post-hoc compression [38, 60]. Each\\nof these techniques struggle to meet the requirements for adaptive large-scale deployment either\\n‚àóEqual contribution ‚Äì AK led the project with extensive support from GB and AR for experimentation.\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\\narXiv:2205.13147v4  [cs.LG]  8 Feb 2024\\n']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents[0:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU-RFP_R6yv"
      },
      "source": [
        "## Task 3: Embeddings and Vectors\n",
        "\n",
        "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
        "\n",
        "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI API Key\n",
        "\n",
        "In order to access OpenAI's APIs, we'll need to provide our OpenAI API Key!\n",
        "\n",
        "You can work through the folder \"OpenAI API Key Setup\" for more information on this process if you don't already have an API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Database\n",
        "\n",
        "Let's set up our vector database to hold all our documents and their embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQrfAR1R6yv"
      },
      "source": [
        "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
        "\n",
        "Let's look at our `VectorDatabase().__init__()`:\n",
        "\n",
        "```python\n",
        "def __init__(self, embedding_model: EmbeddingModel = None):\n",
        "        self.vectors = defaultdict(np.array)\n",
        "        self.embedding_model = embedding_model or EmbeddingModel()\n",
        "```\n",
        "\n",
        "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
        "\n",
        "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
        "\n",
        "> **Quick Info About `text-embedding-3-small`**:\n",
        "> - It has a context window of **8191** tokens\n",
        "> - It returns vectors with dimension **1536**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L273pRdeR6yv"
      },
      "source": [
        "#### ‚ùìQuestion #1:\n",
        "\n",
        "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
        "\n",
        "1. Is there any way to modify this dimension? There is an optional parameter 'dimensions' when calling create where we can customize the dimension.\n",
        "2. What technique does OpenAI use to achieve this? The applies Matryoshka Representation Learning which allows the developers shorten the embedding size without losing information.\n",
        "\n",
        "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #2!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5FZY7K3R6yv"
      },
      "source": [
        "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
        "\n",
        "```python\n",
        "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
        "        return await aget_embeddings(\n",
        "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSct6X0aR6yv"
      },
      "source": [
        "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
        "\n",
        "```python\n",
        "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
        "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
        "        for text, embedding in zip(list_of_text, embeddings):\n",
        "            self.insert(text, np.array(embedding))\n",
        "        return self\n",
        "```\n",
        "\n",
        "And that's all we need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "O4KoLbVDR6yv"
      },
      "outputs": [],
      "source": [
        "vector_db = VectorDatabase()\n",
        "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZwaGvpR6yv"
      },
      "source": [
        "#### ‚ùìQuestion #2:\n",
        "\n",
        "What are the benefits of using an `async` approach to collecting our embeddings? 'async' basically means 'while waiting, do something else. Between sending a request to receiving the embeddings, there is lot of waiting. 'async' can prevent the system from idling.\n",
        "\n",
        "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBdIt-xR6yw"
      },
      "source": [
        "So, to review what we've done so far in natural language:\n",
        "\n",
        "1. We load source documents\n",
        "2. We split those source documents into smaller chunks (documents)\n",
        "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
        "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-vWANZyR6yw"
      },
      "source": [
        "### Semantic Similarity\n",
        "\n",
        "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
        "\n",
        "We're going to use the following process to achieve this in our toy example:\n",
        "\n",
        "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
        "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
        "3. We return a list of the top `k` closest vectors, with their text representations\n",
        "\n",
        "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
        "\n",
        "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
        "\n",
        "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d96uavR6yw",
        "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('due to training/maintenance overhead, numerous expensive forward passes through all of the data,\\nstorage and memory cost for multiple copies of encoded data, expensive on-the-fly feature selection\\nor a significant drop in accuracy. By encoding coarse-to-fine-grained representations, which are as\\naccurate as the independently trained counterparts, we learn with minimal overhead a representation\\nthat can be deployed adaptively at no additional cost during inference.\\nWe introduce\\nMatryoshka Representation Learning (MRL) to induce flexibility in the learned\\nrepresentation. MRL learns representations of varying capacities within the same high-dimensional\\nvector through explicit optimization of O(log(d)) lower-dimensional vectors in a nested fashion,\\nhence the name Matryoshka. MRL can be adapted to any existing representation pipeline and\\nis easily extended to many standard tasks in computer vision and natural language processing.\\nFigure 1 illustrates the core idea of Matryoshka Representation Learning (MRL) and the adaptive\\ndeployment settings of the learned Matryoshka Representations.\\nAdaptive Retrieval\\nShortlisting\\nRe-ranking\\nAdaptive Classification\\nTraining\\nInference\\n<latexit sha1_base64=\"eh9hk+peBkdsPY6v+r4rONmxYLY=\">A\\nB7nicbVBNSwMxEJ2tX7V+VT16CRbBU9kVoR6LXjxWsB/QLiWbZtvQbBKSrFCW/gvHhTx6u/x5r8x2+5BWx8MPN6bYWZepDgz1ve/vdLG5tb2T\\nnm3srd/cHhUPT7pGJlqQtEcql7ETaUM0HblOe0pTnEScdqPpXe53n6g2TIpHO1M0TPBYsJgRbJ3UHUjFU1MZVmt+3V8ArZOgIDUo0BpWvwY\\njSdKECks4NqYf+MqGdaWEU7nlUFqMJkise076jACTVhtjh3ji6cMkKx1K6ERQv190SGE2NmSeQ6E2wnZtXLxf+8fmrjmzBjQqWCrJcFKcW\\nYny39GIaUosnzmCiWbuVkQmWGNiXUJ5CMHqy+ukc1UP/HrwcF1r3hZxlOEMzuESAmhAE+6hBW0gMIVneIU3T3kv3rv3sWwtecXMKfyB9/kDBrK\\nPWQ=</latexit>\\n<latexit sha1_base64=\"szvlzTDYQEH5M/GSBtp0D9XaTiU=\">AB/HicbVDLSsNAFL2pr1pf0S7dDBahbkoiguKq6MaFiwr2AW0Ik8m0HTp5MDMRYqi/4saFIm79EHf+jZM2C20\\n9MHA4517umePFnElWd9GaWV1bX2jvFnZ2t7Z3TP3DzoySgShbRLxSPQ8LClnIW0rpjtxYLiwO0602uc7/7QIVkUXiv0pg6AR6FbMgIVlpyzeogwGpMEe39Uc3sy/96Ylr1qyGNQNaJnZBalCg5ZpfAz8iSUBDRTiWsm9bsXIyLBQjnE4rg0TSGJMJHtG+piEOqHSyWfgpOtaKj4aR0C9UaKb+3shwIGUaeHoyjyoXvVz8z+snanjhZCyME0VDMj80TDhSEcqbQD4TlCie\\naoKJYDorImMsMFG6r4ouwV78jLpnDZsq2HfndWaV0UdZTiEI6iDefQhBtoQRsIpPAMr/BmPBkvxrvxMR8tGcVOFf7A+PwBd6WT/A=</latexit>\\n<latexit sha1_base64=\"EDzxYGdFHE0OT/8r1yzvduiKkY=\">AB/nicbVDLSsNAFL2pr1pfUXHlZrAIdVOTIiuim5cuKhgH9CGMJlM26GTBzMToYaCv+LGhSJu/Q53/o2TNgtPTBwOde7pnjxZ\\nxJZVnfRmFpeWV1rbhe2tjc2t4xd/daMkoEoU0S8Uh0PCwpZyFtKqY47cSC4sDjtO2NrjO/UCFZF4r8YxdQI8CFmfEay05JoHvQCrIcEc3VYe3dS+9E9rkxPXLFtVawq0SOyclCFHwzW/en5EkoCGinAsZde2YuWkWChGOJ2UeomkMSYjPKBdTUMcUOmk0/gTdKwVH/UjoV+o0FT9vZHiQMpx4OnJLKyc9zLxP6+bqP6Fk7IwThQNyexQP+FIRSjrAvlMUKL4WBNMBNZERligYnSjZV0Cfb8lxdJq1a1rap9d1auX\\n+V1FOEQjqACNpxDHW6gAU0gkMIzvMKb8WS8GO/Gx2y0YOQ7+/AHxucPYdOUcQ=</latexit>\\n<latexit sha1_base64=\"GlgPMD8z4lB+hiIvM4R0NcwGwo=\">AB/nicbVDLSsNAFL2pr1pfUXHlZrAIdVMTKSiuim5cuKhgH9CGMJlM26GTBzMToYaCv+LGhSJu/Q53/o2TNgtPTBwOde7pnjxZ\\nxJZVnfRmFpeWV1rbhe2tjc2t4xd/daMkoEoU0S8Uh0PCwpZyFtKqY47cSC4sDjtO2NrjO/UCFZF4r8YxdQI8CFmfEay05JoHvQCrIcEc3VYe3dS+9E9rkxPXLFtVawq0SOyclCFHwzW/en5EkoCGinAsZde2YuWkWChGOJ2UeomkMSYjPKBdTUMcUOmk0/gTdKwVH/UjoV+o0FT9vZHiQMpx4OnJLKyc9zLxP6+bqP6Fk7IwThQNyexQP+FIRSjrAvlMUKL4WBNMBNZERligYnSjZV0Cfb8lxdJ6xqW1X7rlauX\\n+V1FOEQjqACNpxDHW6gAU0gkMIzvMKb8WS8GO/Gx2y0YOQ7+/AHxucPZN+Ucw=</latexit>\\n<latexit sha1_base64=\"tEtInXKd9mqmi/oFctu/VjSe+v0=\">AB/nicbVDLSsNAFL2pr1pfUXHlZrAIdVMTESyuim5cuKhgH9CGMJlM26GTBzMToYaCv+LGhSJu/Q53/o2TNgtPTBwOde7pnjxZ\\nxJZVnfRmFpeWV1rbhe2tjc2t4xd/daMkoEoU0S8Uh0PCwpZyFtKqY47cSC4sDjtO2NrjO/UCFZF4r8YxdQI8CFmfEay05JoHvQCrIcEc3VYe3dS+9E9rkxPXLFtVawq0SOyclCFHwzW/en5EkoCGinAsZde2YuWkWChGOJ2UeomkMSYjPKBdTUMcUOmk0/gTdKwVH/UjoV+o0FT9vZHiQMpx4OnJLKyc9zLxP6+bqH7NSVkYJ4qGZHaon3CkIpR1gXwmKF8rAkmgumsiAyxwETpxkq6BHv+y4ukdVa1rap9d16uX\\n+V1FOEQjqACNlxAHW6gAU0gkMIzvMKb8WS8GO/Gx2y0YOQ7+/AHxucPaveUdw=</latexit>\\n<latexit sha1_base64=\"dyCsZ/ny7rQzKcXztjElUtg2QPg=\">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qZORFRcFd24cFHBPqAdhkwmbUMzmSHJCHXswl9x40IRt/6GO/GTDsLrR4IHM65l3ty/J\\ngzpR3nyrMzS8sLhWXSyura+sb9uZWU0WJLRBIh7Jto8V5UzQhma03YsKQ59Tlv+8DLzW3dUKhaJWz2KqRvivmA9RrA2kmfvdEOsBwRzeF2591J0Hhyik/GBZ5edqjMB/EtQTsogR92zP7tBRJKQCk04VqDnFi7KZaEU7HpW6iaIzJEPdpx1CBQ6rcdJ/DPeNEsBeJM0TGk7UnxspDpUahb6ZzNKqWS8T/M6ie6duSkTcaKpINDvYRDHcGsDBgwSYnmI0MwkcxkhWSAJSbaVFYyJaDZL/8lzaMqcqro5rhcu\\n8jrKIJdsAcqAIFTUANXoA4agIAH8ARewKv1aD1b9b7dLRg5Tvb4Besj2/eCZSw</latexit>\\n<latexit sha1_base64=\"OPHM4ACsGr0VI7qMpDgoN+t2ICI=\">AB9XicbVDLSgMx\\nFL3xWeur6tJNsAh1U2ZE0GXRjQsXFewD2rFk0kwbmskMSUapQ/DjQtF3Pov7vwbM+0stPVA4HDOvdyT48eCa+M432hpeWV1b2wUdzc2t7ZLe3tN3WUKMoaNBKRavtEM8Elaxh\\nuBGvHipHQF6zlj64yv/XAlOaRvDPjmHkhGUgecEqMle67ITHDQJERvqk8nfRKZafqTIEXiZuTMuSo90pf3X5Ek5BJQwXRuM6sfFSogyngk2K3USzmNARGbCOpZKETHvpNPUEH1\\nulj4NI2ScNnq/N1ISaj0OfTuZpdTzXib+53USE1x4KZdxYpiks0NBIrCJcFYB7nPFqBFjSwhV3GbFdEgUocYWVbQluPNfXiTN06rVN3bs3LtMq+jAIdwBVw4RxqcA1aAFB\\nc/wCm/oEb2gd/QxG1C+c4B/AH6/AGZEJHn</latexit>\\nFigure\\n1:\\nMatryoshka Representation Learning\\nis\\nadaptable to any representation learning setup and begets\\na Matryoshka Representation z\\nby optimizing the orig-\\ninal\\nloss\\nL(.)\\nat\\nO(log(d))\\nchosen\\nrepresentation\\nsizes.\\nMatryoshka Representation can be utilized effectively for adap-\\ntive deployment across environments and downstream tasks.\\nThe first m-dimensions, m ‚àà[d], of\\nthe Matryoshka Representation is\\nan information-rich low-dimensional\\nvector, at no additional training cost,\\nthat is as accurate as an indepen-\\ndently trained m-dimensional repre-\\nsentation.\\nThe information within\\nthe Matryoshka Representation in-\\ncreases with the dimensionality creat-\\ning a coarse-to-fine grained represen-\\ntation, all without significant training\\nor additional deployment overhead.\\nMRL equips the representation vector\\nwith the desired flexibility and multi-\\nfidelity that can ensure a near-optimal\\naccuracy-vs-compute trade-off. With\\nthese advantages, MRL enables adap-\\ntive deployment based on accuracy\\nand compute constraints.\\nThe Matryoshka Representations improve efficiency for large-scale classification and retrieval\\nwithout any significant loss of accuracy. While there are potentially several applications of coarse-to-\\nfine Matryoshka Representations, in this work we focus on two key building blocks of real-world\\nML systems: large-scale classification and retrieval. For classification, we use adaptive cascades with\\nthe variable-size representations from a model trained with MRL, significantly reducing the average\\ndimension of embeddings needed to achieve a particular accuracy. For example, on ImageNet-1K,\\nMRL + adaptive classification results in up to a 14√ó smaller representation size at the same accuracy\\nas baselines (Section 4.2.1). Similarly, we use MRL in an adaptive retrieval system. Given a query,\\nwe shortlist retrieval candidates using the first few dimensions of the query embedding, and then\\nsuccessively use more dimensions to re-rank the retrieved set. A simple implementation of this\\napproach leads to 128√ó theoretical (in terms of FLOPS) and 14√ó wall-clock time speedups compared\\nto a single-shot retrieval system that uses a standard embedding vector; note that MRL‚Äôs retrieval\\naccuracy is comparable to that of single-shot retrieval (Section 4.3.1). Finally, as MRL explicitly\\nlearns coarse-to-fine representation vectors, intuitively it should share more semantic information\\namong its various dimensions (Figure 5). This is reflected in up to 2% accuracy gains in long-tail\\ncontinual learning settings while being as robust as the original embeddings. Furthermore, due to its\\ncoarse-to-fine grained nature, MRL can also be used as method to analyze hardness of classification\\namong instances and information bottlenecks.\\nWe make the following key contributions:\\n1. We introduce\\nMatryoshka Representation Learning (MRL) to obtain flexible representa-\\ntions (Matryoshka Representations) for adaptive deployment (Section 3).\\n2. Up to 14√ó faster yet accurate large-scale classification and retrieval using MRL (Section 4).\\n3. Seamless adaptation of MRL across modalities (vision - ResNet & ViT, vision + language -\\nALIGN, language - BERT) and to web-scale data (ImageNet-1K/4K, JFT-300M and ALIGN data).\\n4. Further analysis of MRL‚Äôs representations in the context of other downstream tasks (Section 5).\\n2\\n',\n",
              "  np.float64(0.5240534689190771)),\n",
              " ('Figure 12: Progression of relative per-class accuracy vs MRL-2048. As the dimensionality increases,\\nthe spread shrinks while the class marked (x) (Madagascar cat) loses accuracy.\\nTable 22: Percentage of ImageNet-1K validation set that is first correctly predicted using each\\nrepresentation size d. We note that 18.46% of the samples cannot be correctly predicted by any\\nrepresentation size. The remaining 81.54% constitutes the oracle accuracy.\\nRep. Size\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nAlways\\nWrong\\nCorrectly\\nPredicted\\n67.46\\n8.78\\n2.58\\n1.35\\n0.64\\n0.31\\n0.20\\n0.12\\n0.06\\n18.46\\nof disagreement arising when the models got confused within the same superclass. For example,\\nImageNet-1K has multiple \"snake\" classes, and models often confuse a snake image for an incorrect\\nspecies of snake.\\nSuperclass Performance\\nWe created a 30 superclass subset of the validation set based on wordnet\\nhierarchy (Table 24) to quantify the performance of MRL model on ImageNet-1K superclasses.\\nTable 25 quantifies the performance with different representation size.\\nK\\nAblation Studies\\nK.1\\nMRL Training Paradigm\\nMatryoshka Representations via Finetuning.\\nTo observe if nesting can be induced in models that\\nwere not explicitly trained with nesting from scratch, we loaded a pretrained FF-2048 ResNet50 model\\nand initialized a new MRL layer, as defined in Algorithm 2, Appendix C. We then unfroze different\\nlayers of the backbone to observe how much non-linearity in the form of unfrozen conv layers needed\\nto be present to enforce nesting into a pretrained FF model. A description of these layers can be found\\nin the ResNet50 architecture [29]. All models were finetuned with the FFCV pipeline, with same\\ntraining configuration as in the end-to-end training aside from changing lr = 0.1 and epochs = 10. We\\nobserved that finetuning the linear layer alone was insufficient to learn Matryoshka Representations\\nat lower dimensionalities. Adding more and more non-linear conv+ReLU layers steadily improved\\nclassification accuracy of d = 8 from 5% to 60% after finetuning, which was only 6% less than\\ntraining MRL end-to-end for 40 epochs. This difference was successively less pronounced as we\\nincreased dimensionality past d = 64, to within 1.5% for all larger dimensionalities. The full results\\nof this ablation can be seen in Table 26.\\nRelative Importance.\\nWe performed an ablation of MRL over the relative importance, cm, of\\ndifferent nesting dimensions m ‚ààM, as defined in Sec. 3. In an attempt to improve performance at\\nlower dimensionalities, we boosted the relative importance cm of training loss at lower dimensions as\\nin Eq. 1 with two models, MRL-8boost and MRL-8+16boost. The MRL-8boost model had cm‚ààM =\\n[2, 1, 1, 1, 1, 1, 1, 1, 1] and the MRL-8+16boost model had cm‚ààM = [2, 1.5, 1, 1, 1, 1, 1, 1, 1]. The\\nrelative importance list cm‚ààM had a 1-to-1 correspondence with nesting dimension set M. In\\nTable 27, we observed that MRL-8boost improves top-1 accuracy by 3% at d = 8, and also improves\\ntop-1 accuracy of all representation scales from 16 to 256 over MRL, while only hurting the\\nperformance at 512 to 2048 representation scales by a maximum of 0.1%. This suggests that the\\nrelative importance cm can be tuned/set for optimal accuracy for all m ‚ààM, but we leave this\\nextension for future work.\\n32\\n',\n",
              "  np.float64(0.4906092788163212)),\n",
              " ('Matryoshka Representation Learning (MRL) enables each of the first m dimensions of the em-\\nbedding vector, z1:m ‚ààRm to be independently capable of being a transferable and general purpose\\nrepresentation of the datapoint x. We obtain z using a deep neural network F( ¬∑ ; Œ∏F ): X ‚ÜíRd\\nparameterized by learnable weights Œ∏F , i.e., z := F(x; Œ∏F ). The multi-granularity is captured through\\nthe set of the chosen dimensions M, that contains less than log(d) elements, i.e., |M| ‚â§‚åälog(d)‚åã.\\nThe usual set M consists of consistent halving until the representation size hits a low information\\nbottleneck. We discuss the design choices in Section 4 for each of the representation learning settings.\\nFor the ease of exposition, we present the formulation for fully supervised representation learning\\nvia multi-class classification. Matryoshka Representation Learning modifies the typical setting\\nto become a multi-scale representation learning problem on the same task. For example, we train\\nResNet50 [29] on ImageNet-1K [76] which embeds a 224 √ó 224 pixel image into a d = 2048\\nrepresentation vector and then passed through a linear classifier to make a prediction, ÀÜ\\ny among the\\nL = 1000 labels. For MRL, we choose M = {8, 16, . . . , 1024, 2048} as the nesting dimensions.\\nSuppose we are given a labelled dataset D = {(x1, y1), . . . , (xN, yN)} where xi ‚ààX is an input\\npoint and yi ‚àà[L] is the label of xi for all i ‚àà[N]. MRL optimizes the multi-class classification loss\\nfor each of the nested dimension m ‚ààM using standard empirical risk minimization using a separate\\nlinear classifier, parameterized by W(m) ‚ààRL√óm. All the losses are aggregated after scaling with\\ntheir relative importance (cm ‚â•0)m‚ààM respectively. That is, we solve\\nmin\\n{W(m)}m‚ààM, Œ∏F\\n1\\nN\\nX\\ni‚àà[N]\\nX\\nm‚ààM\\ncm ¬∑ L\\n\\x10\\nW(m) ¬∑ F(xi; Œ∏F )1:m ; yi\\n\\x11\\n,\\n(1)\\nwhere L: RL √ó [L] ‚ÜíR+ is the multi-class softmax cross-entropy loss function. This is a standard\\noptimization problem that can be solved using sub-gradient descent methods. We set all the impor-\\ntance scales, cm = 1 for all m ‚ààM; see Section 5 for ablations. Lastly, despite only optimizing\\nfor O(log(d)) nested dimensions, MRL results in accurate representations, that interpolate, for\\ndimensions that fall between the chosen granularity of the representations (Section 4.2).\\nWe call this formulation as Matryoshka Representation Learning (MRL). A natural way to make\\nthis efficient is through weight-tying across all the linear classifiers, i.e., by defining W(m) = W1:m\\nfor a set of common weights W ‚ààRL√ód. This would reduce the memory cost due to the linear\\nclassifiers by almost half, which would be crucial in cases of extremely large output spaces [89, 99].\\nThis variant is called Efficient Matryoshka Representation Learning (MRL‚ÄìE). Refer to Alg 1\\nand Alg 2 in Appendix A for the building blocks of Matryoshka Representation Learning (MRL).\\nAdaptation to Learning Frameworks.\\nMRL can be adapted seamlessly to most representation\\nlearning frameworks at web-scale with minimal modifications (Section 4.1). For example, MRL‚Äôs\\nadaptation to masked language modelling reduces to MRL‚ÄìE due to the weight-tying between the\\ninput embedding matrix and the linear classifier. For contrastive learning, both in context of vision &\\nvision + language, MRL is applied to both the embeddings that are being contrasted with each other.\\nThe presence of normalization on the representation needs to be handled independently for each of\\nthe nesting dimension for best results (see Appendix C for more details).\\n4\\nApplications\\nIn this section, we discuss Matryoshka Representation Learning (MRL) for a diverse set of ap-\\nplications along with an extensive evaluation of the learned multifidelity representations. Further,\\nwe showcase the downstream applications of the learned Matryoshka Representations for flexible\\nlarge-scale deployment through (a) Adaptive Classification (AC) and (b) Adaptive Retrieval (AR).\\n4.1\\nRepresentation Learning\\nWe adapt Matryoshka Representation Learning (MRL) to various representation learning setups\\n(a) Supervised learning for vision: ResNet50 [29] on ImageNet-1K [76] and ViT-B/16 [22] on\\nJFT-300M [85], (b) Contrastive learning for vision + language: ALIGN model with ViT-B/16 vision\\nencoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling:\\nBERT [19] on English Wikipedia and BooksCorpus [102]. Please refer to Appendices B and C for\\ndetails regarding the model architectures, datasets and training specifics.\\n4\\n',\n",
              "  np.float64(0.47355925039741026))]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.search_by_text(\"What problem does MRL solve?\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(<function numpy.array>,\n",
              "            {'Matryoshka Representation Learning\\nAditya Kusupati‚àó‚Ä†‚ãÑ, Gantavya Bhatt‚àó‚Ä†, Aniket Rege‚àó‚Ä†,\\nMatthew Wallingford‚Ä†, Aditya Sinha‚ãÑ, Vivek Ramanujan‚Ä†, William Howard-Snyder‚Ä†,\\nKaifeng Chen‚ãÑ, Sham Kakade‚Ä°, Prateek Jain‚ãÑand Ali Farhadi‚Ä†\\n‚Ä†University of Washington, ‚ãÑGoogle Research, ‚Ä°Harvard University\\n{kusupati,ali}@cs.washington.edu, prajain@google.com\\nAbstract\\nLearned representations are a central component in modern ML systems, serv-\\ning a multitude of downstream tasks. When training such representations, it\\nis often the case that computational and statistical constraints for each down-\\nstream task are unknown. In this context, rigid fixed-capacity representations\\ncan be either over or under-accommodating to the task at hand. This leads us\\nto ask: can we design a flexible representation that can adapt to multiple down-\\nstream tasks with varying computational resources? Our main contribution is\\nMatryoshka Representation Learning (MRL) which encodes information at\\ndifferent granularities and allows a single embedding to adapt to the computational\\nconstraints of downstream tasks. MRL minimally modifies existing representation\\nlearning pipelines and imposes no additional cost during inference and deployment.\\nMRL learns coarse-to-fine representations that are at least as accurate and rich as\\nindependently trained low-dimensional representations. The flexibility within the\\nlearned Matryoshka Representations offer: (a) up to 14√ó smaller embedding\\nsize for ImageNet-1K classification at the same level of accuracy; (b) up to 14√ó\\nreal-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up\\nto 2% accuracy improvements for long-tail few-shot classification, all while being\\nas robust as the original representations. Finally, we show that MRL extends seam-\\nlessly to web-scale datasets (ImageNet, JFT) across various modalities ‚Äì vision\\n(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\\npretrained models are open-sourced at https://github.com/RAIVNLab/MRL.\\n1\\nIntroduction\\nLearned representations [57] are fundamental building blocks of real-world ML systems [66, 91].\\nTrained once and frozen, d-dimensional representations encode rich information and can be used\\nto perform multiple downstream tasks [4]. The deployment of deep representations has two steps:\\n(1) an expensive yet constant-cost forward pass to compute the representation [29] and (2) utilization\\nof the representation for downstream applications [50, 89]. Compute costs for the latter part of the\\npipeline scale with the embedding dimensionality as well as the data size (N) and label space (L).\\nAt web-scale [15, 85] this utilization cost overshadows the feature computation cost. The rigidity in\\nthese representations forces the use of high-dimensional embedding vectors across multiple tasks\\ndespite the varying resource and accuracy constraints that require flexibility.\\nHuman perception of the natural world has a naturally coarse-to-fine granularity [28, 32]. However,\\nperhaps due to the inductive bias of gradient-based training [84], deep learning models tend to diffuse\\n‚Äúinformation‚Äù across the entire representation vector. The desired elasticity is usually enabled in the\\nexisting flat and fixed representations either through training multiple low-dimensional models [29],\\njointly optimizing sub-networks of varying capacity [9, 100] or post-hoc compression [38, 60]. Each\\nof these techniques struggle to meet the requirements for adaptive large-scale deployment either\\n‚àóEqual contribution ‚Äì AK led the project with extensive support from GB and AR for experimentation.\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\\narXiv:2205.13147v4  [cs.LG]  8 Feb 2024\\n': array([ 0.03098394,  0.02475571,  0.03311242, ...,  0.01093266,\n",
              "                    -0.01182759,  0.01169456]),\n",
              "             'due to training/maintenance overhead, numerous expensive forward passes through all of the data,\\nstorage and memory cost for multiple copies of encoded data, expensive on-the-fly feature selection\\nor a significant drop in accuracy. By encoding coarse-to-fine-grained representations, which are as\\naccurate as the independently trained counterparts, we learn with minimal overhead a representation\\nthat can be deployed adaptively at no additional cost during inference.\\nWe introduce\\nMatryoshka Representation Learning (MRL) to induce flexibility in the learned\\nrepresentation. MRL learns representations of varying capacities within the same high-dimensional\\nvector through explicit optimization of O(log(d)) lower-dimensional vectors in a nested fashion,\\nhence the name Matryoshka. MRL can be adapted to any existing representation pipeline and\\nis easily extended to many standard tasks in computer vision and natural language processing.\\nFigure 1 illustrates the core idea of Matryoshka Representation Learning (MRL) and the adaptive\\ndeployment settings of the learned Matryoshka Representations.\\nAdaptive Retrieval\\nShortlisting\\nRe-ranking\\nAdaptive Classification\\nTraining\\nInference\\n<latexit sha1_base64=\"eh9hk+peBkdsPY6v+r4rONmxYLY=\">A\\nB7nicbVBNSwMxEJ2tX7V+VT16CRbBU9kVoR6LXjxWsB/QLiWbZtvQbBKSrFCW/gvHhTx6u/x5r8x2+5BWx8MPN6bYWZepDgz1ve/vdLG5tb2T\\nnm3srd/cHhUPT7pGJlqQtEcql7ETaUM0HblOe0pTnEScdqPpXe53n6g2TIpHO1M0TPBYsJgRbJ3UHUjFU1MZVmt+3V8ArZOgIDUo0BpWvwY\\njSdKECks4NqYf+MqGdaWEU7nlUFqMJkise076jACTVhtjh3ji6cMkKx1K6ERQv190SGE2NmSeQ6E2wnZtXLxf+8fmrjmzBjQqWCrJcFKcW\\nYny39GIaUosnzmCiWbuVkQmWGNiXUJ5CMHqy+ukc1UP/HrwcF1r3hZxlOEMzuESAmhAE+6hBW0gMIVneIU3T3kv3rv3sWwtecXMKfyB9/kDBrK\\nPWQ=</latexit>\\n<latexit sha1_base64=\"szvlzTDYQEH5M/GSBtp0D9XaTiU=\">AB/HicbVDLSsNAFL2pr1pf0S7dDBahbkoiguKq6MaFiwr2AW0Ik8m0HTp5MDMRYqi/4saFIm79EHf+jZM2C20\\n9MHA4517umePFnElWd9GaWV1bX2jvFnZ2t7Z3TP3DzoySgShbRLxSPQ8LClnIW0rpjtxYLiwO0602uc7/7QIVkUXiv0pg6AR6FbMgIVlpyzeogwGpMEe39Uc3sy/96Ylr1qyGNQNaJnZBalCg5ZpfAz8iSUBDRTiWsm9bsXIyLBQjnE4rg0TSGJMJHtG+piEOqHSyWfgpOtaKj4aR0C9UaKb+3shwIGUaeHoyjyoXvVz8z+snanjhZCyME0VDMj80TDhSEcqbQD4TlCie\\naoKJYDorImMsMFG6r4ouwV78jLpnDZsq2HfndWaV0UdZTiEI6iDefQhBtoQRsIpPAMr/BmPBkvxrvxMR8tGcVOFf7A+PwBd6WT/A=</latexit>\\n<latexit sha1_base64=\"EDzxYGdFHE0OT/8r1yzvduiKkY=\">AB/nicbVDLSsNAFL2pr1pfUXHlZrAIdVOTIiuim5cuKhgH9CGMJlM26GTBzMToYaCv+LGhSJu/Q53/o2TNgtPTBwOde7pnjxZ\\nxJZVnfRmFpeWV1rbhe2tjc2t4xd/daMkoEoU0S8Uh0PCwpZyFtKqY47cSC4sDjtO2NrjO/UCFZF4r8YxdQI8CFmfEay05JoHvQCrIcEc3VYe3dS+9E9rkxPXLFtVawq0SOyclCFHwzW/en5EkoCGinAsZde2YuWkWChGOJ2UeomkMSYjPKBdTUMcUOmk0/gTdKwVH/UjoV+o0FT9vZHiQMpx4OnJLKyc9zLxP6+bqP6Fk7IwThQNyexQP+FIRSjrAvlMUKL4WBNMBNZERligYnSjZV0Cfb8lxdJq1a1rap9d1auX\\n+V1FOEQjqACNpxDHW6gAU0gkMIzvMKb8WS8GO/Gx2y0YOQ7+/AHxucPYdOUcQ=</latexit>\\n<latexit sha1_base64=\"GlgPMD8z4lB+hiIvM4R0NcwGwo=\">AB/nicbVDLSsNAFL2pr1pfUXHlZrAIdVMTKSiuim5cuKhgH9CGMJlM26GTBzMToYaCv+LGhSJu/Q53/o2TNgtPTBwOde7pnjxZ\\nxJZVnfRmFpeWV1rbhe2tjc2t4xd/daMkoEoU0S8Uh0PCwpZyFtKqY47cSC4sDjtO2NrjO/UCFZF4r8YxdQI8CFmfEay05JoHvQCrIcEc3VYe3dS+9E9rkxPXLFtVawq0SOyclCFHwzW/en5EkoCGinAsZde2YuWkWChGOJ2UeomkMSYjPKBdTUMcUOmk0/gTdKwVH/UjoV+o0FT9vZHiQMpx4OnJLKyc9zLxP6+bqP6Fk7IwThQNyexQP+FIRSjrAvlMUKL4WBNMBNZERligYnSjZV0Cfb8lxdJ6xqW1X7rlauX\\n+V1FOEQjqACNpxDHW6gAU0gkMIzvMKb8WS8GO/Gx2y0YOQ7+/AHxucPZN+Ucw=</latexit>\\n<latexit sha1_base64=\"tEtInXKd9mqmi/oFctu/VjSe+v0=\">AB/nicbVDLSsNAFL2pr1pfUXHlZrAIdVMTESyuim5cuKhgH9CGMJlM26GTBzMToYaCv+LGhSJu/Q53/o2TNgtPTBwOde7pnjxZ\\nxJZVnfRmFpeWV1rbhe2tjc2t4xd/daMkoEoU0S8Uh0PCwpZyFtKqY47cSC4sDjtO2NrjO/UCFZF4r8YxdQI8CFmfEay05JoHvQCrIcEc3VYe3dS+9E9rkxPXLFtVawq0SOyclCFHwzW/en5EkoCGinAsZde2YuWkWChGOJ2UeomkMSYjPKBdTUMcUOmk0/gTdKwVH/UjoV+o0FT9vZHiQMpx4OnJLKyc9zLxP6+bqH7NSVkYJ4qGZHaon3CkIpR1gXwmKF8rAkmgumsiAyxwETpxkq6BHv+y4ukdVa1rap9d16uX\\n+V1FOEQjqACNlxAHW6gAU0gkMIzvMKb8WS8GO/Gx2y0YOQ7+/AHxucPaveUdw=</latexit>\\n<latexit sha1_base64=\"dyCsZ/ny7rQzKcXztjElUtg2QPg=\">AB/3icbVDLSgMxFM3UV62vUcGNm2AR6qZORFRcFd24cFHBPqAdhkwmbUMzmSHJCHXswl9x40IRt/6GO/GTDsLrR4IHM65l3ty/J\\ngzpR3nyrMzS8sLhWXSyura+sb9uZWU0WJLRBIh7Jto8V5UzQhma03YsKQ59Tlv+8DLzW3dUKhaJWz2KqRvivmA9RrA2kmfvdEOsBwRzeF2591J0Hhyik/GBZ5edqjMB/EtQTsogR92zP7tBRJKQCk04VqDnFi7KZaEU7HpW6iaIzJEPdpx1CBQ6rcdJ/DPeNEsBeJM0TGk7UnxspDpUahb6ZzNKqWS8T/M6ie6duSkTcaKpINDvYRDHcGsDBgwSYnmI0MwkcxkhWSAJSbaVFYyJaDZL/8lzaMqcqro5rhcu\\n8jrKIJdsAcqAIFTUANXoA4agIAH8ARewKv1aD1b9b7dLRg5Tvb4Besj2/eCZSw</latexit>\\n<latexit sha1_base64=\"OPHM4ACsGr0VI7qMpDgoN+t2ICI=\">AB9XicbVDLSgMx\\nFL3xWeur6tJNsAh1U2ZE0GXRjQsXFewD2rFk0kwbmskMSUapQ/DjQtF3Pov7vwbM+0stPVA4HDOvdyT48eCa+M432hpeWV1b2wUdzc2t7ZLe3tN3WUKMoaNBKRavtEM8Elaxh\\nuBGvHipHQF6zlj64yv/XAlOaRvDPjmHkhGUgecEqMle67ITHDQJERvqk8nfRKZafqTIEXiZuTMuSo90pf3X5Ek5BJQwXRuM6sfFSogyngk2K3USzmNARGbCOpZKETHvpNPUEH1\\nulj4NI2ScNnq/N1ISaj0OfTuZpdTzXib+53USE1x4KZdxYpiks0NBIrCJcFYB7nPFqBFjSwhV3GbFdEgUocYWVbQluPNfXiTN06rVN3bs3LtMq+jAIdwBVw4RxqcA1aAFB\\nc/wCm/oEb2gd/QxG1C+c4B/AH6/AGZEJHn</latexit>\\nFigure\\n1:\\nMatryoshka Representation Learning\\nis\\nadaptable to any representation learning setup and begets\\na Matryoshka Representation z\\nby optimizing the orig-\\ninal\\nloss\\nL(.)\\nat\\nO(log(d))\\nchosen\\nrepresentation\\nsizes.\\nMatryoshka Representation can be utilized effectively for adap-\\ntive deployment across environments and downstream tasks.\\nThe first m-dimensions, m ‚àà[d], of\\nthe Matryoshka Representation is\\nan information-rich low-dimensional\\nvector, at no additional training cost,\\nthat is as accurate as an indepen-\\ndently trained m-dimensional repre-\\nsentation.\\nThe information within\\nthe Matryoshka Representation in-\\ncreases with the dimensionality creat-\\ning a coarse-to-fine grained represen-\\ntation, all without significant training\\nor additional deployment overhead.\\nMRL equips the representation vector\\nwith the desired flexibility and multi-\\nfidelity that can ensure a near-optimal\\naccuracy-vs-compute trade-off. With\\nthese advantages, MRL enables adap-\\ntive deployment based on accuracy\\nand compute constraints.\\nThe Matryoshka Representations improve efficiency for large-scale classification and retrieval\\nwithout any significant loss of accuracy. While there are potentially several applications of coarse-to-\\nfine Matryoshka Representations, in this work we focus on two key building blocks of real-world\\nML systems: large-scale classification and retrieval. For classification, we use adaptive cascades with\\nthe variable-size representations from a model trained with MRL, significantly reducing the average\\ndimension of embeddings needed to achieve a particular accuracy. For example, on ImageNet-1K,\\nMRL + adaptive classification results in up to a 14√ó smaller representation size at the same accuracy\\nas baselines (Section 4.2.1). Similarly, we use MRL in an adaptive retrieval system. Given a query,\\nwe shortlist retrieval candidates using the first few dimensions of the query embedding, and then\\nsuccessively use more dimensions to re-rank the retrieved set. A simple implementation of this\\napproach leads to 128√ó theoretical (in terms of FLOPS) and 14√ó wall-clock time speedups compared\\nto a single-shot retrieval system that uses a standard embedding vector; note that MRL‚Äôs retrieval\\naccuracy is comparable to that of single-shot retrieval (Section 4.3.1). Finally, as MRL explicitly\\nlearns coarse-to-fine representation vectors, intuitively it should share more semantic information\\namong its various dimensions (Figure 5). This is reflected in up to 2% accuracy gains in long-tail\\ncontinual learning settings while being as robust as the original embeddings. Furthermore, due to its\\ncoarse-to-fine grained nature, MRL can also be used as method to analyze hardness of classification\\namong instances and information bottlenecks.\\nWe make the following key contributions:\\n1. We introduce\\nMatryoshka Representation Learning (MRL) to obtain flexible representa-\\ntions (Matryoshka Representations) for adaptive deployment (Section 3).\\n2. Up to 14√ó faster yet accurate large-scale classification and retrieval using MRL (Section 4).\\n3. Seamless adaptation of MRL across modalities (vision - ResNet & ViT, vision + language -\\nALIGN, language - BERT) and to web-scale data (ImageNet-1K/4K, JFT-300M and ALIGN data).\\n4. Further analysis of MRL‚Äôs representations in the context of other downstream tasks (Section 5).\\n2\\n': array([0.03058085, 0.0335088 , 0.05365412, ..., 0.0117994 , 0.00332523,\n",
              "                    0.00404783]),\n",
              "             '2\\nRelated Work\\nRepresentation Learning.\\nLarge-scale datasets like ImageNet [16, 76] and JFT [85] enabled\\nthe learning of general purpose representations for computer vision [4, 98]. These representations\\nare typically learned through supervised and un/self-supervised learning paradigms. Supervised\\npretraining [29, 51, 82] casts representation learning as a multi-class/label classification problem,\\nwhile un/self-supervised learning learns representation via proxy tasks like instance classification [97]\\nand reconstruction [31, 63]. Recent advances [12, 30] in contrastive learning [27] enabled learning\\nfrom web-scale data [21] that powers large-capacity cross-modal models [18, 46, 71, 101]. Similarly,\\nnatural language applications are built [40] on large language models [8] that are pretrained [68, 75]\\nin a un/self-supervised fashion with masked language modelling [19] or autoregressive training [70].\\nMatryoshka Representation Learning (MRL) is complementary to all these setups and can be\\nadapted with minimal overhead (Section 3). MRL equips representations with multifidelity at no\\nadditional cost which enables adaptive deployment based on the data and task (Section 4).\\nEfficient Classification and Retrieval.\\nEfficiency in classification and retrieval during inference\\ncan be studied with respect to the high yet constant deep featurization costs or the search cost which\\nscales with the size of the label space and data. Efficient neural networks address the first issue\\nthrough a variety of algorithms [25, 54] and design choices [39, 53, 87]. However, with a strong\\nfeaturizer, most of the issues with scale are due to the linear dependence on number of labels (L), size\\nof the data (N) and representation size (d), stressing RAM, disk and processor all at the same time.\\nThe sub-linear complexity dependence on number of labels has been well studied in context of\\ncompute [3, 43, 69] and memory [20] using Approximate Nearest Neighbor Search (ANNS) [62] or\\nleveraging the underlying hierarchy [17, 55]. In case of the representation size, often dimensionality\\nreduction [77, 88], hashing techniques [14, 52, 78] and feature selection [64] help in alleviating\\nselective aspects of the O(d) scaling at a cost of significant drops in accuracy. Lastly, most real-world\\nsearch systems [11, 15] are often powered by large-scale embedding based retrieval [10, 66] that\\nscales in cost with the ever increasing web-data. While categorization [89, 99] clusters similar things\\ntogether, it is imperative to be equipped with retrieval capabilities that can bring forward every\\ninstance [7]. Approximate Nearest Neighbor Search (ANNS) [42] makes it feasible with efficient\\nindexing [14] and traversal [5, 6] to present the users with the most similar documents/images from\\nthe database for a requested query. Widely adopted HNSW [62] (O(d log(N))) is as accurate as\\nexact retrieval (O(dN)) at the cost of a graph-based index overhead for RAM and disk [44].\\nMRL tackles the linear dependence on embedding size,\\nd,\\nby learning multifidelity\\nMatryoshka Representations.\\nLower-dimensional Matryoshka Representations are as accu-\\nrate as independently trained counterparts without the multiple expensive forward passes.\\nMatryoshka Representations provide an intermediate abstraction between high-dimensional vec-\\ntors and their efficient ANNS indices through the adaptive embeddings nested within the original\\nrepresentation vector (Section 4). All other aforementioned efficiency techniques are complementary\\nand can be readily applied to the learned Matryoshka Representations obtained from MRL.\\nSeveral works in efficient neural network literature [9, 93, 100] aim at packing neural networks of\\nvarying capacity within the same larger network. However, the weights for each progressively smaller\\nnetwork can be different and often require distinct forward passes to isolate the final representations.\\nThis is detrimental for adaptive inference due to the need for re-encoding the entire retrieval database\\nwith expensive sub-net forward passes of varying capacities. Several works [23, 26, 65, 59] investigate\\nthe notions of intrinsic dimensionality and redundancy of representations and objective spaces pointing\\nto minimum description length [74]. Finally, ordered representations proposed by Rippel et al. [73]\\nuse nested dropout in the context of autoencoders to learn nested representations. MRL differentiates\\nitself in formulation by optimizing only for O(log(d)) nesting dimensions instead of O(d). Despite\\nthis, MRL diffuses information to intermediate dimensions interpolating between the optimized\\nMatryoshka Representation sizes accurately (Figure 5); making web-scale feasible.\\n3\\nMatryoshka Representation Learning\\nFor d ‚ààN, consider a set M ‚äÇ[d] of representation sizes. For a datapoint x in the input do-\\nmain X, our goal is to learn a d-dimensional representation vector z ‚ààRd. For every m ‚ààM,\\n3\\n': array([0.0060241 , 0.01624397, 0.0484272 , ..., 0.02477616, 0.01314988,\n",
              "                    0.00967489]),\n",
              "             'Matryoshka Representation Learning (MRL) enables each of the first m dimensions of the em-\\nbedding vector, z1:m ‚ààRm to be independently capable of being a transferable and general purpose\\nrepresentation of the datapoint x. We obtain z using a deep neural network F( ¬∑ ; Œ∏F ): X ‚ÜíRd\\nparameterized by learnable weights Œ∏F , i.e., z := F(x; Œ∏F ). The multi-granularity is captured through\\nthe set of the chosen dimensions M, that contains less than log(d) elements, i.e., |M| ‚â§‚åälog(d)‚åã.\\nThe usual set M consists of consistent halving until the representation size hits a low information\\nbottleneck. We discuss the design choices in Section 4 for each of the representation learning settings.\\nFor the ease of exposition, we present the formulation for fully supervised representation learning\\nvia multi-class classification. Matryoshka Representation Learning modifies the typical setting\\nto become a multi-scale representation learning problem on the same task. For example, we train\\nResNet50 [29] on ImageNet-1K [76] which embeds a 224 √ó 224 pixel image into a d = 2048\\nrepresentation vector and then passed through a linear classifier to make a prediction, ÀÜ\\ny among the\\nL = 1000 labels. For MRL, we choose M = {8, 16, . . . , 1024, 2048} as the nesting dimensions.\\nSuppose we are given a labelled dataset D = {(x1, y1), . . . , (xN, yN)} where xi ‚ààX is an input\\npoint and yi ‚àà[L] is the label of xi for all i ‚àà[N]. MRL optimizes the multi-class classification loss\\nfor each of the nested dimension m ‚ààM using standard empirical risk minimization using a separate\\nlinear classifier, parameterized by W(m) ‚ààRL√óm. All the losses are aggregated after scaling with\\ntheir relative importance (cm ‚â•0)m‚ààM respectively. That is, we solve\\nmin\\n{W(m)}m‚ààM, Œ∏F\\n1\\nN\\nX\\ni‚àà[N]\\nX\\nm‚ààM\\ncm ¬∑ L\\n\\x10\\nW(m) ¬∑ F(xi; Œ∏F )1:m ; yi\\n\\x11\\n,\\n(1)\\nwhere L: RL √ó [L] ‚ÜíR+ is the multi-class softmax cross-entropy loss function. This is a standard\\noptimization problem that can be solved using sub-gradient descent methods. We set all the impor-\\ntance scales, cm = 1 for all m ‚ààM; see Section 5 for ablations. Lastly, despite only optimizing\\nfor O(log(d)) nested dimensions, MRL results in accurate representations, that interpolate, for\\ndimensions that fall between the chosen granularity of the representations (Section 4.2).\\nWe call this formulation as Matryoshka Representation Learning (MRL). A natural way to make\\nthis efficient is through weight-tying across all the linear classifiers, i.e., by defining W(m) = W1:m\\nfor a set of common weights W ‚ààRL√ód. This would reduce the memory cost due to the linear\\nclassifiers by almost half, which would be crucial in cases of extremely large output spaces [89, 99].\\nThis variant is called Efficient Matryoshka Representation Learning (MRL‚ÄìE). Refer to Alg 1\\nand Alg 2 in Appendix A for the building blocks of Matryoshka Representation Learning (MRL).\\nAdaptation to Learning Frameworks.\\nMRL can be adapted seamlessly to most representation\\nlearning frameworks at web-scale with minimal modifications (Section 4.1). For example, MRL‚Äôs\\nadaptation to masked language modelling reduces to MRL‚ÄìE due to the weight-tying between the\\ninput embedding matrix and the linear classifier. For contrastive learning, both in context of vision &\\nvision + language, MRL is applied to both the embeddings that are being contrasted with each other.\\nThe presence of normalization on the representation needs to be handled independently for each of\\nthe nesting dimension for best results (see Appendix C for more details).\\n4\\nApplications\\nIn this section, we discuss Matryoshka Representation Learning (MRL) for a diverse set of ap-\\nplications along with an extensive evaluation of the learned multifidelity representations. Further,\\nwe showcase the downstream applications of the learned Matryoshka Representations for flexible\\nlarge-scale deployment through (a) Adaptive Classification (AC) and (b) Adaptive Retrieval (AR).\\n4.1\\nRepresentation Learning\\nWe adapt Matryoshka Representation Learning (MRL) to various representation learning setups\\n(a) Supervised learning for vision: ResNet50 [29] on ImageNet-1K [76] and ViT-B/16 [22] on\\nJFT-300M [85], (b) Contrastive learning for vision + language: ALIGN model with ViT-B/16 vision\\nencoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling:\\nBERT [19] on English Wikipedia and BooksCorpus [102]. Please refer to Appendices B and C for\\ndetails regarding the model architectures, datasets and training specifics.\\n4\\n': array([0.02028089, 0.01468419, 0.05658507, ..., 0.01411193, 0.0010737 ,\n",
              "                    0.00389995]),\n",
              "             '8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n40\\n50\\n60\\n70\\n80\\nTop-1 Accuracy (%)\\nMRL\\nMRL-E\\nFF\\nSVD\\nSlim. Net\\nRand. LP\\nFigure 2: ImageNet-1K linear classification ac-\\ncuracy of ResNet50 models. MRL is as accurate\\nas the independently trained FF models for every\\nrepresentation size.\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n40\\n50\\n60\\n70\\n1-NN Accuracy (%)\\nMRL\\nMRL-E\\nFF\\nSVD\\nSlim. Net\\nRand. FS\\nFigure 3:\\nImageNet-1K 1-NN accuracy of\\nResNet50 models measuring the representation\\nquality for downstream task. MRL outperforms\\nall the baselines across all representation sizes.\\nWe do not search for best hyper-parameters for all MRL experiments but use the same hyper-\\nparameters as the independently trained baselines. ResNet50 outputs a 2048-dimensional repre-\\nsentation while ViT-B/16 and BERT-Base output 768-dimensional embeddings for each data point.\\nWe use M = {8, 16, 32, 64, 128, 256, 512, 1024, 2048} and M = {12, 24, 48, 96, 192, 384, 768} as\\nthe explicitly optimized nested dimensions respectively. Lastly, we extensively compare the MRL\\nand MRL‚ÄìE models to independently trained low-dimensional (fixed feature) representations (FF),\\ndimensionality reduction (SVD), sub-net method (slimmable networks [100]) and randomly selected\\nfeatures of the highest capacity FF model.\\nIn section 4.2, we evaluate the quality and capacity of the learned representations through linear\\nclassification/probe (LP) and 1-nearest neighbour (1-NN) accuracy. Experiments show that MRL\\nmodels remove the dependence on |M| resource-intensive independently trained models for the\\ncoarse-to-fine representations while being as accurate. Lastly, we show that despite optimizing only\\nfor |M| dimensions, MRL models diffuse the information, in an interpolative fashion, across all the\\nd dimensions providing the finest granularity required for adaptive deployment.\\n4.2\\nClassification\\nFigure 2 compares the linear classification accuracy of ResNet50 models trained and evaluated\\non ImageNet-1K. ResNet50‚ÄìMRL model is at least as accurate as each FF model at every rep-\\nresentation size in M while MRL‚ÄìE is within 1% starting from 16-dim. Similarly, Figure 3\\nshowcases the comparison of learned representation quality through 1-NN accuracy on ImageNet-1K\\n(trainset with 1.3M samples as the database and validation set with 50K samples as the queries).\\nMatryoshka Representations are up to 2% more accurate than their fixed-feature counterparts for\\nthe lower-dimensions while being as accurate elsewhere. 1-NN accuracy is an excellent proxy, at no\\nadditional training cost, to gauge the utility of learned representations in the downstream tasks.\\nWe also evaluate the quality of the representations from training ViT-B/16 on JFT-300M alongside the\\nViT-B/16 vision encoder of the ALIGN model ‚Äì two web-scale setups. Due to the expensive nature of\\nthese experiments, we only train the highest capacity fixed feature model and choose random features\\nfor evaluation in lower-dimensions. Web-scale is a compelling setting for MRL due to its relatively\\ninexpensive training overhead while providing multifidelity representations for downstream tasks.\\nFigure 4, evaluated with 1-NN on ImageNet-1K, shows that all the MRL models for JFT and ALIGN\\nare highly accurate while providing an excellent cost-vs-accuracy trade-off at lower-dimensions.\\nThese experiments show that MRL seamlessly scales to large-scale models and web-scale datasets\\nwhile providing the otherwise prohibitively expensive multi-granularity in the process. We also\\nhave similar observations when pretraining BERT; please see Appendix D.2 for more details. Our\\nexperiments also show that post-hoc compression (SVD), linear probe on random features, and\\nsub-net style slimmable networks drastically lose accuracy compared to MRL as the representation\\nsize decreases. Finally, Figure 5 shows that, while MRL explicitly optimizes O(log(d)) nested\\nrepresentations ‚Äì removing the O(d) dependence [73] ‚Äì, the coarse-to-fine grained information is\\ninterpolated across all d dimensions providing highest flexibility for adaptive deployment.\\n5\\n': array([ 0.01460139,  0.02980874,  0.0734148 , ..., -0.00357752,\n",
              "                     0.00138964,  0.01244556]),\n",
              "             '12\\n24\\n48\\n96\\n192\\n384\\n768\\nRepresentation Size\\n20\\n40\\n60\\n80\\n1-NN Accuracy (%)\\nJFT MRL\\nALIGN MRL\\nJFT MRL-E\\nJFT Rand.\\nALIGN Rand.\\nFigure 4: ImageNet-1K 1-NN accuracy for\\nViT-B/16 models trained on JFT-300M & as\\npart of ALIGN. MRL scales seamlessly to\\nweb-scale with minimal training overhead.\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n50\\n60\\n70\\n1-NN Accuracy (%)\\nViT-ALIGN\\nViT-JFT\\nRN50-IN1K\\nViT-ALIGN-Int\\nViT-JFT-Int\\nRN50-IN1K-Int\\nFigure 5: Despite optimizing MRL only for\\nO(log(d)) dimensions for ResNet50 and ViT-\\nB/16 models; the accuracy in the intermediate\\ndimensions shows interpolating behaviour.\\n4.2.1\\nAdaptive Classification\\nThe flexibility and coarse-to-fine granularity within Matryoshka Representations allows model\\ncascades [90] for Adaptive Classification (AC) [28]. Unlike standard model cascades [95], MRL does\\nnot require multiple expensive neural network forward passes. To perform AC with an MRL trained\\nmodel, we learn thresholds on the maximum softmax probability [33] for each nested classifier on\\na holdout validation set. We then use these thresholds to decide when to transition to the higher\\ndimensional representation (e.g 8 ‚Üí16 ‚Üí32) of the MRL model. Appendix D.1 discusses the\\nimplementation and learning of thresholds for cascades used for adaptive classification in detail.\\nFigure 6 shows the comparison between cascaded MRL representations (MRL‚ÄìAC) and indepen-\\ndently trained fixed feature (FF) models on ImageNet-1K with ResNet50. We computed the expected\\nrepresentation size for MRL‚ÄìAC based on the final dimensionality used in the cascade. We observed\\nthat MRL‚ÄìAC was as accurate, 76.30%, as a 512-dimensional FF model but required an expected\\ndimensionality of ‚àº37 while being only 0.8% lower than the 2048-dimensional FF baseline. Note\\nthat all MRL‚ÄìAC models are significantly more accurate than the FF baselines at comparable repre-\\nsentation sizes. MRL‚ÄìAC uses up to ‚àº14√ó smaller representation size for the same accuracy which\\naffords computational efficiency as the label space grows [89]. Lastly, our results with MRL‚ÄìAC\\nindicate that instances and classes vary in difficulty which we analyze in Section 5 and Appendix J.\\n4.3\\nRetrieval\\nNearest neighbour search with learned representations powers a plethora of retrieval and search appli-\\ncations [15, 91, 11, 66]. In this section, we discuss the image retrieval performance of the pretrained\\nResNet50 models (Section 4.1) on two large-scale datasets ImageNet-1K [76] and ImageNet-4K.\\nImageNet-1K has a database size of ‚àº1.3M and a query set of 50K samples uniformly spanning\\n1000 classes. We also introduce ImageNet-4K which has a database size of ‚àº4.2M and query set of\\n‚àº200K samples uniformly spanning 4202 classes (see Appendix B for details). A single forward pass\\non ResNet50 costs 4 GFLOPs while exact retrieval costs 2.6 GFLOPs per query for ImageNet-1K.\\nAlthough retrieval overhead is 40% of the total cost, retrieval cost grows linearly with the size of\\nthe database. ImageNet-4K presents a retrieval benchmark where the exact search cost becomes\\nthe computational bottleneck (8.6 GFLOPs per query). In both these settings, the memory and disk\\nusage are also often bottlenecked by the large databases. However, in most real-world applications\\nexact search, O(dN), is replaced with an approximate nearest neighbor search (ANNS) method like\\nHNSW [62], O(d log(N)), with minimal accuracy drop at the cost of additional memory overhead.\\nThe goal of image retrieval is to find images that belong to the same class as the query using\\nrepresentations obtained from a pretrained model. In this section, we compare retrieval performance\\nusing mean Average Precision @ 10 (mAP@10) which comprehensively captures the setup of\\nrelevant image retrieval at scale. We measure the cost per query using exact search in MFLOPs.\\nAll embeddings are unit normalized and retrieved using the L2 distance metric. Lastly, we report\\n6\\n': array([0.01175269, 0.01686158, 0.08277298, ..., 0.01474408, 0.01781389,\n",
              "                    0.0136013 ]),\n",
              "             '14x smaller \\nrepresentation size\\nFigure 6:\\nAdaptive classification on MRL\\nResNet50 using cascades results in 14√ó smaller\\nrepresentation size for the same level of accuracy\\non ImageNet-1K (‚àº37 vs 512 dims for 76.3%).\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n40\\n45\\n50\\n55\\n60\\n65\\nmAP@10 (%)\\nMRL\\nMRL-E\\nFF\\nSVD\\nSlim. Net\\nRand. FS\\nFigure 7: mAP@10 for Image Retrieval on\\nImageNet-1K with ResNet50. MRL consistently\\nproduces better retrieval performance over the\\nbaselines across all the representation sizes.\\nan extensive set of metrics spanning mAP@k and P@k for k = {10, 25, 50, 100} and real-world\\nwall-clock times for exact search and HNSW. See Appendices E and F for more details.\\nFigure 7 compares the mAP@10 performance of ResNet50 representations on ImageNet-1K across\\ndimensionalities for MRL, MRL‚ÄìE, FF, slimmable networks along with post-hoc compression\\nof vectors using SVD and random feature selection. Matryoshka Representations are often the\\nmost accurate while being up to 3% better than the FF baselines. Similar to classification, post-hoc\\ncompression and slimmable network baselines suffer from significant drop-off in retrieval mAP@10\\nwith ‚â§256 dimensions. Appendix E discusses the mAP@10 of the same models on ImageNet-4K.\\nMRL models are capable of performing accurate retrieval at various granularities without the\\nadditional expense of multiple model forward passes for the web-scale databases. FF models\\nalso generate independent databases which become prohibitively expense to store and switch in\\nbetween. Matryoshka Representations enable adaptive retrieval (AR) which alleviates the need\\nto use full-capacity representations, d = 2048, for all data and downstream tasks. Lastly, all the\\nvector compression techniques [60, 45] used as part of the ANNS pipelines are complimentary to\\nMatryoshka Representations and can further improve the efficiency-vs-accuracy trade-off.\\n4.3.1\\nAdaptive Retrieval\\nWe benchmark MRL in the adaptive retrieval setting (AR) [50]. For a given query image, we obtained\\na shortlist, K = 200, of images from the database using a lower-dimensional representation, e.g.\\nDs = 16 followed by reranking with a higher capacity representation, e.g. Dr = 2048. In real-world\\nscenarios where top ranking performance is the key objective, measured with mAP@k where k\\ncovers a limited yet crucial real-estate, AR provides significant compute and memory gains over\\nsingle-shot retrieval with representations of fixed dimensionality. Finally, the most expensive part\\nof AR, as with any retrieval pipeline, is the nearest neighbour search for shortlisting. For example,\\neven naive re-ranking of 200 images with 2048 dimensions only costs 400 KFLOPs. While we report\\nexact search cost per query for all AR experiments, the shortlisting component of the pipeline can\\nbe sped-up using ANNS (HNSW). Appendix I has a detailed discussion on compute cost for exact\\nsearch, memory overhead of HNSW indices and wall-clock times for both implementations. We note\\nthat using HNSW with 32 neighbours for shortlisting does not decrease accuracy during retrieval.\\nFigure\\n8\\nshowcases\\nthe\\ncompute-vs-accuracy\\ntrade-off\\nfor\\nadaptive\\nretrieval\\nusing\\nMatryoshka Representations compared to single-shot using fixed features with ResNet50\\non ImageNet-1K. We observed that all AR settings lied above the Pareto frontier of single-shot\\nretrieval with varying representation sizes. In particular for ImageNet-1K, we show that the AR\\nmodel with Ds = 16 & Dr = 2048 is as accurate as single-shot retrieval with d = 2048 while being\\n‚àº128√ó more efficient in theory and ‚àº14√ó faster in practice (compared using HNSW on the same\\nhardware). We show similar trends with ImageNet-4K, but note that we require Ds = 64 given\\nthe increased difficulty of the dataset. This results in ‚àº32√ó and ‚àº6√ó theoretical and in-practice\\nspeedups respectively. Lastly, while K = 200 works well for our adaptive retrieval experiments, we\\nablated over the shortlist size k in Appendix K.2 and found that the accuracy gains stopped after a\\n7\\n': array([-0.01033042,  0.01491419,  0.07683517, ...,  0.00760476,\n",
              "                     0.01678462, -0.00396235]),\n",
              "             '128x theoretical speed-up\\n14x real-world speed-up\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nDs\\nDr\\n6x real-world speed-up\\n32x theoretical speed-up\\n(a) ImageNet-1K\\n(b) ImageNet-4K\\nFigure 8: The trade-off between mAP@10 vs MFLOPs/Query for Adaptive Retrieval (AR) on\\nImageNet-1K (left) and ImageNet-4K (right). Every combination of Ds & Dr falls above the Pareto\\nline (orange dots) of single-shot retrieval with a fixed representation size while having configurations\\nthat are as accurate while being up to 14√ó faster in real-world deployment. Funnel retrieval is almost\\nas accurate as the baseline while alleviating some of the parameter choices of Adaptive Retrieval.\\npoint, further strengthening the use-case for Matryoshka Representation Learning and adaptive\\nretrieval.\\nEven with adaptive retrieval, it is hard to determine the choice of Ds & Dr. In order to alleviate this\\nissue to an extent, we propose Funnel Retrieval, a consistent cascade for adaptive retrieval. Funnel\\nthins out the initial shortlist by a repeated re-ranking and shortlisting with a series of increasing\\ncapacity representations. Funnel halves the shortlist size and doubles the representation size at\\nevery step of re-ranking. For example on ImageNet-1K, a funnel with the shortlist progression of\\n200 ‚Üí100 ‚Üí50 ‚Üí25 ‚Üí10 with the cascade of 16 ‚Üí32 ‚Üí64 ‚Üí128 ‚Üí256 ‚Üí2048\\nrepresentation sizes within Matryoshka Representation is as accurate as the single-shot 2048-dim\\nretrieval while being ‚àº128√ó more efficient theoretically (see Appendix F for more results). All\\nthese results showcase the potential of MRL and AR for large-scale multi-stage search systems [15].\\n5\\nFurther Analysis and Ablations\\nRobustness.\\nWe evaluate the robustness of the MRL models trained on ImageNet-1K on out-of-\\ndomain datasets, ImageNetV2/R/A/Sketch [72, 34, 35, 94], and compare them to the FF baselines.\\nTable 17 in Appendix H demonstrates that Matryoshka Representations for classification are at\\nleast as robust as the original representation while improving the performance on ImageNet-A by\\n0.6% ‚Äì a 20% relative improvement. We also study the robustness in the context of retrieval by using\\nImageNetV2 as the query set for ImageNet-1K database. Table 9 in Appendix E shows that MRL\\nmodels have more robust retrieval compared to the FF baselines by having up to 3% higher mAP@10\\nperformance. This observation also suggests the need for further investigation into robustness using\\nnearest neighbour based classification and retrieval instead of the standard linear probing setup. We\\nalso find that the zero-shot robustness of ALIGN-MRL (Table 18 in Appendix H) agrees with the\\nobservations made by Wortsman et al. [96]. Lastly, Table 6 in Appendix D.2 shows that MRL also\\nimproves the cosine similarity span between positive and random image-text pairs.\\nFew-shot and Long-tail Learning.\\nWe exhaustively evaluated few-shot learning on MRL models\\nusing nearest class mean [79]. Table 15 in Appendix G shows that that representations learned\\nthrough MRL perform comparably to FF representations across varying shots and number of classes.\\nMatryoshka Representations realize a unique pattern while evaluating on FLUID [92], a long-tail\\nsequential learning framework. We observed that MRL provides up to 2% accuracy higher on novel\\nclasses in the tail of the distribution, without sacrificing accuracy on other classes (Table 16 in\\nAppendix G). Additionally we find the accuracy between low-dimensional and high-dimensional\\nrepresentations is marginal for pretrain classes. We hypothesize that the higher-dimensional represen-\\ntations are required to differentiate the classes when few training examples of each are known. This\\nresults provides further evidence that different tasks require varying capacity based on their difficulty.\\nDisagreement across Dimensions.\\nThe information packing in Matryoshka Representations\\noften results in gradual increase of accuracy with increase in capacity. However, we observed that\\n8\\n': array([ 0.02193828,  0.02233312,  0.0727492 , ...,  0.00605215,\n",
              "                     0.00551542, -0.00113285]),\n",
              "             '(a)\\n(b)\\n(c)\\nFigure 9: Grad-CAM [80] progression of predictions in MRL model across 8, 16, 32 and 2048\\ndimensions. (a) 8-dimensional representation confuses due to presence of other relevant objects (with\\na larger field of view) in the scene and predicts ‚Äúshower cap‚Äù ; (b) 8-dim model confuses within\\nthe same super-class of ‚Äúboa‚Äù ; (c) 8 and 16-dim models incorrectly focus on the eyes of the doll\\n(\"sunglasses\") and not the \"sweatshirt\" which is correctly in focus at higher dimensions; MRL fails\\ngracefully in these scenarios and shows potential use cases of disagreement across dimensions.\\nthis trend was not ubiquitous and certain instances and classes were more accurate when evaluated\\nwith lower-dimensions (Figure 12 in Appendix J). With perfect routing of instances to appropriate\\ndimension, MRL can gain up to 4.6% classification accuracy. At the same time, the low-dimensional\\nmodels are less accurate either due to confusion within the same superclass [24] of the ImageNet\\nhierarchy or presence of multiple objects of interest. Figure 9 showcases 2 such examples for 8-\\ndimensional representation. These results along with Appendix J put forward the potential for MRL\\nto be a systematic framework for analyzing the utility and efficiency of information bottlenecks.\\nSuperclass Accuracy.\\nAs the information bottleneck becomes smaller, the overall accuracy on\\nfine-grained classes decreases rapidly (Figure 3). However, the drop-off is not as significant when\\nevaluated at a superclass level (Table 24 in Appendix J). Figure 10 presents that this phenomenon\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n84\\n86\\n88\\n90\\nTop-1 Accuracy (%)\\nMRL\\nFF\\nFigure 10: 31-way ImageNet-1K superclass clas-\\nsification across representation size for MRL &\\nFF models showing the capture of underlying\\nhierarchy through tight information bottlenecks.\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\nT\\nop-1 Accuracy (%)\\nmeasuring device\\nbuilding\\ngarment\\ntool\\nnourishment\\nprotective covering\\nvessel\\noscine\\nFigure 11:\\nDiverse per-superclass accuracy\\ntrends across representation sizes for ResNet50-\\nMRL on ImageNet-1K.\\n9\\n': array([ 0.00999569,  0.01646574,  0.08451307, ..., -0.00952305,\n",
              "                    -0.00266019,  0.01002124]),\n",
              "             'occurs with both MRL and FF models; MRL is more accurate across dimensions. This shows that\\ntight information bottlenecks while not highly accurate for fine-grained classification, do capture\\nrequired semantic information for coarser classification that could be leveraged for adaptive routing\\nfor retrieval and classification. Mutifidelity of Matryoshka Representation naturally captures the\\nunderlying hierarchy of the class labels with one single model. Lastly, Figure 11 showcases the\\naccuracy trends per superclass with MRL. The utility of additional dimensions in distinguishing\\na class from others within the same superclass is evident for ‚Äúgarment‚Äù which has up to 11%\\nimprovement for 8 ‚Üí16 dimensional representation transition. We also observed that superclasses\\nsuch as ‚Äúoscine (songbird)‚Äù had a clear visual distinction between the object and background and\\nthus predictions using 8 dimensions also led to a good inter-class separability within the superclass.\\n5.1\\nAblations\\nTable 26 in Appendix K presents that Matryoshka Representations can be enabled within off-the-\\nshelf pretrained models with inexpensive partial finetuning thus paving a way for ubiquitous adoption\\nof MRL. At the same time, Table 27 in Appendix C indicates that with optimal weighting of the\\nnested losses we could improve accuracy of lower-dimensions representations without accuracy\\nloss. Tables 28 and 29 in Appendix C ablate over the choice of initial granularity and spacing of the\\ngranularites. Table 28 reaffirms the design choice to shun extremely low dimensions that have poor\\nclassification accuracy as initial granularity for MRL while Table 29 confirms the effectiveness of\\nlogarthmic granularity spacing inspired from the behaviour of accuracy saturation across dimensions\\nover uniform. Lastly, Tables 30 and 31 in Appendix K.2 show that the retrieval performance saturates\\nafter a certain shortlist dimension and length depending on the complexity of the dataset.\\n6\\nDiscussion and Conclusions\\nThe results in Section 5.1 reveal interesting weaknesses of MRL that would be logical directions\\nfor future work. (1) Optimizing the weightings of the nested losses to obtain a Pareto optimal\\naccuracy-vs-efficiency trade-off ‚Äì a potential solution could emerge from adaptive loss balancing\\naspects of anytime neural networks [41]. (2) Using different losses at various fidelities aimed at\\nsolving a specific aspect of adaptive deployment ‚Äì e.g. high recall for 8-dimension and robustness\\nfor 2048-dimension. (3) Learning a search data-structure, like differentiable k-d tree, on top of\\nMatryoshka Representation to enable dataset and representation aware retrieval. (4) Finally, the\\njoint optimization of multi-objective MRL combined with end-to-end learnable search data-structure\\nto have data-driven adaptive large-scale retrieval for web-scale search applications.\\nIn conclusion, we presented\\nMatryoshka Representation Learning (MRL), a flexible represen-\\ntation learning approach that encodes information at multiple granularities in a single embedding\\nvector. This enables the MRL to adapt to a downstream task‚Äôs statistical complexity as well as\\nthe available compute resources. We demonstrate that MRL can be used for large-scale adaptive\\nclassification as well as adaptive retrieval. On standard benchmarks, MRL matches the accuracy of\\nthe fixed-feature baseline despite using 14√ó smaller representation size on average. Furthermore, the\\nMatryoshka Representation based adaptive shortlisting and re-ranking system ensures comparable\\nmAP@10 to the baseline while being 128√ó cheaper in FLOPs and 14√ó faster in wall-clock time.\\nFinally, most of the efficiency techniques for model inference and vector search are complementary\\nto MRL\\nfurther assisting in deployment at the compute-extreme environments.\\nAcknowledgments\\nWe are grateful to Srinadh Bhojanapalli, Lovish Madaan, Raghav Somani, Ludwig Schmidt, and\\nVenkata Sailesh Sanampudi for helpful discussions and feedback. Aditya Kusupati also thanks Tom\\nDuerig and Rahul Sukthankar for their support. Part of the paper‚Äôs large-scale experimentation is\\nsupported through a research GCP credit award from Google Cloud and Google Research. Gantavya\\nBhatt is supported in part by the CONIX Research Center, one of six centers in JUMP, a Semicon-\\nductor Research Corporation (SRC) program sponsored by DARPA. Sham Kakade acknowledges\\nfunding from the NSF award CCF-1703574 and ONR N00014-22-1-2377. Ali Farhadi acknowledges\\nfunding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA\\nW911NF-15-1-0543 and gifts from Allen Institute for Artificial Intelligence.\\n10\\n': array([ 0.04152173,  0.03697176,  0.06416758, ...,  0.02599983,\n",
              "                    -0.00516747, -0.00907394]),\n",
              "             'References\\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis,\\nJ. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Joze-\\nfowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man√©, R. Monga, S. Moore, D. Murray,\\nC. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke,\\nV. Vasudevan, F. Vi√©gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and\\nX. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL\\nhttps://www.tensorflow.org/. Software available from tensorflow.org.\\n[2] A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz.\\nObjectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\\nmodels. Advances in neural information processing systems, 32, 2019.\\n[3] S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks.\\nAdvances in Neural Information Processing Systems, 23, 2010.\\n[4] Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In\\nProceedings of ICML workshop on unsupervised and transfer learning, pages 17‚Äì36. JMLR\\nWorkshop and Conference Proceedings, 2012.\\n[5] J. L. Bentley. K-d trees for semidynamic point sets. In Proceedings of the sixth annual\\nsymposium on Computational geometry, pages 187‚Äì197, 1990.\\n[6] A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. In Proceedings\\nof the 23rd international conference on Machine learning, pages 97‚Äì104, 2006.\\n[7] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer\\nnetworks and ISDN systems, 30(1-7):107‚Äì117, 1998.\\n[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\\nP. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in\\nneural information processing systems, 33:1877‚Äì1901, 2020.\\n[9] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han. Once-for-all: Train one network and specialize\\nit for efficient deployment. arXiv preprint arXiv:1908.09791, 2019.\\n[10] W.-C. Chang, F. X. Yu, Y.-W. Chang, Y. Yang, and S. Kumar. Pre-training tasks for embedding-\\nbased large-scale retrieval. arXiv preprint arXiv:2002.03932, 2020.\\n[11] W.-C. Chang, D. Jiang, H.-F. Yu, C. H. Teo, J. Zhang, K. Zhong, K. Kolluri, Q. Hu,\\nN. Shandilya, V. Ievgrafov, et al. Extreme multi-label learning for semantic matching in\\nproduct search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discov-\\nery & Data Mining, pages 2643‚Äì2651, 2021.\\n[12] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\\nof visual representations. In International conference on machine learning, pages 1597‚Äì1607.\\nPMLR, 2020.\\n[13] Y. Chen, Z. Liu, H. Xu, T. Darrell, and X. Wang. Meta-baseline: exploring simple meta-\\nlearning for few-shot learning. In Proceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 9062‚Äì9071, 2021.\\n[14] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based\\non p-stable distributions. In Proceedings of the twentieth annual symposium on Computational\\ngeometry, pages 253‚Äì262, 2004.\\n[15] J. Dean. Challenges in building large-scale information retrieval systems. In Keynote of the\\n2nd ACM International Conference on Web Search and Data Mining (WSDM), volume 10,\\n2009.\\n[16] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale\\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern\\nrecognition, pages 248‚Äì255. Ieee, 2009.\\n11\\n': array([-0.01791081, -0.00436676,  0.02407419, ...,  0.00410173,\n",
              "                     0.00529435, -0.0440559 ]),\n",
              "             '[17] J. Deng, A. C. Berg, and L. Fei-Fei. Hierarchical semantic indexing for large scale image\\nretrieval. In CVPR 2011, pages 785‚Äì792. IEEE, 2011.\\n[18] K. Desai and J. Johnson. Virtex: Learning visual representations from textual annotations. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n11162‚Äì11173, 2021.\\n[19] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n[20] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting\\noutput codes. Journal of artificial intelligence research, 2:263‚Äì286, 1994.\\n[21] S. K. Divvala, A. Farhadi, and C. Guestrin. Learning everything about anything: Webly-\\nsupervised visual concept learning. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, pages 3270‚Äì3277, 2014.\\n[22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. De-\\nhghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers\\nfor image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n[23] J. J. Engelsma, A. K. Jain, and V. N. Boddeti. Hers: Homomorphically encrypted representation\\nsearch. IEEE Transactions on Biometrics, Behavior, and Identity Science, 4(3):349‚Äì360, 2022.\\n[24] L. Engstrom, A. Ilyas, H. Salman, S. Santurkar, and D. Tsipras. Robustness (python library),\\n2019. URL https://github.com/MadryLab/robustness.\\n[25] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer. A survey of\\nquantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630,\\n2021.\\n[26] S. Gong, V. N. Boddeti, and A. K. Jain. On the intrinsic dimensionality of image representations.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\\npages 3987‚Äì3996, 2019.\\n[27] M. Gutmann and A. Hyv√§rinen. Noise-contrastive estimation: A new estimation principle for\\nunnormalized statistical models. In Proceedings of the thirteenth international conference\\non artificial intelligence and statistics, pages 297‚Äì304. JMLR Workshop and Conference\\nProceedings, 2010.\\n[28] M. G. Harris and C. D. Giachritsis. Coarse-grained information dominates fine-grained\\ninformation in judgments of time-to-contact from retinal flow. Vision research, 40(6):601‚Äì611,\\n2000.\\n[29] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770‚Äì\\n778, 2016.\\n[30] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual\\nrepresentation learning. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 9729‚Äì9738, 2020.\\n[31] K. He, X. Chen, S. Xie, Y. Li, P. Doll√°r, and R. Girshick. Masked autoencoders are scalable\\nvision learners. arXiv preprint arXiv:2111.06377, 2021.\\n[32] J. Hegd√©. Time course of visual perception: coarse-to-fine processing and beyond. Progress in\\nneurobiology, 84(4):405‚Äì439, 2008.\\n[33] D. Hendrycks and K. Gimpel. A baseline for detecting misclassified and out-of-distribution\\nexamples in neural networks. arXiv preprint arXiv:1610.02136, 2016.\\n[34] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu,\\nS. Parajuli, M. Guo, et al. The many faces of robustness: A critical analysis of out-of-\\ndistribution generalization. In Proceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 8340‚Äì8349, 2021.\\n12\\n': array([-0.02375413,  0.01473994,  0.00674452, ...,  0.01728042,\n",
              "                    -0.01716436, -0.03430292]),\n",
              "             '[35] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\\npages 15262‚Äì15271, 2021.\\n[36] S. Hooker, A. Courville, G. Clark, Y. Dauphin, and A. Frome. What do compressed deep\\nneural networks forget? arXiv preprint arXiv:1911.05248, 2019.\\n[37] S. Hooker, N. Moorosi, G. Clark, S. Bengio, and E. Denton. Characterising bias in compressed\\nmodels. arXiv preprint arXiv:2010.03058, 2020.\\n[38] H. Hotelling. Analysis of a complex of statistical variables into principal components. Journal\\nof educational psychology, 24(6):417, 1933.\\n[39] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and\\nH. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications.\\narXiv preprint arXiv:1704.04861, 2017.\\n[40] J. Howard and S. Ruder. Universal language model fine-tuning for text classification. arXiv\\npreprint arXiv:1801.06146, 2018.\\n[41] H. Hu, D. Dey, M. Hebert, and J. A. Bagnell. Learning anytime predictions in neural networks\\nvia adaptive loss balancing. In Proceedings of the AAAI Conference on Artificial Intelligence,\\nvolume 33, pages 3812‚Äì3821, 2019.\\n[42] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse\\nof dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of\\ncomputing, pages 604‚Äì613, 1998.\\n[43] H. Jain, V. Balasubramanian, B. Chunduri, and M. Varma. Slice: Scalable linear extreme\\nclassifiers trained on 100 million labels for related searches. In Proceedings of the Twelfth\\nACM International Conference on Web Search and Data Mining, pages 528‚Äì536, 2019.\\n[44] S. Jayaram Subramanya, F. Devvrit, H. V. Simhadri, R. Krishnawamy, and R. Kadekodi.\\nDiskann: Fast accurate billion-point nearest neighbor search on a single node. Advances in\\nNeural Information Processing Systems, 32, 2019.\\n[45] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE\\ntransactions on pattern analysis and machine intelligence, 33(1):117‚Äì128, 2010.\\n[46] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig.\\nScaling up visual and vision-language representation learning with noisy text supervision. In\\nInternational Conference on Machine Learning, pages 4904‚Äì4916. PMLR, 2021.\\n[47] J. Johnson, M. Douze, and H. J√©gou. Billion-scale similarity search with GPUs. IEEE\\nTransactions on Big Data, 7(3):535‚Äì547, 2019.\\n[48] W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26:\\n189‚Äì206, 1984.\\n[49] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia,\\nN. Boden, A. Borchers, et al. In-datacenter performance analysis of a tensor processing unit.\\nIn Proceedings of the 44th annual international symposium on computer architecture, pages\\n1‚Äì12, 2017.\\n[50] T. C. Kaz Sato.\\nVertex ai matching engine.\\nMicrosoft AI Blog, 2021.\\nURL\\nhttps://cloud.google.com/blog/topics/developers-practitioners/\\nfind-anything-blazingly-fast-googles-vector-search-technology.\\n[51] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional\\nneural networks. Advances in neural information processing systems, 25, 2012.\\n[52] B. Kulis, P. Jain, and K. Grauman. Fast similarity search for learned metrics. IEEE Transactions\\non Pattern Analysis and Machine Intelligence, 31(12):2143‚Äì2157, 2009.\\n13\\n': array([-0.04099741,  0.01523213,  0.00019768, ...,  0.00038773,\n",
              "                     0.00486729, -0.03318335]),\n",
              "             '[53] A. Kusupati, M. Singh, K. Bhatia, A. Kumar, P. Jain, and M. Varma. Fastgrnn: A fast, accurate,\\nstable and tiny kilobyte sized gated recurrent neural network. Advances in Neural Information\\nProcessing Systems, 31, 2018.\\n[54] A. Kusupati, V. Ramanujan, R. Somani, M. Wortsman, P. Jain, S. Kakade, and A. Farhadi.\\nSoft threshold weight reparameterization for learnable sparsity. In International Conference\\non Machine Learning, pages 5544‚Äì5555. PMLR, 2020.\\n[55] A. Kusupati, M. Wallingford, V. Ramanujan, R. Somani, J. S. Park, K. Pillutla, P. Jain,\\nS. Kakade, and A. Farhadi. Llc: Accurate, multi-purpose learnt low-dimensional binary codes.\\nAdvances in Neural Information Processing Systems, 34, 2021.\\n[56] G. Leclerc, A. Ilyas, L. Engstrom, S. M. Park, H. Salman, and A. Madry. ffcv. https:\\n//github.com/libffcv/ffcv/, 2022. commit 607d117.\\n[57] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature, 521(7553):436‚Äì444, 2015.\\n[58] S. Lee, S. Purushwalkam Shiva Prakash, M. Cogswell, V. Ranjan, D. Crandall, and D. Batra.\\nStochastic multiple choice learning for training diverse deep ensembles. Advances in Neural\\nInformation Processing Systems, 29, 2016.\\n[59] C. Li, H. Farkhoor, R. Liu, and J. Yosinski. Measuring the intrinsic dimension of objective\\nlandscapes. arXiv preprint arXiv:1804.08838, 2018.\\n[60] Y. Linde, A. Buzo, and R. Gray. An algorithm for vector quantizer design. IEEE Transactions\\non communications, 28(1):84‚Äì95, 1980.\\n[61] I. Loshchilov and F. Hutter.\\nDecoupled weight decay regularization.\\narXiv preprint\\narXiv:1711.05101, 2017.\\n[62] Y. A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search\\nusing hierarchical navigable small world graphs. IEEE transactions on pattern analysis and\\nmachine intelligence, 42(4):824‚Äì836, 2018.\\n[63] J. Masci, U. Meier, D. Cire¬∏\\nsan, and J. Schmidhuber. Stacked convolutional auto-encoders for\\nhierarchical feature extraction. In International conference on artificial neural networks, pages\\n52‚Äì59. Springer, 2011.\\n[64] P. Mitra, C. Murthy, and S. K. Pal. Unsupervised feature selection using feature similarity.\\nIEEE transactions on pattern analysis and machine intelligence, 24(3):301‚Äì312, 2002.\\n[65] V. Nanda, T. Speicher, J. P. Dickerson, S. Feizi, K. P. Gummadi, and A. Weller. Diffused\\nredundancy in pre-trained representations. arXiv preprint arXiv:2306.00183, 2023.\\n[66] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https:\\n//blog.google/products/search/search-language-understanding-bert/.\\n[67] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\\nN. Gimelshein, L. Antiga, et al.\\nPytorch: An imperative style, high-performance deep\\nlearning library. Advances in neural information processing systems, 32, 2019.\\n[68] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer.\\nDeep contextualized word representations. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long Papers), pages 2227‚Äì2237, New Orleans, Louisiana, June\\n2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https:\\n//aclanthology.org/N18-1202.\\n[69] Y. Prabhu, A. Kusupati, N. Gupta, and M. Varma. Extreme regression for dynamic search\\nadvertising. In Proceedings of the 13th International Conference on Web Search and Data\\nMining, pages 456‚Äì464, 2020.\\n[70] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understand-\\ning by generative pre-training. OpenAI Blog, 2018. URL https://openai.com/blog/\\nlanguage-unsupervised/.\\n14\\n': array([-0.03270765,  0.02014146,  0.00334665, ...,  0.00471745,\n",
              "                     0.00272792, -0.02030554]),\n",
              "             '[71] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language su-\\npervision. In International Conference on Machine Learning, pages 8748‚Äì8763. PMLR,\\n2021.\\n[72] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to\\nimagenet? In International Conference on Machine Learning, pages 5389‚Äì5400. PMLR,\\n2019.\\n[73] O. Rippel, M. Gelbart, and R. Adams. Learning ordered representations with nested dropout.\\nIn International Conference on Machine Learning, pages 1746‚Äì1754. PMLR, 2014.\\n[74] J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465‚Äì471, 1978.\\n[75] S. Ruder, M. E. Peters, S. Swayamdipta, and T. Wolf. Transfer learning in natural language\\nprocessing. In Proceedings of the 2019 conference of the North American chapter of the\\nassociation for computational linguistics: Tutorials, pages 15‚Äì18, 2019.\\n[76] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\\nA. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\\njournal of computer vision, 115(3):211‚Äì252, 2015.\\n[77] R. Salakhutdinov and G. Hinton. Learning a nonlinear embedding by preserving class neigh-\\nbourhood structure. In Artificial Intelligence and Statistics, pages 412‚Äì419. PMLR, 2007.\\n[78] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate\\nReasoning, 50(7):969‚Äì978, 2009.\\n[79] J. S. S√°nchez, F. Pla, and F. J. Ferri. On the use of neighbourhood-based non-parametric\\nclassifiers. Pattern Recognition Letters, 18(11-13):1179‚Äì1186, 1997.\\n[80] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam:\\nVisual explanations from deep networks via gradient-based localization. In Proceedings of the\\nIEEE international conference on computer vision, pages 618‚Äì626, 2017.\\n[81] N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In\\nInternational Conference on Machine Learning, pages 4596‚Äì4604. PMLR, 2018.\\n[82] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image\\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\\n[83] L. N. Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter\\nconference on applications of computer vision (WACV), pages 464‚Äì472. IEEE, 2017.\\n[84] D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient\\ndescent on separable data. The Journal of Machine Learning Research, 19(1):2822‚Äì2878,\\n2018.\\n[85] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data\\nin deep learning era. In Proceedings of the IEEE international conference on computer vision,\\npages 843‚Äì852, 2017.\\n[86] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and\\nmomentum in deep learning. In International conference on machine learning, pages 1139‚Äì\\n1147. PMLR, 2013.\\n[87] M. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks.\\nIn International conference on machine learning, pages 6105‚Äì6114. PMLR, 2019.\\n[88] L. Van Der Maaten, E. Postma, J. Van den Herik, et al. Dimensionality reduction: a comparative.\\nJ Mach Learn Res, 10(66-71):13, 2009.\\n[89] M. Varma. Extreme classification. Communications of the ACM, 62(11):44‚Äì45, 2019.\\n15\\n': array([-0.01603432, -0.00080407,  0.0104411 , ..., -0.00385627,\n",
              "                    -0.00526268, -0.02328023]),\n",
              "             '[90] P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In\\nProceedings of the 2001 IEEE computer society conference on computer vision and pattern\\nrecognition. CVPR 2001, volume 1, pages I‚ÄìI. Ieee, 2001.\\n[91] C. Waldburger. As search needs evolve, microsoft makes ai tools for better search available\\nto researchers and developers. Microsoft AI Blog, 2019. URL https://blogs.microsoft.\\ncom/ai/bing-vector-search/.\\n[92] M. Wallingford, A. Kusupati, K. Alizadeh-Vahid, A. Walsman, A. Kembhavi, and A. Farhadi.\\nAre we overfitting to experimental setups in recognition? arXiv preprint arXiv:2007.02519,\\n2020.\\n[93] M. Wallingford, H. Li, A. Achille, A. Ravichandran, C. Fowlkes, R. Bhotika, and S. Soatto.\\nTask adaptive parameter sharing for multi-task learning. arXiv preprint arXiv:2203.16708,\\n2022.\\n[94] H. Wang, S. Ge, Z. Lipton, and E. P. Xing. Learning robust global representations by penalizing\\nlocal predictive power. In Advances in Neural Information Processing Systems, pages 10506‚Äì\\n10518, 2019.\\n[95] X. Wang, D. Kondratyuk, K. M. Kitani, Y. Movshovitz-Attias, and E. Eban. Multiple networks\\nare more efficient than one: Fast and accurate models via ensembles and cascades. arXiv\\npreprint arXiv:2012.01988, 2020.\\n[96] M. Wortsman, G. Ilharco, M. Li, J. W. Kim, H. Hajishirzi, A. Farhadi, H. Namkoong, and\\nL. Schmidt. Robust fine-tuning of zero-shot models. arXiv preprint arXiv:2109.01903, 2021.\\n[97] Z. Wu, Y. Xiong, S. Yu, and D. Lin. Unsupervised feature learning via non-parametric\\ninstance-level discrimination. arXiv preprint arXiv:1805.01978, 2018.\\n[98] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural\\nnetworks? Advances in neural information processing systems, 27, 2014.\\n[99] H.-F. Yu, K. Zhong, J. Zhang, W.-C. Chang, and I. S. Dhillon. Pecos: Prediction for enormous\\nand correlated output spaces. Journal of Machine Learning Research, 23(98):1‚Äì32, 2022.\\n[100] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang. Slimmable neural networks. arXiv preprint\\narXiv:1812.08928, 2018.\\n[101] R. Zellers, J. Lu, X. Lu, Y. Yu, Y. Zhao, M. Salehi, A. Kusupati, J. Hessel, A. Farhadi, and\\nY. Choi. Merlot reserve: Neural script knowledge through vision and language and sound.\\narXiv preprint arXiv:2201.02639, 2022.\\n[102] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning\\nbooks and movies: Towards story-like visual explanations by watching movies and reading\\nbooks. In Proceedings of the IEEE international conference on computer vision, pages 19‚Äì27,\\n2015.\\n16\\n': array([-0.03812596,  0.02091217, -0.0054116 , ..., -0.01325027,\n",
              "                    -0.01087079, -0.01926694]),\n",
              "             'Checklist\\n1. For all authors...\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper‚Äôs\\ncontributions and scope? [Yes]\\n(b) Did you describe the limitations of your work? [Yes] See Section 6\\n(c) Did you discuss any potential negative societal impacts of your work? [N/A] Our work\\ndoes not have any additional negative societal impact on top of the existing impact of\\nrepresentation learning. However, a study on the trade-off between representation size\\nand the tendency to encode biases is an interesting future direction along the lines of\\nexisting literature [36, 37]. A part of this is already presented in Section 5.\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\\nthem? [Yes]\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n3. If you ran experiments...\\n(a) Did you include the code, data, and instructions needed to reproduce the main ex-\\nperimental results (either in the supplemental material or as a URL)? [Yes] See sup-\\nplemental material and Appendix A. All the code and public models will be open\\nsourced.\\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\\nwere chosen)? [Yes] See Section 4 and Appendix C.\\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-\\nments multiple times)? [No] We benchmarked on large-scale datasets like ImageNet-\\n1K, JFT-300M and ALIGN data with models like ResNet and ViT making it extremely\\nexpensive to run things multiple times.\\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\\nof GPUs, internal cluster, or cloud provider)? [Yes] See Appendix C and Appendix I.\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n(a) If your work uses existing assets, did you cite the creators? [Yes]\\n(b) Did you mention the license of the assets? [No] All the non-proprietary datasets and\\ncode used are public under MIT, BSD or CC licenses.\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\nWe created a new subset of ImageNet-21K for downstream evaluation of retrieval\\nperformance at scale. See Section 4.3 and Appendix B\\n(d) Did you discuss whether and how consent was obtained from people whose data you‚Äôre\\nusing/curating? [N/A]\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable\\ninformation or offensive content? [N/A]\\n5. If you used crowdsourcing or conducted research with human subjects...\\n(a) Did you include the full text of instructions given to participants and screenshots, if\\napplicable? [N/A]\\n(b) Did you describe any potential participant risks, with links to Institutional Review\\nBoard (IRB) approvals, if applicable? [N/A]\\n(c) Did you include the estimated hourly wage paid to participants and the total amount\\nspent on participant compensation? [N/A]\\n17\\n': array([ 0.0408325 ,  0.01427934,  0.05690345, ...,  0.00206736,\n",
              "                     0.01873161, -0.00609011]),\n",
              "             'Contents\\n1\\nIntroduction\\n1\\n2\\nRelated Work\\n3\\n3\\nMatryoshka Representation Learning\\n3\\n4\\nApplications\\n4\\n4.1\\nRepresentation Learning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n4.2\\nClassification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n4.2.1\\nAdaptive Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n4.3\\nRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n4.3.1\\nAdaptive Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n5\\nFurther Analysis and Ablations\\n8\\n5.1\\nAblations\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n6\\nDiscussion and Conclusions\\n10\\nA Code for Matryoshka Representation Learning\\n(MRL)\\n19\\nB\\nDatasets\\n20\\nC Matryoshka Representation Learning Model Training\\n20\\nD Classification Results\\n21\\nD.1 Adaptive Classification (MRL‚ÄìAC)\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\nD.2\\nJFT, ALIGN and BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\nE\\nImage Retrieval\\n22\\nF\\nAdaptive Retrieval\\n24\\nG Few-shot and Sample Efficiency\\n25\\nH Robustness Experiments\\n27\\nI\\nIn Practice Costs\\n27\\nJ\\nAnalysis of Model Disagreement\\n29\\nK Ablation Studies\\n32\\nK.1\\nMRL Training Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\nK.2\\nRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n33\\n18\\n': array([0.02991775, 0.01670572, 0.04177659, ..., 0.01673032, 0.01407315,\n",
              "                    0.00010197]),\n",
              "             'A\\nCode for Matryoshka Representation Learning\\n(MRL)\\nWe use Alg 1 and 2 provided below to train supervised ResNet50‚ÄìMRL models on ImageNet-1K.\\nWe provide this code as a template to extend MRL to any domain.\\nAlgorithm 1 Pytorch code for Matryoshka Cross-Entropy Loss\\nclass Matryoshka_CE_Loss(nn.Module):\\ndef __init__(self, relative_importance, **kwargs):\\nsuper(Matryoshka_CE_Loss, self).__init__()\\nself.criterion = nn.CrossEntropyLoss(**kwargs)\\nself.relative_importance = relative_importance # usually set\\nto all ones\\ndef forward(self, output, target):\\nloss=0\\nfor i in range(len(output)):\\nloss+= self.relative_importance[i] * self.criterion(output[\\ni], target)\\nreturn loss\\nAlgorithm 2 Pytorch code for MRL Linear Layer\\nclass MRL_Linear_Layer(nn.Module):\\ndef __init__(self, nesting_list: List, num_classes=1000, efficient=\\nFalse, **kwargs):\\nsuper(MRL_Linear_Layer, self).__init__()\\nself.nesting_list=nesting_list # set of m in M (Eq. 1)\\nself.num_classes=num_classes\\nself.is_efficient=efficient # flag for MRL-E\\nif not is_efficient:\\nfor i, num_feat in enumerate(self.nesting_list):\\nsetattr(self, f\"nesting_classifier_{i}\", nn.Linear(\\nnum_feat, self.num_classes, **kwargs))\\nelse:\\nsetattr(self, \"nesting_classifier_0\", nn.Linear(self.\\nnesting_list[-1], self.num_classes, **kwargs)) #\\nInstantiating one nn.Linear layer for MRL-E\\ndef forward(self, x):\\nnesting_logits = ()\\nfor i, num_feat in enumerate(self.nesting_list):\\nif(self.is_efficient):\\nefficient_logit = torch.matmul(x[:, :num_feat],\\n(self.nesting_classifier_0.weight[:, :\\nnum_feat]).t())\\nelse:\\nnesting_logits.append(getattr(self, f\"\\nnesting_classifier_{i}\")(x[:, :num_feat]))\\nif(self.is_efficient):\\nnesting_logits.append(efficient_logit)\\nreturn nesting_logits\\n19\\n': array([ 0.04395073,  0.01975855,  0.03149321, ..., -0.00685525,\n",
              "                    -0.02518011,  0.00490048]),\n",
              "             'B\\nDatasets\\nImageNet-1K [76] contains 1,281,167 labeled train images, and 50,000 labelled validation images\\nacross 1,000 classes. The images were transformed with standard procedures detailed by FFCV [56].\\nImageNet-4K dataset was constructed by selecting 4,202 classes, non-overlapping with ImageNet-\\n1K, from ImageNet-21K [16] with 1,050 or more examples. The train set contains 1,000 examples and\\nthe query/validation set contains 50 examples per class totalling to ‚àº4.2M and ‚àº200K respectively.\\nWe will release the list of images curated together to construct ImageNet-4K.\\nJFT-300M [85] is a large-scale multi-label dataset with 300M images labelled across 18,291 cate-\\ngories.\\nALIGN [46] utilizes a large scale noisy image-text dataset containing 1.8B image-text pairs.\\nImageNet Robustness Datasets\\nWe experimented on the following datasets to examine the robust-\\nness of MRL models:\\nImageNetV2 [72] is a collection of 10K images sampled a decade after the original construction of\\nImageNet [16]. ImageNetV2 contains 10 examples each from the 1,000 classes of ImageNet-1K.\\nImageNet-A [35] contains 7.5K real-world adversarially filtered images from 200 ImageNet-\\n1K classes.\\nImageNet-R [34] contains 30K artistic image renditions for 200 of the original ImageNet-1K classes.\\nImageNet-Sketch [94] contains 50K sketches, evenly distributed over all 1,000 ImageNet-1K classes.\\nObjectNet [2] contains 50K images across 313 object classes, each containing ‚àº160 images each.\\nC\\nMatryoshka Representation Learning Model Training\\nWe trained all ResNet50‚ÄìMRL models using the efficient dataloaders of FFCV [56]. We utilized the\\nrn50_40_epochs.yaml configuration file of FFCV to train all MRL models defined below:\\n‚Ä¢ MRL: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=False)\\n‚Ä¢ MRL‚ÄìE: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=True)\\n‚Ä¢ FF‚Äìk: ResNet50 model with the fc layer replaced by torch.nn.Linear(k, num_classes),\\nwhere k ‚àà[8, 16, 32, 64, 128, 256, 512, 1024, 2048]. We will henceforth refer to these models as\\nsimply FF, with the k value denoting representation size.\\nWe trained all ResNet50 models with a learning rate of 0.475 with a cyclic learning rate schedule [83].\\nThis was after appropriate scaling (0.25√ó) of the learning rate specified in the configuration file to\\naccommodate for 2xA100 NVIDIA GPUs available for training, compared to the 8xA100 GPUs\\nutilized in the FFCV benchmarks. We trained with a batch size of 256 per GPU, momentum [86] of\\n0.9, and an SGD optimizer with a weight decay of 1e-4.\\nOur code (Appendix A) makes minimal modifications to the training pipeline provided by FFCV to\\nlearn Matryoshka Representations.\\nWe trained ViT-B/16 models for JFT-300M on a 8x8 cloud TPU pod [49] using Tensorflow [1] with a\\nbatchsize of 128 and trained for 300K steps. Similarly, ALIGN models were trained using Tensorflow\\non 8x8 cloud TPU pod for 1M steps with a batchsize of 64 per TPU. Both these models were trained\\nwith adafactor optimizer [81] with a linear learning rate decay starting at 1e-3.\\nLastly, we trained a BERT-Base model on English Wikipedia and BookCorpus. We trained our models\\nin Tensorflow using a 4x4 cloud TPU pod with a total batchsize of 1024. We used AdamW [61]\\noptimizer with a linear learning rate decay starting at 1e-4 and trained for 450K steps.\\nIn each configuration/case, if the final representation was normalized in the FF implementation, MRL\\nmodels adopted the same for each nested dimension for a fair comparison.\\n20\\n': array([-0.00216572,  0.0175566 ,  0.07376595, ..., -0.00512014,\n",
              "                     0.00863723,  0.00997738]),\n",
              "             'Table 1: Top-1 classification accuracy (%) for ResNet50 MRL and baseline models on ImageNet-1K.\\nRep. Size\\nRand. LP\\nSVD\\nFF\\nSlim. Net\\nMRL\\nMRL‚ÄìE\\n8\\n4.56\\n2.34\\n65.29\\n0.42\\n66.63\\n56.66\\n16\\n11.29\\n7.17\\n72.85\\n0.96\\n73.53\\n71.94\\n32\\n27.21\\n20.46\\n74.60\\n2.27\\n75.03\\n74.48\\n64\\n49.47\\n48.10\\n75.27\\n5.59\\n75.82\\n75.35\\n128\\n65.70\\n67.24\\n75.29\\n14.15\\n76.30\\n75.80\\n256\\n72.43\\n74.59\\n75.71\\n38.42\\n76.47\\n76.22\\n512\\n74.94\\n76.78\\n76.18\\n69.80\\n76.65\\n76.36\\n1024\\n76.10\\n76.87\\n76.63\\n74.61\\n76.76\\n76.48\\n2048\\n76.87\\n‚Äì\\n76.87\\n76.26\\n76.80\\n76.51\\nD\\nClassification Results\\nWe show the top-1 classification accuracy of ResNet50‚ÄìMRL models on ImageNet-1K in Table 1\\nand Figure 2. We compare the performance of MRL models (MRL, MRL‚ÄìE) to several baselines:\\n‚Ä¢ FF: We utilize the FF-k models described in Appendix C for k ‚àà{8, ...2048}.\\n‚Ä¢ SVD: We performed a low rank approximation of the 1000-way classification layer of FF-2048,\\nwith rank = 1000.\\n‚Ä¢ Rand. LP: We compared against a linear classifier fit on randomly selected features [30].\\n‚Ä¢ Slim. Net: We take pretrained slimmable neural networks [100] which are trained with a flexible\\nwidth backbone (25%, 50%, 75% and full width). For each representation size, we consider the\\nfirst k dimensions for classification. Note that training of slimmable neural networks becomes\\nunstable when trained below 25% width due to the hardness in optimization and low complexity of\\nthe model.\\nAt lower dimensions ( d ‚â§128), MRL outperforms all baselines significantly, which indicates that\\npretrained models lack the multifidelity of Matryoshka Representations and are incapable of fitting\\nan accurate linear classifier at low representation sizes.\\nWe compared the performance of MRL models at various representation sizes via 1-nearest neighbors\\n(1-NN) image classification accuracy on ImageNet-1K in Table 2 and Figure 3. We provide detailed\\ninformation regarding the k-NN search pipeline in Appendix E. We compared against a baseline\\nof attempting to enforce nesting to a FF-2048 model by 1) Random Feature Selection (Rand. FS):\\nconsidering the first m dimensions of FF-2048 for NN lookup, and 2) FF+SVD: performing SVD\\non the FF-2048 representations at the specified representation size, 3) FF+JL: performing random\\nprojection according to the Johnson-Lindenstrauss lemma [48] on the FF-2048 representations at\\nthe specified representation size. We also compared against the 1-NN accuracy of slimmable neural\\nnets [100] as an additional baseline. We observed these baseline models to perform very poorly at\\nlower dimensions, as they were not explicitly trained to learn Matryoshka Representations.\\nTable 2: 1-NN accuracy (%) on ImageNet-1K for various ResNet50 models.\\nRep. Size\\nRand. FS\\nSVD\\nJL\\nFF\\nSlimmable\\nMRL\\nMRL‚ÄìE\\n8\\n2.36\\n19.14\\n0.11\\n58.93\\n1.00\\n62.19\\n57.45\\n16\\n12.06\\n46.02\\n0.09\\n66.77\\n5.12\\n67.91\\n67.05\\n32\\n32.91\\n60.78\\n0.06\\n68.84\\n16.95\\n69.46\\n68.6\\n64\\n49.91\\n67.04\\n0.05\\n69.41\\n35.60\\n70.17\\n69.61\\n128\\n60.91\\n69.63\\n0.06\\n69.35\\n51.16\\n70.52\\n70.12\\n256\\n65.75\\n70.67\\n0.04\\n69.72\\n60.61\\n70.62\\n70.36\\n512\\n68.77\\n71.06\\n0.03\\n70.18\\n65.82\\n70.82\\n70.74\\n1024\\n70.41\\n71.22\\n-\\n70.34\\n67.19\\n70.89\\n71.07\\n2048\\n71.19\\n71.21\\n-\\n71.19\\n66.10\\n70.97\\n71.21\\nD.1\\nAdaptive Classification (MRL‚ÄìAC)\\nIn an attempt to use the smallest representation that works well for classification for every image in\\nthe ImageNet-1K validation set, we learned a policy to increase the representation size from mi to\\n21\\n': array([-0.00838517,  0.02048419,  0.07497329, ...,  0.00400399,\n",
              "                     0.00539088, -0.01180307]),\n",
              "             'Table 3: Threshold-based adaptive classification performance of ResNet50 MRL on a 40K sized\\nheld-out subset of the ImageNet-1K validation set. Results are averaged over 30 random held-out\\nsubsets.\\nExpected Rep. Size\\nAccuracy\\n13.43 ¬± 0.81\\n73.79 ¬± 0.10\\n18.32 ¬± 1.36\\n75.25 ¬± 0.11\\n25.87 ¬± 2.41\\n76.05 ¬± 0.15\\n36.26 ¬± 4.78\\n76.28 ¬± 0.16\\n48.00 ¬± 8.24\\n76.43 ¬± 0.18\\n64.39 ¬± 12.55\\n76.53 ¬± 0.19\\n90.22 ¬± 20.88\\n76.55 ¬± 0.20\\n118.85 ¬± 33.37\\n76.56 ¬± 0.20\\nmi+1 using a 10K sized subset of the ImageNet-1K validation set. This policy is based on whether the\\nprediction confidence pi using representation size mi exceeds a learned threshold t‚àó\\ni . If pi ‚â•t‚àó\\ni , we\\nused predictions from representation size mi otherwise, we increased to representation size mi+1. To\\nlearn the optimal threshold t‚àó\\ni , we performed a grid search between 0 and 1 (100 samples). For each\\nthreshold tk, we computed the classification accuracy over our 10K image subset. We set t‚àó\\ni equal\\nto the smallest threshold tk that gave the best accuracy. We use this procedure to obtain thresholds\\nfor successive models, i.e., {t‚àó\\nj | j ‚àà{8, 16, 32, 64, . . . , 2048}}. To improve reliability of threshold\\nbased greedy policy, we use test time augmentation which has been used successfully in the past [82].\\nFor inference, we used the remaining held-out 40K samples from the ImageNet-1K validation set. We\\nbegan with smallest sized representation (m = 8) and compared the computed prediction confidence\\np8 to learned optimal threshold t‚àó\\n8. If p8 ‚â§t‚àó\\n8, then we increased m = 16, and repeated this\\nprocedure until m = d = 2048. To compute the expected dimensions, we performed early stopping\\nat m = {16, 32, 64, . . . 2048} and computed the expectation using the distribution of representation\\nsizes. As shown in Table 3 and Figure 6, we observed that in expectation, we only needed a ‚àº37\\nsized representation to achieve 76.3% classification accuracy on ImageNet-1K, which was roughly\\n14√ó smaller than the FF‚Äì512 baseline. Even if we computed the expectation as a weighted average\\nover the cumulative sum of representation sizes {8, 24, 56, . . .}, due to the nature of multiple linear\\nheads for MRL, we ended up with an expected size of 62 that still provided a roughly 8.2√ó efficient\\nrepresentation than the FF‚Äì512 baseline. However, MRL‚ÄìE alleviates this extra compute with a\\nminimal drop in accuracy.\\nD.2\\nJFT, ALIGN and BERT\\nWe examine the k-NN classification accuracy of learned Matryoshka Representations via\\nALIGN‚ÄìMRL and JFT-ViT‚ÄìMRL in Table 4.\\nFor ALIGN [46], we observed that learning\\nMatryoshka Representations via ALIGN‚ÄìMRL improved classification accuracy at nearly all\\ndimensions when compared to ALIGN. We observed a similar trend when training ViT-B/16 [22]\\nfor JFT-300M [85] classification, where learning Matryoshka Representations via MRL and\\nMRL‚ÄìE on top of JFT-ViT improved classification accuracy for nearly all dimensions, and signif-\\nicantly for lower ones. This demonstrates that training to learn Matryoshka Representations\\nis feasible and extendable even for extremely large scale datasets.\\nWe also demonstrate that\\nMatryoshka Representations are learned at interpolated dimensions for both ALIGN and JFT-\\nViT, as shown in Table 5, despite not being trained explicitly at these dimensions. Lastly, Table 6\\nshows that MRL training leads to a increase in the cosine similarity span between positive and\\nrandom image-text pairs.\\nWe also evaluated the capability of Matryoshka Representations to extend to other natural language\\nprocessing via masked language modeling (MLM) with BERT [19], whose results are tabulated\\nin Table 7. Without any hyper-parameter tuning, we observed Matryoshka Representations to be\\nwithin 0.5% of FF representations for BERT MLM validation accuracy. This is a promising initial\\nresult that could help with large-scale adaptive document retrieval using BERT‚ÄìMRL.\\nE\\nImage Retrieval\\nWe evaluated the strength of Matryoshka Representations via image retrieval on ImageNet-1K (the\\ntraining distribution), as well as on out-of-domain datasets ImageNetV2 and ImageNet-4K for all\\n22\\n': array([-0.0002646 ,  0.00691579,  0.05696205, ...,  0.01091492,\n",
              "                     0.01849823,  0.00055364]),\n",
              "             'Table 4: ViT-B/16 and ViT-B/16-MRL top-1 and top-5 k-NN accuracy (%) for ALIGN and JFT. Top-1\\nentries where MRL‚ÄìE and MRL outperform baselines are bolded for both ALIGN and JFT-ViT.\\nRep. Size\\nALIGN\\nALIGN-MRL\\nJFT-ViT\\nJFT-ViT-MRL\\nJFT-ViT-MRL‚ÄìE\\nTop-1\\nTop-5\\nTop-1\\nTop-5\\nTop-1\\nTop-5\\nTop-1\\nTop-5\\nTop-1\\nTop-5\\n12\\n11.90\\n28.05\\n43.57\\n67.36\\n27.07\\n48.57\\n53.61\\n75.30\\n51.54\\n73.94\\n24\\n33.35\\n55.58\\n56.44\\n78.19\\n48.64\\n70.20\\n62.80\\n81.51\\n62.40\\n81.36\\n48\\n51.32\\n73.15\\n62.33\\n82.30\\n63.58\\n81.80\\n67.24\\n84.37\\n66.89\\n83.80\\n96\\n61.82\\n81.97\\n65.72\\n84.61\\n68.56\\n85.13\\n69.74\\n85.86\\n68.80\\n85.13\\n192\\n66.71\\n85.27\\n67.00\\n85.36\\n71.32\\n86.21\\n71.34\\n86.62\\n70.41\\n86.01\\n384\\n67.65\\n85.70\\n67.70\\n85.73\\n71.67\\n86.98\\n71.73\\n87.08\\n71.18\\n86.46\\n768\\n68.00\\n86.10\\n67.85\\n85.85\\n72.10\\n87.20\\n71.85\\n86.92\\n71.31\\n86.62\\nTable 5: Examining top-1 and top-5 k-NN accuracy (%) at interpolated hidden dimensions for ALIGN\\nand JFT. This indicates that MRL is able to scale classification accuracy as hidden dimensions increase\\neven at dimensions that were not explicitly considered during training.\\nInterpolated\\nRep. Size\\nALIGN-MRL\\nJFT-ViT-MRL\\nTop-1\\nTop-5\\nTop-1\\nTop-5\\n16\\n49.06\\n72.26\\n58.35\\n78.55\\n32\\n58.64\\n79.96\\n64.98\\n82.89\\n64\\n63.90\\n83.39\\n68.19\\n84.85\\n128\\n66.63\\n85.00\\n70.35\\n86.24\\n256\\n67.10\\n85.30\\n71.57\\n86.77\\n512\\n67.64\\n85.72\\n71.55\\n86.67\\nMRL ResNet50 models. We generated the database and query sets, containing N and Q samples\\nrespectively, with a standard PyTorch [67] forward pass on each dataset. We specify the representation\\nsize at which we retrieve a shortlist of k-nearest neighbors (k-NN) by Ds. The database is a\\nthus a [N, Ds] array, the query set is a [Q, Ds] array, and the neighbors set is a [Q, k] array.\\nFor metrics, we utilized corrected mean average precision (mAP@k) [55] and precision (P@k):\\nP@k = correct_pred\\nk\\nwhere correct_pred is the average number of retrieved NN with the correct\\nlabel over the entire query set using a shortlist of length k.\\nWe performed retrieval with FAISS [47], a library for efficient similarity search. To obtain a shortlist\\nof k-NN, we built an index to search the database. We performed an exhaustive NN search with\\nthe L2 distance metric with faiss.IndexFlatL2, as well as an approximate NN search (ANNS)\\nvia HNSW [47] with faiss.IndexHNSWFlat. We used HNSW with M = 32 unless otherwise\\nmentioned, and henceforth referred to as HNSW32. The exact search index was moved to the GPU\\nfor fast k-NN search computation, whereas the HNSW index was kept on the CPU as it currently\\nlacks GPU support. We show the wall clock times for building the index as well as the index size\\nin Table 20. We observed exact search to have a smaller index size which was faster to build when\\ncompared to HNSW, which trades off a larger index footprint for fast NN search (discussed in more\\ndetail in Appendix K). The database and query vectors are normalized with faiss.normalize_L2\\nbefore building the index and performing search.\\nRetrieval performance on ImageNet-1K, i.e. the training distribution, is shown in Table 8. MRL out-\\nperforms FF models for nearly all representation size for both top-1 and mAP@10, and especially\\nat low representation size (Ds ‚â§32). MRL‚ÄìE loses out to FF significantly only at Ds = 8. This\\nindicates that training ResNet50 models via the MRL training paradigm improves retrieval at low\\nrepresentation size over models explicitly trained at those representation size (FF-8...2048).\\nWe carried out all retrieval experiments at Ds ‚àà{8, 16, 32, 64, 128, 256, 512, 1024, 2048}, as\\nthese were the representation sizes which were a part of the nesting_list at which losses\\nwere added during training, as seen in Algorithm 1, Appendix A. To examine whether MRL\\nis able to learn Matryoshka Representations at dimensions in between the representation size\\nfor which it was trained, we also tabulate the performance of MRL at interpolated Ds ‚àà\\n{12, 24, 48, 96, 192, 384, 768, 1536} as MRL‚ÄìInterpolated and MRL‚ÄìE‚ÄìInterpolated (see Table 8).\\nWe observed that performance scaled nearly monotonically between the original representation\\n23\\n': array([-0.01873738,  0.0062573 ,  0.09830843, ..., -0.00115931,\n",
              "                     0.00984591,  0.00374245]),\n",
              "             'Table 6: Cosine similarity between embeddings\\nAvg. Cosine Similarity\\nALIGN\\nALIGN-MRL\\nPositive Text to Image\\n0.27\\n0.49\\nRandom Text to Image\\n8e-3\\n-4e-03\\nRandom Image to Image\\n0.10\\n0.08\\nRandom Text to Text\\n0.22\\n0.07\\nTable 7: Masked Language Modelling (MLM) accuracy(%) of FF and MRL models on the validation\\nset.\\nRep. Size\\nBERT-FF\\nBERT-MRL\\n12\\n60.12\\n59.92\\n24\\n62.49\\n62.05\\n48\\n63.85\\n63.40\\n96\\n64.32\\n64.15\\n192\\n64.70\\n64.58\\n384\\n65.03\\n64.81\\n768\\n65.54\\n65.00\\nsize and the interpolated representation size as we increase Ds, which demonstrates that MRL is\\nable to learn Matryoshka Representations at nearly all representation size m ‚àà[8, 2048] despite\\noptimizing only for |M| nested representation sizes.\\nWe examined the robustness of MRL for retrieval on out-of-domain datasets ImageNetV2 and\\nImageNet-4K, as shown in Table 9 and Table 10 respectively. On ImageNetV2, we observed that MRL\\noutperformed FF at all Ds on top-1 Accuracy and mAP@10, and MRL‚ÄìE outperformed FF at all\\nDs except Ds = 8. This demonstrates the robustness of the learned Matryoshka Representations\\nfor out-of-domain image retrieval.\\nF\\nAdaptive Retrieval\\nThe time complexity of retrieving a shortlist of k-NN often scales as O(d), where d =Ds, for a\\nfixed k and N. We thus will have a theoretical 256√ó higher cost for Ds = 2048 over Ds = 8. We\\ndiscuss search complexity in more detail in Appendix I. In an attempt to replicate performance at\\nhigher Ds while using less FLOPs, we perform adaptive retrieval via retrieving a k-NN shortlist with\\nrepresentation size Ds, and then re-ranking the shortlist with representations of size Dr. Adaptive\\nretrieval for a shortlist length k = 200 is shown in Table 11 for ImageNet-1K, and in Table 12 for\\nImageNet-4K. On ImageNet-1K, we are able to achieve comparable performance to retrieval with\\nDs = 2048 (from Table 8) with Ds = 16 at 128√ó less MFLOPs/Query (used interchangeably with\\nMFLOPs). Similarly, on ImageNet-4K, we are able to achieve comparable performance to retrieval\\nwith Ds = 2048 (from Table 10) with Ds = 64 on ImageNet-1K and ImageNet-4K, at 32√ó less\\nMFLOPs. This demonstrates the value of intelligent routing techniques which utilize appropriately\\nsized Matryoshka Representations for retrieval.\\n24\\n': array([-0.00534687, -0.00358494,  0.07577842, ...,  0.01011967,\n",
              "                    -0.00045651,  0.01873696]),\n",
              "             'Table 8: Retrieve a shortlist of 200-NN with Ds sized representations on ImageNet-1K via exact\\nsearch with L2 distance metric. Top-1 and mAP@10 entries (%) where MRL‚ÄìE and MRL outperform\\nFF at their respective representation sizes are bolded.\\nModel\\nDs\\nMFlops\\nTop-1\\nTop-5\\nTop-10\\nmAP@10\\nmAP@25\\nmAP@50\\nmAP@100\\nP@10\\nP@25\\nP@50\\nP@100\\nFF\\n8\\n10\\n58.93\\n75.76\\n80.25\\n53.42\\n52.29\\n51.84\\n51.57\\n59.32\\n59.28\\n59.25\\n59.21\\n16\\n20\\n66.77\\n80.88\\n84.40\\n61.63\\n60.51\\n59.98\\n59.62\\n66.76\\n66.58\\n66.43\\n66.27\\n32\\n41\\n68.84\\n82.58\\n86.14\\n63.35\\n62.08\\n61.36\\n60.76\\n68.43\\n68.13\\n67.83\\n67.48\\n64\\n82\\n69.41\\n83.56\\n87.33\\n63.26\\n61.64\\n60.63\\n59.67\\n68.49\\n67.91\\n67.38\\n66.74\\n128\\n164\\n69.35\\n84.23\\n88.24\\n62.30\\n60.16\\n58.73\\n57.29\\n67.84\\n66.83\\n65.96\\n64.92\\n256\\n328\\n69.72\\n84.71\\n88.54\\n61.47\\n58.85\\n57.02\\n55.13\\n67.19\\n65.82\\n64.64\\n63.24\\n512\\n656\\n70.18\\n85.04\\n88.91\\n61.37\\n58.41\\n56.26\\n53.98\\n67.12\\n65.49\\n64.07\\n62.35\\n1024\\n1312\\n70.34\\n85.38\\n89.19\\n61.13\\n57.87\\n55.47\\n52.90\\n66.93\\n65.08\\n63.43\\n61.45\\n2048\\n2624\\n71.19\\n85.66\\n89.17\\n62.90\\n60.06\\n57.99\\n55.76\\n68.46\\n66.9\\n65.52\\n63.83\\nMRL‚ÄìE\\n8\\n10\\n57.39\\n74.18\\n79.16\\n51.80\\n50.41\\n49.60\\n48.86\\n57.50\\n57.16\\n56.81\\n56.36\\n16\\n20\\n67.08\\n81.38\\n85.15\\n61.60\\n60.36\\n59.66\\n59.04\\n66.79\\n66.53\\n66.24\\n65.87\\n32\\n41\\n68.62\\n82.92\\n86.44\\n63.34\\n61.97\\n61.14\\n60.39\\n68.49\\n68.06\\n67.65\\n67.17\\n64\\n82\\n69.56\\n83.49\\n86.85\\n63.84\\n62.33\\n61.43\\n60.57\\n68.93\\n68.4\\n67.96\\n67.38\\n128\\n164\\n70.13\\n83.63\\n87.07\\n64.15\\n62.58\\n61.61\\n60.70\\n69.19\\n68.62\\n68.11\\n67.50\\n256\\n328\\n70.39\\n83.8\\n87.28\\n64.35\\n62.76\\n61.76\\n60.82\\n69.36\\n68.79\\n68.26\\n67.63\\n512\\n656\\n70.74\\n83.91\\n87.33\\n64.69\\n63.05\\n62.06\\n61.14\\n69.63\\n69.00\\n68.50\\n67.88\\n1024\\n1312\\n71.05\\n84.13\\n87.46\\n64.85\\n63.22\\n62.19\\n61.26\\n69.78\\n69.16\\n68.60\\n67.99\\n2048\\n2624\\n71.17\\n84.27\\n87.67\\n64.99\\n63.33\\n62.29\\n61.33\\n69.90\\n69.24\\n68.68\\n68.05\\nMRL‚ÄìE\\nInterpolated\\n12\\n15\\n64.25\\n79.21\\n83.29\\n58.83\\n57.50\\n56.71\\n56.02\\n64.10\\n63.78\\n63.42\\n63.02\\n24\\n31\\n68.28\\n82.31\\n85.89\\n62.75\\n61.41\\n60.62\\n59.92\\n67.89\\n67.49\\n67.11\\n66.69\\n48\\n61\\n69.20\\n83.15\\n86.67\\n63.58\\n62.12\\n61.23\\n60.42\\n68.71\\n68.19\\n67.75\\n67.22\\n96\\n123\\n70.05\\n83.63\\n87.11\\n64.04\\n62.46\\n61.52\\n60.63\\n69.10\\n68.51\\n68.04\\n67.45\\n192\\n246\\n70.36\\n83.72\\n87.21\\n64.26\\n62.65\\n61.65\\n60.72\\n69.26\\n68.67\\n68.15\\n67.53\\n384\\n492\\n70.54\\n83.88\\n87.28\\n64.55\\n62.94\\n61.93\\n61.01\\n69.51\\n68.92\\n68.40\\n67.78\\n768\\n984\\n70.96\\n84.05\\n87.44\\n64.79\\n63.15\\n62.15\\n61.22\\n69.72\\n69.10\\n68.56\\n67.95\\n1536\\n1968\\n71.19\\n84.17\\n87.57\\n64.94\\n63.29\\n62.26\\n61.32\\n69.85\\n69.21\\n68.66\\n68.04\\nMRL\\n8\\n10\\n62.19\\n77.05\\n81.34\\n56.74\\n55.47\\n54.76\\n54.12\\n62.06\\n61.81\\n61.54\\n61.17\\n16\\n20\\n67.91\\n81.44\\n85.00\\n62.94\\n61.79\\n61.16\\n60.64\\n67.93\\n67.71\\n67.48\\n67.20\\n32\\n41\\n69.46\\n83.01\\n86.30\\n64.21\\n62.96\\n62.22\\n61.58\\n69.18\\n68.87\\n68.54\\n68.17\\n64\\n82\\n70.17\\n83.53\\n86.95\\n64.69\\n63.33\\n62.53\\n61.80\\n69.67\\n69.25\\n68.89\\n68.42\\n128\\n164\\n70.52\\n83.98\\n87.25\\n64.94\\n63.50\\n62.63\\n61.83\\n69.93\\n69.44\\n69.02\\n68.50\\n256\\n328\\n70.62\\n84.17\\n87.38\\n65.04\\n63.56\\n62.66\\n61.81\\n70.02\\n69.52\\n69.07\\n68.50\\n512\\n656\\n70.82\\n84.31\\n87.55\\n65.14\\n63.57\\n62.62\\n61.73\\n70.12\\n69.53\\n69.04\\n68.45\\n1024\\n1312\\n70.89\\n84.44\\n87.68\\n65.16\\n63.58\\n62.60\\n61.68\\n70.14\\n69.54\\n69.01\\n68.41\\n2048\\n2624\\n70.97\\n84.41\\n87.74\\n65.20\\n63.57\\n62.56\\n61.60\\n70.18\\n69.52\\n68.98\\n68.35\\nMRL\\nInterpolated\\n12\\n15\\n65.89\\n80.04\\n83.68\\n60.84\\n59.66\\n58.98\\n58.37\\n65.94\\n65.72\\n65.45\\n65.08\\n24\\n31\\n68.76\\n82.48\\n85.87\\n63.64\\n62.42\\n61.74\\n61.13\\n68.64\\n68.35\\n68.07\\n67.71\\n48\\n61\\n69.96\\n83.40\\n86.65\\n64.58\\n63.2\\n62.42\\n61.72\\n69.53\\n69.10\\n68.75\\n68.32\\n96\\n123\\n70.40\\n83.83\\n87.04\\n64.86\\n63.46\\n62.62\\n61.84\\n69.82\\n69.38\\n68.98\\n68.48\\n192\\n246\\n70.64\\n84.09\\n87.37\\n65.00\\n63.53\\n62.66\\n61.83\\n69.98\\n69.49\\n69.05\\n68.50\\n384\\n492\\n70.69\\n84.25\\n87.41\\n65.09\\n63.56\\n62.64\\n61.76\\n70.05\\n69.51\\n69.04\\n68.46\\n768\\n984\\n70.84\\n84.40\\n87.63\\n65.16\\n63.59\\n62.62\\n61.71\\n70.14\\n69.55\\n69.03\\n68.44\\n1536\\n1968\\n70.88\\n84.39\\n87.71\\n65.18\\n63.59\\n62.58\\n61.64\\n70.16\\n69.54\\n68.99\\n68.38\\nFunnel Retrieval.\\nWe also designed a simple cascade policy which we call funnel retrieval to\\nsuccessively improve and refine the k-NN shortlist at increasing Ds. This was an attempt to remove\\nthe dependence on manual choice of Ds & Dr. We retrieved a shortlist at Ds and then re-ranked the\\nshortlist five times while simultaneously increasing Dr (rerank cascade) and decreasing the shortlist\\nlength (shortlist cascade), which resembles a funnel structure. We tabulate the performance of funnel\\nretrieval in various configurations in Table 13 on ImageNet-1K, and in Table 14 on ImageNet-4K.\\nWith funnel retrieval on ImageNet-1K, we were able to achieve top-1 accuracy within 0.1% of\\nretrieval with Ds = 2048 (as in Table 8) with a funnel with Ds = 16, with 128√ó less MFLOPs.\\nSimilarly, we are able to achieve equivalent top-1 accuracy within 0.15% of retrieval at Ds = 2048\\n(as in Table 10) with funnel retrieval at Ds = 32 on ImageNet-4K, with 64√ó less MFLOPs. This\\ndemonstrates that with funnel retrieval, we can emulate the performance of retrieval with Ds = 2048\\nwith a fraction of the MFLOPs.\\nG\\nFew-shot and Sample Efficiency\\nWe compared MRL, MRL‚ÄìE, and FF on various benchmarks to observe the effect of representation\\nsize on sample efficiency. We used Nearest Class Means [79] for classification which has been shown\\nto be effective in the few-shot regime [13].\\nImageNetV2.\\nRepresentations are evaluated on ImageNetV2 with the n-shot k-way setup. Ima-\\ngeNetV2 is a dataset traditionally used to evaluate the robustness of models to natural distribution\\nshifts. For our experiments we evaluate accuracy of the model given n examples from the Ima-\\ngeNetV2 distribution. We benchmark representations in the traditional small-scale (10-way) and\\n25\\n': array([ 0.02622247, -0.02087462,  0.07181735, ..., -0.00389672,\n",
              "                     0.02466017, -0.00766725]),\n",
              "             'Table 9: Retrieve a shortlist of 200-NN with Ds sized representations on ImageNetV2 via exact\\nsearch with L2 distance metric. Top-1 and mAP@10 entries (%) where MRL‚ÄìE outperforms FF are\\nbolded. MRL outperforms FF at all Ds and is thus not bolded.\\nConfig\\nDs\\nMFLOPs\\nTop-1\\nTop-5\\nTop-10\\nmAP@10\\nmAP@25\\nmAP@50\\nmAP@100\\nP@10\\nP@25\\nP@50\\nP@100\\nFF\\n8\\n10\\n48.79\\n64.70\\n69.72\\n43.04\\n41.89\\n41.42\\n41.17\\n48.43\\n48.27\\n48.25\\n48.19\\n16\\n20\\n55.08\\n69.50\\n74.08\\n49.63\\n48.53\\n48.06\\n47.75\\n54.76\\n54.64\\n54.53\\n54.39\\n32\\n41\\n56.69\\n71.10\\n76.47\\n51.11\\n49.85\\n49.17\\n48.65\\n56.23\\n55.96\\n55.71\\n55.42\\n64\\n82\\n57.37\\n72.71\\n77.48\\n51.28\\n49.75\\n48.85\\n47.99\\n56.65\\n56.14\\n55.71\\n55.15\\n128\\n164\\n57.17\\n73.31\\n78.64\\n50.07\\n48.09\\n46.79\\n45.58\\n55.75\\n54.89\\n54.12\\n53.28\\n256\\n328\\n57.09\\n74.04\\n79.24\\n49.11\\n46.66\\n44.99\\n43.35\\n55.02\\n53.77\\n52.74\\n51.53\\n512\\n656\\n57.12\\n73.91\\n79.32\\n48.95\\n46.25\\n44.37\\n42.42\\n54.88\\n53.49\\n52.29\\n50.83\\n1024\\n1312\\n57.53\\n74.17\\n79.55\\n48.27\\n45.41\\n43.36\\n41.26\\n54.31\\n52.84\\n51.49\\n49.87\\n2048\\n2624\\n57.84\\n74.59\\n79.45\\n49.99\\n47.47\\n45.66\\n43.87\\n55.89\\n54.63\\n53.45\\n52.12\\nMRL‚ÄìE\\n8\\n10\\n47.05\\n62.53\\n67.60\\n40.79\\n39.47\\n38.78\\n38.16\\n46.03\\n45.77\\n45.54\\n45.17\\n16\\n20\\n55.73\\n70.54\\n74.86\\n49.86\\n48.57\\n47.84\\n47.26\\n54.97\\n54.71\\n54.44\\n54.10\\n32\\n41\\n57.33\\n71.61\\n76.64\\n51.26\\n49.92\\n49.09\\n48.42\\n56.46\\n56.11\\n55.70\\n55.30\\n64\\n82\\n57.90\\n72.55\\n77.44\\n51.89\\n50.29\\n49.34\\n48.53\\n57.06\\n56.45\\n55.97\\n55.43\\n128\\n164\\n57.73\\n72.79\\n77.28\\n52.02\\n50.38\\n49.49\\n48.62\\n57.13\\n56.58\\n56.15\\n55.58\\n256\\n328\\n58.22\\n72.77\\n77.67\\n52.16\\n50.61\\n49.67\\n48.81\\n57.30\\n56.79\\n56.33\\n55.77\\n512\\n656\\n58.46\\n73.00\\n77.88\\n52.52\\n50.97\\n50.02\\n49.16\\n57.65\\n57.10\\n56.64\\n56.08\\n1024\\n1312\\n58.71\\n73.29\\n78.00\\n52.70\\n51.13\\n50.17\\n49.30\\n57.83\\n57.26\\n56.77\\n56.20\\n2048\\n2624\\n58.86\\n73.17\\n78.00\\n52.88\\n51.25\\n50.26\\n49.36\\n57.95\\n57.35\\n56.85\\n56.25\\nMRL\\n8\\n10\\n50.41\\n65.56\\n70.27\\n45.51\\n44.38\\n43.71\\n43.17\\n50.55\\n50.44\\n50.17\\n49.91\\n16\\n20\\n56.64\\n70.19\\n74.61\\n50.98\\n49.76\\n49.16\\n48.69\\n55.90\\n55.66\\n55.52\\n55.29\\n32\\n41\\n57.96\\n71.88\\n76.41\\n52.06\\n50.78\\n50.09\\n49.54\\n57.18\\n56.83\\n56.57\\n56.27\\n64\\n82\\n58.94\\n72.74\\n77.17\\n52.65\\n51.24\\n50.44\\n49.76\\n57.72\\n57.29\\n56.94\\n56.52\\n128\\n164\\n59.13\\n73.07\\n77.49\\n52.94\\n51.42\\n50.53\\n49.74\\n58.00\\n57.47\\n57.05\\n56.55\\n256\\n328\\n59.18\\n73.64\\n77.75\\n52.96\\n51.45\\n50.52\\n49.70\\n58.01\\n57.53\\n57.06\\n56.54\\n512\\n656\\n59.40\\n73.85\\n77.97\\n53.01\\n51.39\\n50.46\\n49.61\\n58.11\\n57.49\\n57.04\\n56.48\\n1024\\n1312\\n59.11\\n73.77\\n77.92\\n52.98\\n51.37\\n50.40\\n49.54\\n58.13\\n57.51\\n57.00\\n56.45\\n2048\\n2624\\n59.63\\n73.84\\n77.97\\n52.96\\n51.34\\n50.34\\n49.44\\n58.07\\n57.48\\n56.95\\n56.36\\nTable 10: Retrieve a shortlist of 200-NN with Ds sized representations on ImageNet-4K via exact\\nsearch with L2 distance metric. MRL‚ÄìE and FF models are omitted for clarity and compute/infer-\\nence time costs. All entries are in %.\\nConfig\\nDs\\nMFLOPs\\nTop-1\\nTop-5\\nTop-10\\nmAP@10\\nmAP@25\\nmAP@50\\nmAP@100\\nP@10\\nP@25\\nP@50\\nP@100\\nMRL\\n8\\n34\\n10.60\\n26.23\\n35.57\\n5.32\\n4.29\\n3.76\\n3.36\\n9.13\\n8.77\\n8.46\\n8.13\\n16\\n67\\n16.74\\n36.91\\n47.28\\n8.64\\n6.83\\n5.84\\n5.05\\n13.82\\n12.79\\n12.04\\n13.27\\n32\\n134\\n21.54\\n43.75\\n54.11\\n11.36\\n8.88\\n7.47\\n6.31\\n17.25\\n15.67\\n14.47\\n13.27\\n64\\n269\\n25.00\\n47.97\\n58.25\\n13.38\\n10.40\\n8.67\\n7.23\\n19.68\\n17.64\\n16.14\\n14.65\\n128\\n538\\n27.27\\n50.35\\n60.47\\n14.77\\n11.47\\n9.53\\n7.91\\n21.25\\n18.95\\n17.26\\n15.59\\n256\\n1076\\n28.53\\n51.95\\n61.90\\n15.66\\n12.19\\n10.12\\n8.38\\n22.28\\n19.81\\n18.01\\n16.22\\n512\\n2151\\n29.46\\n53.03\\n62.81\\n16.29\\n12.70\\n10.55\\n8.72\\n22.96\\n20.42\\n18.54\\n16.68\\n1024\\n4303\\n30.23\\n53.72\\n63.45\\n16.76\\n13.08\\n10.86\\n8.97\\n23.48\\n20.88\\n18.93\\n17.00\\n2048\\n8606\\n30.87\\n54.32\\n64.02\\n17.20\\n13.43\\n11.14\\n9.19\\n23.97\\n21.28\\n19.28\\n17.30\\nMRL-\\nInterpolated\\n12\\n50\\n14.04\\n32.56\\n42.71\\n7.16\\n5.70\\n4.92\\n4.32\\n11.81\\n11.08\\n10.52\\n9.94\\n24\\n101\\n19.49\\n40.82\\n51.26\\n10.17\\n7.98\\n6.75\\n5.75\\n15.76\\n14.43\\n13.42\\n12.40\\n48\\n202\\n23.51\\n46.23\\n56.56\\n12.49\\n9.72\\n8.13\\n6.81\\n18.62\\n16.75\\n15.39\\n14.04\\n96\\n403\\n26.25\\n49.32\\n59.48\\n14.15\\n11.00\\n9.15\\n7.61\\n20.55\\n18.36\\n16.78\\n15.17\\n192\\n807\\n27.94\\n51.32\\n61.32\\n15.29\\n11.89\\n9.88\\n8.18\\n21.86\\n19.46\\n17.71\\n15.96\\n384\\n1614\\n29.03\\n52.53\\n62.45\\n15.99\\n12.46\\n10.35\\n8.56\\n22.64\\n20.14\\n18.29\\n16.47\\n768\\n3227\\n29.87\\n53.36\\n63.13\\n16.54\\n12.90\\n10.71\\n8.85\\n23.23\\n20.67\\n18.75\\n16.85\\n1536\\n6454\\n30.52\\n54.02\\n63.79\\n16.99\\n13.27\\n11.01\\n9.08\\n23.73\\n21.09\\n19.12\\n17.16\\nlarge-scale (1000-way) setting. We evaluate for n ‚àà1, 3, 5, 7, 9 with 9 being the maximum value for\\nn because there are 10 images per class.\\nWe observed that MRL had equal performance to FF across all representation sizes and shot numbers.\\nWe also found that for both MRL and FF, as the shot number decreased, the required representa-\\ntion size to reach optimal accuracy decreased (Table 15). For example, we observed that 1-shot\\nperformance at 32 representation size had equal accuracy to 2048 representation size.\\nFLUID.\\nFor the long-tailed setting we evaluated MRL on the FLUID benchmark [92] which\\ncontains a mixture of pretrain and new classes. Table 16 shows the evaluation of the learned\\nrepresentation on FLUID. We observed that MRL provided up to 2% higher accuracy on novel\\nclasses in the tail of the distribution, without sacrificing accuracy on other classes. Additionally we\\nfound the accuracy between low-dimensional and high-dimensional representations was marginal for\\npretrain classes. For example, the 64-dimensional MRL performed ‚àº1% lower in accuracy compared\\nto the 2048-dimensional counterpart on pretrain-head classes (84.46% vs 85.60%). However for novel-\\ntail classes the gap was far larger (6.22% vs 12.88%). We hypothesize that the higher-dimensional\\nrepresentations are required to differentiate the classes when few training examples of each are known.\\n26\\n': array([ 0.01557623,  0.00616288,  0.06246094, ..., -0.0216481 ,\n",
              "                     0.00599711,  0.01235827]),\n",
              "             'Table 11: Retrieve a shortlist of k-NN with Ds sized representations on ImageNet-1K with MRL rep-\\nresentations, and then re-order the neighbors shortlist with L2 distances using Dr sized representations.\\nTop-1 and mAP@10 entries (%) that are within 0.1% of the maximum value achievable without\\nreranking on MRL representations, as seen in Table 8, are bolded.\\nShortlist Length = 200\\nDs\\nDr\\nMFLOPs\\nTop-1\\nmAP@10\\nmAP@25\\nmAP@50\\nmAP@100\\nP@10\\nP@25\\nP@50\\nP@100\\n8\\n16\\n10\\n68.21\\n63.35\\n62.25\\n61.70\\n61.19\\n68.32\\n68.14\\n67.96\\n67.65\\n32\\n69.42\\n64.12\\n62.81\\n62.03\\n61.32\\n69.04\\n68.63\\n68.22\\n67.71\\n64\\n70.05\\n64.46\\n63.03\\n62.14\\n61.29\\n69.37\\n68.83\\n68.32\\n67.66\\n128\\n70.34\\n64.68\\n63.16\\n62.21\\n61.27\\n69.59\\n68.96\\n68.38\\n67.65\\n256\\n70.40\\n64.77\\n63.21\\n62.23\\n61.26\\n69.66\\n69.02\\n68.41\\n67.65\\n512\\n70.60\\n64.86\\n63.22\\n62.21\\n61.22\\n69.74\\n69.02\\n68.39\\n67.62\\n1024\\n70.71\\n64.88\\n63.23\\n62.20\\n61.20\\n69.76\\n69.01\\n68.39\\n67.60\\n2048\\n70.81\\n64.90\\n63.22\\n62.17\\n61.16\\n69.77\\n68.99\\n68.36\\n67.57\\n16\\n32\\n21\\n69.47\\n64.27\\n63.04\\n62.36\\n61.75\\n69.21\\n68.90\\n68.58\\n68.12\\n64\\n70.16\\n64.74\\n63.42\\n62.66\\n61.94\\n69.66\\n69.22\\n68.81\\n68.22\\n128\\n70.52\\n65.00\\n63.60\\n62.77\\n61.98\\n69.91\\n69.36\\n68.89\\n68.24\\n256\\n70.55\\n65.10\\n63.67\\n62.82\\n62.01\\n69.98\\n69.43\\n68.92\\n68.25\\n512\\n70.74\\n65.21\\n63.70\\n62.83\\n62.00\\n70.08\\n69.43\\n68.92\\n68.24\\n1024\\n70.83\\n65.23\\n63.72\\n62.83\\n61.99\\n70.08\\n69.45\\n68.92\\n68.23\\n2048\\n70.90\\n65.27\\n63.73\\n62.82\\n61.97\\n70.10\\n69.44\\n68.90\\n68.21\\n32\\n64\\n41\\n70.16\\n64.69\\n63.35\\n62.57\\n61.93\\n69.68\\n69.26\\n68.92\\n68.51\\n128\\n70.52\\n64.97\\n63.54\\n62.73\\n62.04\\n69.95\\n69.47\\n69.06\\n68.59\\n256\\n70.63\\n65.07\\n63.63\\n62.79\\n62.07\\n70.04\\n69.55\\n69.12\\n68.61\\n512\\n70.82\\n65.17\\n63.66\\n62.80\\n62.06\\n70.11\\n69.57\\n69.12\\n68.60\\n1024\\n70.89\\n65.20\\n63.68\\n62.80\\n62.04\\n70.15\\n69.59\\n69.12\\n68.59\\n2048\\n70.97\\n65.24\\n63.70\\n62.79\\n62.02\\n70.19\\n69.59\\n69.10\\n68.56\\n64\\n128\\n82\\n70.51\\n64.94\\n63.50\\n62.64\\n61.88\\n69.94\\n69.44\\n69.02\\n68.54\\n256\\n70.63\\n65.04\\n63.57\\n62.69\\n61.91\\n70.02\\n69.52\\n69.08\\n68.57\\n512\\n70.83\\n65.14\\n63.59\\n62.67\\n61.87\\n70.12\\n69.54\\n69.06\\n68.54\\n1024\\n70.89\\n65.16\\n63.59\\n62.65\\n61.85\\n70.15\\n69.54\\n69.05\\n68.52\\n2048\\n70.97\\n65.20\\n63.59\\n62.63\\n61.82\\n70.18\\n69.53\\n69.03\\n68.49\\n128\\n256\\n164\\n70.63\\n65.04\\n63.56\\n62.66\\n61.82\\n70.02\\n69.52\\n69.07\\n68.51\\n512\\n70.82\\n65.14\\n63.58\\n62.63\\n61.77\\n70.11\\n69.54\\n69.04\\n68.47\\n1024\\n70.89\\n65.16\\n63.58\\n62.60\\n61.73\\n70.14\\n69.54\\n69.02\\n68.45\\n2048\\n70.97\\n65.20\\n63.57\\n62.57\\n61.68\\n70.18\\n69.52\\n68.99\\n68.41\\n256\\n512\\n328\\n70.82\\n65.14\\n63.57\\n62.62\\n61.74\\n70.12\\n69.53\\n69.04\\n68.45\\n1024\\n70.88\\n65.16\\n63.58\\n62.60\\n61.69\\n70.14\\n69.54\\n69.01\\n68.41\\n2048\\n70.97\\n65.20\\n63.56\\n62.56\\n61.62\\n70.18\\n69.52\\n68.98\\n68.37\\n512\\n1024\\n656\\n70.90\\n65.16\\n63.58\\n62.60\\n61.68\\n70.14\\n69.54\\n69.01\\n68.41\\n2048\\n70.98\\n65.20\\n63.57\\n62.56\\n61.60\\n70.18\\n69.52\\n68.98\\n68.35\\n1024\\n2048\\n1312\\n70.97\\n65.20\\n63.57\\n62.56\\n61.60\\n70.18\\n69.52\\n68.98\\n68.35\\nThese results provide further evidence that different tasks require varying capacity based on their\\ndifficulty.\\nH\\nRobustness Experiments\\nWe evaluated the robustness of MRL models on out-of-domain datasets (ImageNetV2/R/A/Sketch)\\nand compared them to the FF baseline. Each of these datasets is described in Appendix B. The\\nresults in Table 17 demonstrate that learning Matryoshka Representations does not hurt out-of-\\ndomain generalization relative to FF models, and Matryoshka Representations in fact improve\\nthe performance on ImageNet-A. For a ALIGN‚ÄìMRL model, we examine the the robustness via\\nzero-shot retrieval on out-of-domain datasets, including ObjectNet, in Table 18.\\nI\\nIn Practice Costs\\nAll approximate NN search experiments via HNSW32 were run on an Intel Xeon 2.20GHz CPU with\\n24 cores. All exact search experiments were run with CUDA 11.0 on 2xA100-SXM4 NVIDIA GPUs\\nwith 40G RAM each.\\nMRL models.\\nAs MRL makes minimal modifications to the ResNet50 model in the final fc layer\\nvia multiple heads for representations at various scales, it has only an 8MB storage overhead when\\ncompared to a standard ResNet50 model. MRL‚ÄìE has no storage overhead as it has a shared head\\nfor logits at the final fc layer.\\nRetrieval\\nExact search has a search time complexity of O(dkN), and HNSW has a search time\\ncomplexity of O(dk log(N)), where N is the database size, d is the representation size, and k is the\\n27\\n': array([ 0.01227134,  0.01810484,  0.09282708, ...,  0.00508602,\n",
              "                    -0.00706127, -0.0057349 ]),\n",
              "             'Table 12: Retrieve a shortlist of k-NN with Ds sized representations on ImageNet-4K with MRL representations,\\nand then re-order the neighbors shortlist with L2 distances using Dr sized representations. Top-1 and mAP@10\\nentries (%) that are within 0.1% of the maximum value achievable without reranking on MRL representations,\\nas seen in Table 10, are bolded.\\nShortlist Length = 200\\nDs\\nDr\\nMFLOPs\\nTop-1\\nmAP@10\\nmAP@25\\nmAP@50\\nmAP@100\\nP@10\\nP@25\\nP@50\\nP@100\\n8\\n16\\n34\\n16.84\\n8.70\\n6.88\\n5.88\\n5.08\\n13.86\\n12.80\\n11.98\\n11.10\\n32\\n20.73\\n10.66\\n8.19\\n6.77\\n5.61\\n16.18\\n14.39\\n13.02\\n11.61\\n64\\n23.11\\n11.91\\n9.03\\n7.36\\n6.00\\n17.56\\n15.34\\n13.67\\n11.99\\n128\\n24.63\\n12.71\\n9.59\\n7.76\\n6.25\\n18.42\\n15.94\\n14.08\\n12.22\\n256\\n25.5\\n13.24\\n9.96\\n8.03\\n6.42\\n19.00\\n16.35\\n14.36\\n12.37\\n512\\n26.07\\n13.59\\n10.21\\n8.20\\n6.53\\n19.37\\n16.62\\n14.54\\n12.46\\n1024\\n26.52\\n13.85\\n10.40\\n8.34\\n6.61\\n19.65\\n16.80\\n14.68\\n12.53\\n2048\\n26.94\\n14.11\\n10.57\\n8.45\\n6.68\\n19.92\\n16.98\\n14.79\\n12.58\\n16\\n32\\n67\\n21.44\\n11.24\\n8.72\\n7.26\\n6.02\\n17.02\\n15.30\\n13.92\\n12.41\\n64\\n24.36\\n12.78\\n9.75\\n7.96\\n6.43\\n18.72\\n16.41\\n14.63\\n12.74\\n128\\n26.08\\n13.70\\n10.39\\n8.39\\n6.69\\n19.68\\n17.07\\n15.05\\n12.94\\n256\\n26.99\\n14.27\\n10.79\\n8.67\\n6.85\\n20.27\\n17.48\\n15.31\\n13.07\\n512\\n27.60\\n14.66\\n11.06\\n8.86\\n6.97\\n20.67\\n17.75\\n15.50\\n13.16\\n1024\\n28.12\\n14.94\\n11.26\\n8.99\\n7.05\\n20.96\\n17.95\\n15.62\\n13.22\\n2048\\n28.56\\n15.21\\n11.43\\n9.11\\n7.12\\n21.23\\n18.13\\n15.73\\n13.27\\n32\\n64\\n134\\n24.99\\n13.35\\n10.35\\n8.59\\n7.09\\n19.61\\n17.52\\n15.92\\n14.21\\n128\\n27.17\\n14.61\\n11.27\\n9.26\\n7.51\\n20.99\\n18.52\\n16.62\\n14.59\\n256\\n28.33\\n15.37\\n11.83\\n9.67\\n7.77\\n21.80\\n19.12\\n17.05\\n14.81\\n512\\n29.12\\n15.88\\n12.20\\n9.94\\n7.93\\n22.33\\n19.51\\n17.32\\n14.94\\n1024\\n29.78\\n16.25\\n12.47\\n10.13\\n8.05\\n22.71\\n19.79\\n17.5\\n15.03\\n2048\\n30.33\\n16.59\\n12.72\\n10.30\\n8.16\\n23.07\\n20.05\\n17.66\\n15.11\\n64\\n128\\n269\\n27.27\\n14.76\\n11.47\\n9.51\\n7.85\\n21.25\\n18.92\\n17.20\\n15.40\\n256\\n28.54\\n15.64\\n12.15\\n10.05\\n8.21\\n22.24\\n19.71\\n17.81\\n15.76\\n512\\n29.45\\n16.25\\n12.62\\n10.40\\n8.44\\n22.88\\n20.24\\n18.20\\n15.97\\n1024\\n30.19\\n16.69\\n12.96\\n10.66\\n8.60\\n23.35\\n20.61\\n18.46\\n16.10\\n2048\\n30.81\\n17.10\\n13.27\\n10.88\\n8.74\\n23.79\\n20.93\\n18.69\\n16.21\\n128\\n256\\n538\\n28.54\\n15.66\\n12.19\\n10.12\\n8.36\\n22.28\\n19.81\\n18.00\\n16.16\\n512\\n29.45\\n16.29\\n12.69\\n10.53\\n8.66\\n22.96\\n20.41\\n18.50\\n16.48\\n1024\\n30.22\\n16.76\\n13.07\\n10.83\\n8.86\\n23.47\\n20.84\\n18.83\\n16.68\\n2048\\n30.86\\n17.19\\n13.41\\n11.09\\n9.03\\n23.95\\n21.22\\n19.12\\n16.84\\n256\\n512\\n1076\\n29.45\\n16.29\\n12.70\\n10.55\\n8.71\\n22.97\\n20.42\\n18.54\\n16.66\\n1024\\n30.21\\n16.76\\n13.08\\n10.86\\n8.95\\n23.48\\n20.87\\n18.92\\n16.94\\n2048\\n30.85\\n17.20\\n13.43\\n11.14\\n9.15\\n23.97\\n21.27\\n19.26\\n17.16\\n512\\n1024\\n2152\\n30.22\\n16.76\\n13.08\\n10.86\\n8.97\\n23.48\\n20.88\\n18.93\\n17.00\\n2048\\n30.87\\n17.20\\n13.43\\n11.14\\n9.19\\n23.97\\n21.28\\n19.28\\n17.28\\n1024\\n2048\\n4303\\n30.87\\n17.20\\n13.43\\n11.15\\n9.19\\n23.97\\n21.28\\n19.28\\n17.29\\nTable 13: Retrieve a shortlist of k-NN with Ds sized representations on ImageNet-1K with MRL.\\nThis shortlist is then reranked with funnel retrieval, which uses a rerank cascade with a one-to-\\none mapping with a monotonically decreasing shortlist length as shown in the shortlist cascade.\\nTop-1 and mAP@10 entries (%) within 0.1% of the maximum achievable without reranking on\\nMRL representations, as seen in Table 8, are bolded.\\nDs\\nRerank Cascade\\nShortlist Cascade\\nMFLOPs\\nTop-1\\nTop-5\\nTop-10\\nmAP@10\\nP@10\\n8\\n16‚Üí32‚Üí64‚Üí128‚Üí2048\\n200‚Üí100‚Üí50‚Üí25‚Üí10\\n10.28\\n70.22\\n82.63\\n85.49\\n64.06\\n68.65\\n400‚Üí200‚Üí50‚Üí25‚Üí10\\n10.29\\n70.46\\n83.13\\n86.08\\n64.43\\n69.10\\n800‚Üí400‚Üí200‚Üí50‚Üí10\\n10.31\\n70.58\\n83.54\\n86.53\\n64.62\\n69.37\\n16\\n32‚Üí64‚Üí128‚Üí256‚Üí2048\\n200‚Üí100‚Üí50‚Üí25‚Üí10\\n20.54\\n70.90\\n83.96\\n86.85\\n65.19\\n69.97\\n400‚Üí200‚Üí50‚Üí25‚Üí10\\n20.56\\n70.95\\n84.05\\n87.04\\n65.18\\n70.00\\n800‚Üí400‚Üí200‚Üí50‚Üí10\\n20.61\\n70.96\\n84.18\\n87.22\\n65.14\\n70.01\\n32\\n64‚Üí128‚Üí256‚Üí512‚Üí2048\\n200‚Üí100‚Üí50‚Üí25‚Üí10\\n41.07\\n70.96\\n84.32\\n87.47\\n65.21\\n70.11\\n400‚Üí200‚Üí50‚Üí25‚Üí10\\n41.09\\n70.97\\n84.32\\n87.47\\n65.19\\n70.11\\n800‚Üí400‚Üí200‚Üí50‚Üí10\\n41.20\\n70.97\\n84.36\\n87.53\\n65.18\\n70.11\\nshortlist length. To examine real-world performance, we tabulated wall clock search time for every\\nquery in the ImageNet-1K and ImageNet-4K validation sets over all representation sizes d in Table 19\\nfor both Exact Search and HNSW32, and ablated wall clock query time over shortlist length k on the\\nImageNet-1K validation set in Table 21. The wall clock time to build the index and the index size is\\nalso shown in Table 20.\\n28\\n': array([ 0.00444263, -0.00860134,  0.0884783 , ..., -0.02200343,\n",
              "                     0.01212447, -0.00716241]),\n",
              "             'Table 14: Retrieve a shortlist of k-NN with Ds sized representations on ImageNet-4K with MRL.\\nThis shortlist is then reranked with funnel retrieval, which uses a rerank cascade with a one-to-\\none mapping with a monotonically decreasing shortlist length as shown in the shortlist cascade.\\nTop-1 and mAP@10 entries (%) within 0.15% of the maximum achievable without reranking on\\nMRL representations, as seen in Table 10, are bolded.\\nDs\\nRerank Cascade\\nShortlist Cascade\\nMFLOPs\\nTop-1\\nTop-5\\nTop-10\\nmAP@10\\nP@10\\n8\\n16‚Üí32‚Üí64‚Üí128‚Üí2048\\n200‚Üí100‚Üí50‚Üí25‚Üí10\\n33.65\\n26.20\\n46.45\\n54.12\\n12.79\\n17.85\\n400‚Üí200‚Üí50‚Üí25‚Üí10\\n33.66\\n26.55\\n47.02\\n54.72\\n13.02\\n18.15\\n800‚Üí400‚Üí200‚Üí50‚Üí10\\n33.68\\n26.83\\n47.54\\n55.35\\n13.24\\n18.44\\n16\\n32‚Üí64‚Üí128‚Üí256‚Üí2048\\n200‚Üí100‚Üí50‚Üí25‚Üí10\\n67.28\\n29.51\\n51.44\\n59.56\\n15.27\\n21.03\\n400‚Üí200‚Üí50‚Üí25‚Üí10\\n67.29\\n29.66\\n51.71\\n59.88\\n15.42\\n21.22\\n800‚Üí400‚Üí200‚Üí50‚Üí10\\n67.34\\n29.79\\n52.00\\n60.25\\n15.55\\n21.41\\n32\\n64‚Üí128‚Üí256‚Üí512‚Üí2048\\n200‚Üí100‚Üí50‚Üí25‚Üí10\\n134.54\\n30.64\\n53.52\\n62.16\\n16.45\\n22.64\\n400‚Üí200‚Üí50‚Üí25‚Üí10\\n134.56\\n30.69\\n53.65\\n62.31\\n16.51\\n22.73\\n800‚Üí400‚Üí200‚Üí50‚Üí10\\n134.66\\n30.72\\n53.78\\n62.43\\n16.55\\n22.79\\n64\\n128‚Üí256‚Üí512‚Üí1024‚Üí2048\\n200‚Üí100‚Üí50‚Üí25‚Üí10\\n269.05\\n30.81\\n54.06\\n63.15\\n16.87\\n23.34\\n400‚Üí200‚Üí50‚Üí25‚Üí10\\n269.10\\n30.84\\n54.20\\n63.31\\n16.92\\n23.42\\n800‚Üí400‚Üí200‚Üí50‚Üí10\\n269.31\\n30.87\\n54.27\\n63.42\\n16.95\\n23.46\\nTable 15: Few-shot accuracy (%) on ImageNetV2 for 1000-way classification. MRL performs equally\\nto FF across all shots and representation sizes. We also observed that accuracy saturated at a lower\\ndimension for lower shot numbers. E.g. for 1-shot, 32-dim performed comparably to 2048-dim.\\nRep. Size\\nMethod\\n1-Shot\\n3-Shot\\n5-Shot\\n7-Shot\\n9-Shot\\n8\\nFF\\n35.41\\n45.73\\n49.23\\n50.89\\n51.72\\nMRL\\n35.37\\n45.69\\n49.25\\n50.85\\n51.73\\n16\\nFF\\n40.88\\n53.96\\n57.36\\n58.72\\n59.39\\nMRL\\n40.90\\n53.94\\n57.37\\n58.65\\n59.29\\n32\\nFF\\n41.41\\n54.88\\n58.28\\n59.63\\n60.40\\nMRL\\n41.40\\n54.91\\n58.30\\n59.65\\n60.45\\n64\\nFF\\n41.25\\n54.83\\n58.29\\n59.82\\n60.61\\nMRL\\n41.28\\n54.80\\n58.32\\n59.77\\n60.69\\n128\\nFF\\n41.36\\n54.90\\n58.50\\n60.05\\n60.90\\nMRL\\n41.38\\n54.95\\n58.50\\n60.06\\n60.83\\n256\\nFF\\n41.36\\n54.90\\n58.50\\n60.05\\n60.90\\nMRL\\n41.38\\n54.95\\n58.50\\n60.06\\n60.83\\n512\\nFF\\n41.36\\n55.05\\n58.70\\n60.19\\n61.02\\nMRL\\n41.34\\n55.14\\n58.78\\n60.40\\n61.18\\n1024\\nFF\\n41.32\\n55.20\\n58.85\\n60.46\\n61.38\\nMRL\\n41.31\\n55.24\\n58.86\\n60.42\\n61.34\\n2048\\nFF\\n41.18\\n55.09\\n58.77\\n60.38\\n61.34\\nMRL\\n41.16\\n55.10\\n58.77\\n60.40\\n61.28\\nJ\\nAnalysis of Model Disagreement\\nClass Trends\\nDoes increasing representation size necessarily help improve classification per-\\nformance across all classes in ImageNet-1K? We studied this question by examining trends in\\nperformance with increasing representation size from d = 8, ...2048. For MRL models, we observed\\nthat 244 classes showed a monotonic improvement in performance with increasing d, 177 classes\\nfirst improved but then observed a slight dip (one or two misclassifications per class), 49 classes\\nshowed a decline first and then an improvement, and the remaining classes did not show a clear\\ntrend. When we repeated this experiment with independently trained FF models, we noticed that 950\\nclasses did not show a clear trend. This motivated us to leverage the disagreement as well as gradual\\nimprovement of accuracy at different representation sizes by training Matryoshka Representations.\\nFigure 12 showcases the progression of relative per-class accuracy distribution compared to the\\n29\\n': array([ 0.02116764,  0.0055891 ,  0.08359685, ...,  0.0054453 ,\n",
              "                    -0.00206276, -0.00301664]),\n",
              "             'Table 16: Accuracy (%) categories indicates whether classes were present during ImageNet pretraining\\nand head/tail indicates classes that have greater/less than 50 examples in the streaming test set. We\\nobserved that MRL performed better than the baseline on novel tail classes by ‚àº2% on average.\\nRep. Size\\nMethod\\nPretrain\\n- Head (>50)\\nNovel\\n- Head (>50)\\nPretrain\\n- Tail (<50)\\nNovel\\n- Tail (<50)\\nMean Per Class\\nAcc.\\nAcc.\\n8\\nFF\\n68.04\\n11.30\\n33.18\\n0.36\\n16.29\\n28.47\\nMRL\\n71.75\\n10.70\\n38.29\\n0.19\\n17.15\\n29.34\\nMRL‚ÄìE\\n57.40\\n6.25\\n23.14\\n0.04\\n11.78\\n22.81\\n16\\nFF\\n80.74\\n19.12\\n63.29\\n2.78\\n25.65\\n37.61\\nMRL\\n81.79\\n17.90\\n61.39\\n1.95\\n24.73\\n37.59\\nMRL‚ÄìE\\n79.08\\n9.15\\n60.33\\n0.08\\n20.45\\n30.24\\n32\\nFF\\n83.67\\n24.30\\n66.66\\n4.23\\n28.86\\n42.40\\nMRL\\n83.46\\n23.26\\n65.82\\n3.75\\n28.16\\n41.90\\nMRL‚ÄìE\\n81.42\\n10.47\\n68.01\\n0.23\\n22.31\\n32.17\\n64\\nFF\\n84.12\\n27.49\\n68.20\\n5.17\\n30.64\\n45.18\\nMRL\\n84.46\\n27.61\\n67.59\\n6.22\\n31.03\\n45.35\\nMRL‚ÄìE\\n82.57\\n13.23\\n70.18\\n0.52\\n23.83\\n34.74\\n128\\nFF\\n84.87\\n29.96\\n68.79\\n5.54\\n31.84\\n47.06\\nMRL\\n84.88\\n30.86\\n68.58\\n8.41\\n33.23\\n47.79\\nMRL‚ÄìE\\n82.76\\n18.93\\n64.46\\n2.22\\n25.75\\n39.19\\n256\\nFF\\n84.77\\n32.78\\n69.96\\n7.21\\n33.65\\n49.15\\nMRL\\n85.10\\n32.91\\n69.39\\n9.99\\n34.74\\n49.39\\nMRL‚ÄìE\\n82.96\\n22.63\\n64.55\\n3.59\\n27.64\\n41.96\\n512\\nFF\\n85.62\\n35.27\\n70.27\\n9.05\\n35.42\\n51.14\\nMRL\\n85.62\\n34.67\\n70.24\\n11.43\\n36.11\\n50.79\\nMRL‚ÄìE\\n82.86\\n25.62\\n64.34\\n4.99\\n29.22\\n44.20\\n1024\\nFF\\n86.30\\n37.49\\n71.12\\n10.92\\n37.14\\n52.88\\nMRL\\n85.64\\n35.88\\n70.02\\n12.19\\n36.80\\n51.58\\nMRL‚ÄìE\\n83.03\\n27.78\\n64.58\\n6.32\\n30.57\\n45.71\\n2048\\nFF\\n86.40\\n37.09\\n71.74\\n10.77\\n37.04\\n52.67\\nMRL\\n85.60\\n36.83\\n70.34\\n12.88\\n37.46\\n52.18\\nMRL‚ÄìE\\n83.01\\n29.99\\n65.37\\n7.60\\n31.97\\n47.16\\nTable 17: Top-1 classification accuracy (%) on out-of-domain datasets (ImageNet-V2/R/A/Sketch) to\\nexamine robustness of Matryoshka Representation Learning. Note that these results are without\\nany fine tuning on these datasets.\\nImageNet-V1\\nImageNet-V2\\nImageNet-R\\nImageNet-A\\nImageNet-Sketch\\nRep. Size\\nFF\\nMRL‚ÄìE\\nMRL\\nFF\\nMRL‚ÄìE\\nMRL\\nFF\\nMRL‚ÄìE\\nMRL\\nFF\\nMRL‚ÄìE\\nMRL\\nFF\\nMRL‚ÄìE\\nMRL\\n8\\n65.86\\n56.92\\n67.46\\n54.05\\n47.40\\n55.59\\n24.60\\n22.98\\n23.57\\n2.92\\n3.63\\n3.39\\n17.73\\n15.07\\n17.98\\n16\\n73.10\\n72.38\\n73.80\\n60.52\\n60.48\\n61.71\\n28.51\\n28.45\\n28.85\\n3.00\\n3.55\\n3.59\\n21.70\\n20.38\\n21.77\\n32\\n74.68\\n74.80\\n75.26\\n62.24\\n62.23\\n63.05\\n31.28\\n30.79\\n31.47\\n2.60\\n3.65\\n3.57\\n22.03\\n21.87\\n22.48\\n64\\n75.45\\n75.48\\n76.17\\n63.51\\n63.15\\n63.99\\n32.96\\n32.13\\n33.39\\n2.87\\n3.99\\n3.76\\n22.13\\n22.56\\n23.43\\n128\\n75.47\\n76.05\\n76.46\\n63.67\\n63.52\\n64.69\\n33.93\\n33.48\\n34.54\\n2.81\\n3.71\\n3.73\\n22.73\\n22.73\\n23.70\\n256\\n75.78\\n76.31\\n76.66\\n64.13\\n63.80\\n64.71\\n34.80\\n33.91\\n34.85\\n2.77\\n3.65\\n3.60\\n22.63\\n22.88\\n23.59\\n512\\n76.30\\n76.48\\n76.82\\n64.11\\n64.09\\n64.78\\n35.53\\n34.20\\n34.97\\n2.37\\n3.57\\n3.59\\n23.41\\n22.89\\n23.67\\n1024\\n76.74\\n76.60\\n76.93\\n64.43\\n64.20\\n64.95\\n36.06\\n34.22\\n34.99\\n2.53\\n3.56\\n3.68\\n23.44\\n22.98\\n23.72\\n2048\\n77.10\\n76.65\\n76.95\\n64.69\\n64.17\\n64.93\\n37.10\\n34.29\\n35.07\\n2.93\\n3.49\\n3.59\\n24.05\\n23.01\\n23.70\\nMatryoshka Representation Learning-2048 dimensional model. This also showed that some in-\\nstances and classes could benefit from lower-dimensional representations.\\nDiscussion of Oracle Accuracy\\nBased on our observed model disagreements for different rep-\\nresentation sizes d, we defined an optimal oracle accuracy [58] for MRL. We labeled an image as\\ncorrectly predicted if classification using any representation size was correct. The percentage of\\ntotal samples of ImageNet-1K that were firstly correctly predicted using each representation size d is\\nshown in Table 22. This defined an upper bound on the performance of MRL models, as 18.46%\\nof the ImageNet-1K validation set were incorrectly predicted ‚àÄd ‚àà{8, 16, . . . , 2048}. We show the\\noracle performance on MRL models for ImageNet-1K/V2/A/R/Sketch datasets in Table 23.\\nIn an attempt to derive an optimal routing policy to emulate oracle accuracy, we designed the\\nadaptive classification via cascading method as discussed in Appendix D.1. This led to an interesting\\n30\\n': array([ 0.04425621, -0.00421923,  0.072254  , ...,  0.01067274,\n",
              "                    -0.00571822, -0.00587916]),\n",
              "             'Table 18: Zero-shot top-1 image classification accuracy (%) of a ALIGN-MRL model on ImageNet-\\nV1/V2/R/A and ObjectNet.\\nRep. Size\\nV1\\nV2\\nA\\nR\\nObjectNet\\n12\\n30.57\\n23.98\\n14.59\\n24.24\\n25.52\\n24\\n45.64\\n37.71\\n22.75\\n46.40\\n35.89\\n48\\n53.84\\n46.16\\n28.88\\n60.71\\n42.76\\n96\\n58.31\\n51.34\\n33.21\\n70.12\\n45.20\\n192\\n60.95\\n53.56\\n36.10\\n74.41\\n48.24\\n384\\n62.06\\n54.77\\n37.95\\n76.51\\n49.10\\n768\\n62.26\\n55.15\\n37.84\\n76.73\\n49.26\\nBaseline\\n66.39\\n59.57\\n39.97\\n80.49\\n51.60\\nTable 19: Retrieval k-NN wall clock search times (s) over the entire validation (query) set of ImageNet-\\n1K and ImageNet-4K, containing 50K and 200K samples respectively.\\nRep. Size\\nImageNet-1K\\nImageNet-4K\\nExactL2\\nHNSW32\\nExactL2\\nHNSW32\\n8\\n0.60\\n0.14\\n35.70\\n1.17\\n16\\n0.57\\n0.18\\n36.16\\n1.65\\n32\\n0.60\\n0.20\\n36.77\\n1.75\\n64\\n0.66\\n0.24\\n27.88\\n2.21\\n128\\n0.86\\n0.32\\n30.10\\n4.15\\n256\\n1.29\\n0.46\\n34.97\\n3.39\\n512\\n2.17\\n0.68\\n46.97\\n4.83\\n1024\\n3.89\\n1.05\\n70.59\\n7.14\\n2048\\n7.31\\n2.05\\n117.78\\n13.43\\nTable 20: FAISS [47] index size and build times for exact k-NN search with L2 Distance metric and\\napproximate k-NN search with HNSW32 [62].\\nRep. Size\\nExact Search\\nHNSW32\\nImageNet-1K\\nImageNet-4K\\nImageNet-1K\\nImageNet-4K\\nIndex Size\\n(MB)\\nIndex Build\\nTime (s)\\nIndex Size\\n(MB)\\nIndex Build\\nTime (s)\\nIndex Size\\n(MB)\\nIndex Build\\nTime (s)\\nIndex Size\\n(MB)\\nIndex Build\\nTime (s)\\n8\\n40\\n0.04\\n131\\n0.33\\n381\\n4.87\\n1248\\n24.04\\n16\\n80\\n0.08\\n263\\n0.27\\n421\\n6.15\\n1379\\n33.31\\n32\\n160\\n0.16\\n525\\n0.52\\n501\\n6.80\\n1642\\n37.41\\n64\\n320\\n0.38\\n1051\\n1.05\\n661\\n8.31\\n2167\\n47.23\\n128\\n641\\n0.64\\n2101\\n2.10\\n981\\n11.73\\n3218\\n89.87\\n256\\n1281\\n1.27\\n4202\\n4.20\\n1622\\n17.70\\n5319\\n102.84\\n512\\n2562\\n2.52\\n8404\\n8.39\\n2903\\n27.95\\n9521\\n158.47\\n1024\\n5125\\n5.10\\n16808\\n17.20\\n5465\\n44.02\\n17925\\n236.30\\n2048\\n10249\\n10.36\\n33616\\n41.05\\n10590\\n86.15\\n34733\\n468.18\\nTable 21: Retrieval k-NN wall clock search times (s) over entire validation (query) set of ImageNet-\\n1K over various shortlist lengths k.\\nIndex\\nk = 50\\nk = 100\\nk = 200\\nk = 500\\nk = 1000\\nk = 2048\\nExact L2\\n0.4406\\n0.4605\\n0.5736\\n0.6060\\n1.2781\\n2.7047\\nHNSW32\\n0.1193\\n0.1455\\n0.1833\\n0.2145\\n0.2333\\n0.2670\\nobservation on the expected dimensionality for 76.30% top-1 classification accuracy being just\\nd ‚àº37. We leave the design and learning of a more optimal policy for future work.\\nGrad-CAM Examples\\nWe analyzed the nature of model disagreement across representation\\nsizes with MRL models with the help of Grad-CAM visualization [80]. We observed there were\\ncertain classes in ImageNet-1K such as \"tools\", \"vegetables\" and \"meat cutting knife\" which were\\noccasionally located around multiple objects and a cluttered environment. In such scenarios, we\\nobserved that smaller representation size models would often get confused due to other objects and fail\\nto extract the object of interest which generated the correct label. We also observed a different nature\\n31\\n': array([-0.00117244,  0.00579262,  0.07297276, ...,  0.00751989,\n",
              "                     0.00477119, -0.02855928]),\n",
              "             'Figure 12: Progression of relative per-class accuracy vs MRL-2048. As the dimensionality increases,\\nthe spread shrinks while the class marked (x) (Madagascar cat) loses accuracy.\\nTable 22: Percentage of ImageNet-1K validation set that is first correctly predicted using each\\nrepresentation size d. We note that 18.46% of the samples cannot be correctly predicted by any\\nrepresentation size. The remaining 81.54% constitutes the oracle accuracy.\\nRep. Size\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nAlways\\nWrong\\nCorrectly\\nPredicted\\n67.46\\n8.78\\n2.58\\n1.35\\n0.64\\n0.31\\n0.20\\n0.12\\n0.06\\n18.46\\nof disagreement arising when the models got confused within the same superclass. For example,\\nImageNet-1K has multiple \"snake\" classes, and models often confuse a snake image for an incorrect\\nspecies of snake.\\nSuperclass Performance\\nWe created a 30 superclass subset of the validation set based on wordnet\\nhierarchy (Table 24) to quantify the performance of MRL model on ImageNet-1K superclasses.\\nTable 25 quantifies the performance with different representation size.\\nK\\nAblation Studies\\nK.1\\nMRL Training Paradigm\\nMatryoshka Representations via Finetuning.\\nTo observe if nesting can be induced in models that\\nwere not explicitly trained with nesting from scratch, we loaded a pretrained FF-2048 ResNet50 model\\nand initialized a new MRL layer, as defined in Algorithm 2, Appendix C. We then unfroze different\\nlayers of the backbone to observe how much non-linearity in the form of unfrozen conv layers needed\\nto be present to enforce nesting into a pretrained FF model. A description of these layers can be found\\nin the ResNet50 architecture [29]. All models were finetuned with the FFCV pipeline, with same\\ntraining configuration as in the end-to-end training aside from changing lr = 0.1 and epochs = 10. We\\nobserved that finetuning the linear layer alone was insufficient to learn Matryoshka Representations\\nat lower dimensionalities. Adding more and more non-linear conv+ReLU layers steadily improved\\nclassification accuracy of d = 8 from 5% to 60% after finetuning, which was only 6% less than\\ntraining MRL end-to-end for 40 epochs. This difference was successively less pronounced as we\\nincreased dimensionality past d = 64, to within 1.5% for all larger dimensionalities. The full results\\nof this ablation can be seen in Table 26.\\nRelative Importance.\\nWe performed an ablation of MRL over the relative importance, cm, of\\ndifferent nesting dimensions m ‚ààM, as defined in Sec. 3. In an attempt to improve performance at\\nlower dimensionalities, we boosted the relative importance cm of training loss at lower dimensions as\\nin Eq. 1 with two models, MRL-8boost and MRL-8+16boost. The MRL-8boost model had cm‚ààM =\\n[2, 1, 1, 1, 1, 1, 1, 1, 1] and the MRL-8+16boost model had cm‚ààM = [2, 1.5, 1, 1, 1, 1, 1, 1, 1]. The\\nrelative importance list cm‚ààM had a 1-to-1 correspondence with nesting dimension set M. In\\nTable 27, we observed that MRL-8boost improves top-1 accuracy by 3% at d = 8, and also improves\\ntop-1 accuracy of all representation scales from 16 to 256 over MRL, while only hurting the\\nperformance at 512 to 2048 representation scales by a maximum of 0.1%. This suggests that the\\nrelative importance cm can be tuned/set for optimal accuracy for all m ‚ààM, but we leave this\\nextension for future work.\\n32\\n': array([ 0.04189314,  0.01598183,  0.11331876, ..., -0.01058205,\n",
              "                    -0.01099791,  0.01334592]),\n",
              "             'Table 23: Oracle classification accuracy of various evaluation datasets for ResNet50‚ÄìMRL model\\ntrained on ImageNet-1K.\\nTop-1\\nImageNetV1\\nImageNetV2\\nImageNet-A\\nImageNet-R\\nImageNet-Sketch\\nFF‚Äì2048\\n76.9\\n64.9\\n3.6\\n35.1\\n23.7\\nMRL‚ÄìOracle\\n81.5\\n70.6\\n8.7\\n39.8\\n28.9\\nTable 24: 30 Superclasses in ImageNet-1K corresponding to the performance in Table 25.\\ninsect\\nmotor vehicle\\nartiodactyl\\nvegetable\\ngame equipment\\nterrier\\nserpent\\nmachine\\nmeasuring device\\nsheepdog\\nprotective covering\\nsporting dog\\nvessel, watercraft\\nbuilding\\nlizard\\ngarment\\nhound\\nmonkey\\nhome appliance\\nwind instrument\\nvessel\\nfish\\nnourishment\\nelectronic equipment\\noscine\\nfurniture\\nwading bird\\ntool\\ncanine\\nmechanism\\nTable 25: Performance of MRL model on 31-way classification (1 extra class is for reject token) on\\nImageNet-1K superclasses.\\nRep. Size\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nMRL\\n85.57\\n88.67\\n89.48\\n89.82\\n89.97\\n90.11\\n90.18\\n90.22\\n90.21\\nMatryoshka Representations at Arbitrary Granularities.\\nTo train MRL, we used nested di-\\nmensions at logarithmic granularities M = {8, 16, . . . , 1024, 2048} as detailed in Section 3. We\\nmade this choice for two empirically-driven reasons: a) The accuracy improvement with increasing\\nrepresentation size was more logarithmic than linear (as shown by FF models in Figure 2). This indi-\\ncated that optimizing for granularities increasing in a non-logarithmic fashion would be sub-optimal\\nboth for maximum performance and expected efficiency; b) If we have m arbitrary granularities,\\nthe expected cost of the linear classifier to train MRL scales as O(L ‚àó(m2)) while logarithmic\\ngranularities result in O(L ‚àó2log(d)) space and compute costs.\\nTo demonstrate this effect, we learned Matryoshka Representations with uniform (MRL-Uniform)\\nnesting dimensions m\\n‚àà\\nM\\n=\\n{8, 212, 416, 620, 824, 1028, 1232, 1436, 1640, 1844, 2048}.\\nWe\\nevaluated\\nthis\\nmodel\\nat\\nthe\\nstandard\\n(MRL-log)\\ndimensions\\nm\\n‚àà\\nM\\n=\\n{8, 16, 32, 64, 128, 256, 512, 1024, 2048} for ease of comparison to reported numbers using 1-NN ac-\\ncuracy (%). As shown in Table 29, we observed that while performance interpolated, MRL-Uniform\\nsuffered at low dimensions as the logarithmic spacing of MRL-log resulted in tighter packing of\\ninformation in these initial dimensions. The higher nesting dimensions of MRL-Uniform did not\\nhelp in significant accuracy improvement due to accuracy saturation, which is often logarithmic in\\nrepresentation size as shown by FF models. Note that the slight improvement at dimensions higher\\nthan 512 for MRL-Uniform is due to multiple granularities around them compared to just three for\\nMRL-log, which are not useful in practice for efficiency.\\nLower Dimensionality.\\nWe experimented with training MRL with smaller nesting dimension than\\nm = 8, as shown in Table 28, with two models: MRL-4 and MRL-6. We found that using lower\\nthan 8-dimensions to train MRL, i.e. m0 ‚àà{4, 6} for MRL-4 and MRL-6 respectively, did not\\naffect the top-1 accuracy of other granularities significantly. However, granularities smaller than\\n8-dimensions had very low accuracy and were often unusable for deployment along with additional\\ntraining difficulty. We also observed a small dip in accuracy at higher dimensions which we attribute\\nto the joint loss that now also included the harder optimization of the smallest dimension. Lastly, we\\nhypothesize the dimensionality of 8 is an empirically validated design choice due to the considerable\\naccuracy it provided along with the ease of training.\\nK.2\\nRetrieval\\nAdaptive Retrieval.\\nTo examine the effect of increasing shortlist lengths on search time, we\\nperformed a reranking ablation over shortlist lengths for Ds= 16 and Dr= 2048 over ImageNet-1K in\\nTable 30, and over ImageNet-4K in Table 31. We observed that using a larger shortlist k saturated\\nImageNet-1K performance at k=200. But using larger shortlists until k = 2048, the maximum value\\n33\\n': array([ 0.02084836,  0.01186636,  0.08511393, ...,  0.00248112,\n",
              "                     0.00769163, -0.01159437]),\n",
              "             'Table 26: Top-1 classification accuracy (%) on ImageNet-1K of various ResNet50 models which\\nare finetuned on pretrained FF-2048 model. We observed that adding more non-linearities is able to\\ninduce nesting to a reasonable extent even if the model was not pretrained with nesting in mind.\\nRep. Size\\nfc\\n4.2 conv3,\\nfc\\n4.2 conv2,\\nconv3, fc\\n4.2 full,\\nfc\\nAll (MRL)\\n8\\n5.15\\n36.11\\n54.78\\n60.02\\n66.63\\n16\\n13.79\\n58.42\\n67.26\\n70.10\\n73.53\\n32\\n32.52\\n67.81\\n71.62\\n72.84\\n75.03\\n64\\n52.66\\n72.42\\n73.61\\n74.29\\n75.82\\n128\\n64.60\\n74.41\\n74.67\\n75.03\\n76.30\\n256\\n69.29\\n75.30\\n75.23\\n75.38\\n76.47\\n512\\n70.51\\n75.96\\n75.47\\n75.64\\n76.65\\n1024\\n70.19\\n76.18\\n75.70\\n75.75\\n76.76\\n2048\\n69.72\\n76.44\\n75.96\\n75.97\\n76.80\\nTable 27: An ablation over boosting training loss at lower nesting dimensions, with top-1 and top-5\\naccuracy (%). The models are described in Appendix K.1.\\nModel\\nMRL\\nMRL-8boost\\nMRL-8+16boost\\nRep. Size\\nTop-1\\nTop-5\\nTop-1\\nTop-5\\nTop-1\\nTop-5\\n8\\n66.63\\n84.66\\n69.53\\n86.19\\n69.24\\n85.96\\n16\\n73.53\\n89.52\\n73.86\\n89.44\\n73.91\\n89.55\\n32\\n75.03\\n91.31\\n75.28\\n91.21\\n75.10\\n91.14\\n64\\n75.82\\n92.27\\n75.84\\n92.22\\n75.67\\n92.06\\n128\\n76.30\\n92.82\\n76.28\\n92.74\\n76.07\\n92.52\\n256\\n76.47\\n93.02\\n76.48\\n92.97\\n76.22\\n92.72\\n512\\n76.65\\n93.13\\n76.56\\n93.09\\n76.35\\n92.85\\n1024\\n76.76\\n93.22\\n76.71\\n93.21\\n76.39\\n92.98\\n2048\\n76.80\\n93.32\\n76.76\\n93.28\\n76.52\\n93.05\\nTable 28: An ablation over training with smaller\\nnesting dimensionalities in terms of Top-1 accu-\\nracy (%). MRL-4 and MRL-6 are variations of\\nthe original model (MRL-8) with m0 ‚àà{4, 6},\\nwhere m ‚ààM is part of the nesting_list as seen\\nin Alg 2.\\nRep. Size\\nMRL-4\\nMRL-6\\nMRL-8\\n4\\n27.25\\n-\\n-\\n6\\n-\\n58.71\\n-\\n8\\n66.86\\n67.55\\n66.63\\n16\\n73.36\\n73.10\\n73.53\\n32\\n74.82\\n74.49\\n75.03\\n64\\n75.51\\n75.32\\n75.82\\n128\\n75.93\\n75.61\\n76.30\\n256\\n76.08\\n75.82\\n76.47\\n512\\n76.31\\n75.93\\n76.65\\n1024\\n76.38\\n76.04\\n76.76\\n2048\\n76.43\\n76.12\\n76.80\\nTable 29: An ablation over training MRL with\\nnesting list at uniformly distributed granulari-\\nties. Entries in the MRL-Uniform column are\\nevaluated at logarithmic dimensions for a fair\\ncomparison to MRL-Log (standard MRL) with\\n1-NN accuracy (%).\\nRep. Size\\nMRL-Log\\nMRL-Uniform\\n8\\n62.19\\n58.44\\n16\\n67.91\\n61.11\\n32\\n69.46\\n63.82\\n64\\n70.17\\n66.44\\n128\\n70.52\\n68.71\\n256\\n70.62\\n70.06\\n512\\n70.82\\n70.98\\n1024\\n70.89\\n71.37\\n2048\\n70.97\\n71.44\\nsupported by the FAISS framework, steadily improved performance on ImageNet-4K. This is likely\\ndue to the increased database size, but could also indicate a correlation with ImageNet-4K being\\nslightly out-of-distribution making the task at hand harder.\\n34\\n': array([ 0.00271811,  0.02642449,  0.09216782, ..., -0.01520871,\n",
              "                    -0.01494166, -0.01725603]),\n",
              "             'Table 30: Adaptive retrieval ablation over shortlist length k for Ds = 16, Dr = 2048 on ImageNet-\\n1K with exact search. Entries with the highest P@1 and mAP@10 across all k are in bold.\\nShortlist\\nLength\\nP@1\\nmAP@10\\nmAP@25\\nmAP@50\\nmAP@100\\nP@10\\nP@25\\nP@50\\nP@100\\n100\\n70.88\\n65.19\\n63.62\\n62.59\\n61.24\\n69.96\\n69.24\\n68.53\\n67.20\\n200\\n70.90\\n65.27\\n63.73\\n62.82\\n61.97\\n70.10\\n69.44\\n68.90\\n68.21\\n400\\n70.94\\n65.26\\n63.71\\n62.81\\n62.03\\n70.15\\n69.51\\n69.02\\n68.47\\n800\\n70.96\\n65.23\\n63.64\\n62.69\\n61.85\\n70.16\\n69.52\\n69.02\\n68.45\\n1600\\n70.96\\n65.20\\n63.58\\n62.58\\n61.66\\n70.16\\n69.5\\n68.97\\n68.36\\n2048\\n70.97\\n65.20\\n63.57\\n62.58\\n61.64\\n70.16\\n69.5\\n68.97\\n68.35\\nTable 31: Adaptive retrieval ablation over shortlist length k for Ds = 16, Dr = 2048 on ImageNet-\\n4K with exact search.\\nShortlist\\nLength\\nP@1\\nmAP@10\\nmAP@25\\nmAP@50\\nmAP@100\\nP@10\\nP@25\\nP@50\\nP@100\\n100\\n27.70\\n14.38\\n10.62\\n8.26\\n6.07\\n20.12\\n16.87\\n14.29\\n11.26\\n200\\n28.56\\n15.21\\n11.43\\n9.11\\n7.12\\n21.23\\n18.13\\n15.73\\n13.27\\n400\\n29.34\\n15.83\\n12.06\\n9.76\\n7.79\\n22.08\\n19.09\\n16.83\\n14.54\\n800\\n29.86\\n16.30\\n12.53\\n10.23\\n8.26\\n22.72\\n19.83\\n17.65\\n15.45\\n1600\\n30.24\\n16.63\\n12.86\\n10.56\\n8.60\\n23.18\\n20.36\\n18.23\\n16.11\\n2048\\n30.35\\n16.73\\n12.96\\n10.65\\n8.69\\n23.31\\n20.50\\n18.40\\n16.30\\n35\\n': array([ 0.02262504, -0.0004907 ,  0.07529048, ..., -0.00231177,\n",
              "                     0.01966901, -0.00610788])})"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehsfIiKR6yw"
      },
      "source": [
        "## Task 4: Prompts\n",
        "\n",
        "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
        "\n",
        "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
        "\n",
        "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpA0UveR6yw"
      },
      "source": [
        "### XYZRolePrompt\n",
        "\n",
        "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
        "\n",
        "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
        "\n",
        "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
        "\n",
        "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model‚Äôs behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
        "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
        "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
        "\n",
        "The main idea is this:\n",
        "\n",
        "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
        "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
        "3. Then, you prompt the model with the true \"user\" message.\n",
        "\n",
        "In this example, we'll be forgoing the 2nd step for simplicities sake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZ2KWKSR6yw"
      },
      "source": [
        "#### Utility Functions\n",
        "\n",
        "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbeJDDsR6yw"
      },
      "source": [
        "##### XYZRolePrompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mojJSE3R6yw"
      },
      "source": [
        "Here we have our `system`, `user`, and `assistant` role prompts.\n",
        "\n",
        "Let's take a peek at what they look like:\n",
        "\n",
        "```python\n",
        "class BasePrompt:\n",
        "    def __init__(self, prompt):\n",
        "        \"\"\"\n",
        "        Initializes the BasePrompt object with a prompt template.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        \"\"\"\n",
        "        self.prompt = prompt\n",
        "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
        "\n",
        "    def format_prompt(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Formats the prompt string using the keyword arguments provided.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: The formatted prompt string\n",
        "        \"\"\"\n",
        "        matches = self._pattern.findall(self.prompt)\n",
        "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
        "\n",
        "    def get_input_variables(self):\n",
        "        \"\"\"\n",
        "        Gets the list of input variable names from the prompt string.\n",
        "\n",
        "        :return: List of input variable names\n",
        "        \"\"\"\n",
        "        return self._pattern.findall(self.prompt)\n",
        "```\n",
        "\n",
        "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
        "\n",
        "```python\n",
        "class RolePrompt(BasePrompt):\n",
        "    def __init__(self, prompt, role: str):\n",
        "        \"\"\"\n",
        "        Initializes the RolePrompt object with a prompt template and a role.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
        "        \"\"\"\n",
        "        super().__init__(prompt)\n",
        "        self.role = role\n",
        "\n",
        "    def create_message(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a message dictionary with a role and a formatted message.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: Dictionary containing the role and the formatted message\n",
        "        \"\"\"\n",
        "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
        "```\n",
        "\n",
        "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
        "\n",
        "```python\n",
        "class SystemRolePrompt(RolePrompt):\n",
        "    def __init__(self, prompt: str):\n",
        "        super().__init__(prompt, \"system\")\n",
        "```\n",
        "\n",
        "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D361R6sMR6yw"
      },
      "source": [
        "##### ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJVQ2Pm8R6yw"
      },
      "source": [
        "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
        "\n",
        "Let's take a peek at how that is constructed:\n",
        "\n",
        "```python\n",
        "class ChatOpenAI:\n",
        "    def __init__(self, model_name: str = \"gpt-4o-mini\"):\n",
        "        self.model_name = model_name\n",
        "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if self.openai_api_key is None:\n",
        "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
        "\n",
        "    def run(self, messages, text_only: bool = True):\n",
        "        if not isinstance(messages, list):\n",
        "            raise ValueError(\"messages must be a list\")\n",
        "\n",
        "        openai.api_key = self.openai_api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_name, messages=messages\n",
        "        )\n",
        "\n",
        "        if text_only:\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCU7FfhIR6yw"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs? By reducing the temperature.\n",
        "\n",
        "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5wcjMLCR6yw"
      },
      "source": [
        "### Creating and Prompting OpenAI's `gpt-4o-mini`!\n",
        "\n",
        "Let's tie all these together and use it to prompt `gpt-4o-mini`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WIfpIot7R6yw"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.prompts import (\n",
        "    UserRolePrompt,\n",
        "    SystemRolePrompt,\n",
        "    AssistantRolePrompt,\n",
        ")\n",
        "\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "chat_openai = ChatOpenAI()\n",
        "user_prompt_template = \"{content}\"\n",
        "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
        "system_prompt_template = (\n",
        "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
        ")\n",
        "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
        "\n",
        "messages = [\n",
        "    system_role_prompt.create_message(expertise=\"Python\"),\n",
        "    user_role_prompt.create_message(\n",
        "        content=\"What is the best way to write a loop?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = chat_openai.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHo7lssNR6yw",
        "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The \"best\" way to write a loop in Python really depends on your specific use case. Python offers several constructs for looping, and each has its strengths. Here are the most common methods:\n",
            "\n",
            "1. **For Loops**:\n",
            "   If you know the number of iterations in advance or want to iterate over a sequence (like a list or a string), a `for` loop is often the best choice.\n",
            "\n",
            "   ```python\n",
            "   # Example: Loop over a list\n",
            "   my_list = [1, 2, 3, 4, 5]\n",
            "   for item in my_list:\n",
            "       print(item)\n",
            "   ```\n",
            "\n",
            "2. **While Loops**:\n",
            "   Use a `while` loop when the number of iterations is not known beforehand and you want to continue looping until a condition is met.\n",
            "\n",
            "   ```python\n",
            "   # Example: Loop until a condition is met\n",
            "   count = 0\n",
            "   while count < 5:\n",
            "       print(count)\n",
            "       count += 1\n",
            "   ```\n",
            "\n",
            "3. **List Comprehensions**:\n",
            "   If you're constructing a new list based on an iterable, a list comprehension can be a more compact and readable solution.\n",
            "\n",
            "   ```python\n",
            "   # Example: Create a new list with squares\n",
            "   squares = [x**2 for x in range(10)]\n",
            "   print(squares)\n",
            "   ```\n",
            "\n",
            "4. **Enumerate**:\n",
            "   When you need both the index and the value while iterating, `enumerate` can make your code cleaner.\n",
            "\n",
            "   ```python\n",
            "   # Example: Get index and value\n",
            "   my_list = ['a', 'b', 'c']\n",
            "   for index, value in enumerate(my_list):\n",
            "       print(f'Index: {index}, Value: {value}')\n",
            "   ```\n",
            "\n",
            "5. **Using `break` and `continue`**:\n",
            "   You can control the flow of your loops with `break` (to exit the loop) and `continue` (to skip the rest of the current iteration).\n",
            "\n",
            "   ```python\n",
            "   # Example: Skip even numbers\n",
            "   for number in range(10):\n",
            "       if number % 2 == 0:\n",
            "           continue  # Skip even numbers\n",
            "       print(number)\n",
            "   ```\n",
            "\n",
            "### General Tips:\n",
            "- Choose the loop structure that best fits your scenario.\n",
            "- Keep your loops readable and maintainable.\n",
            "- Avoid deep nesting of loops when possible, as it can make code harder to follow.\n",
            "\n",
            "If you have a specific problem or use case in mind, feel free to share, and I would be happy to provide a more tailored example or suggestion!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2nxxhB2R6yy"
      },
      "source": [
        "## Task 5: Retrieval Augmented Generation\n",
        "\n",
        "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
        "\n",
        "There is much you could do here, many tweaks and improvements to be made!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "D1hamzGaR6yy"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\" \\\n",
        "Use the provided context to answer the user's query.\n",
        "\n",
        "You may not answer the user's query unless there is specific context in the following text.\n",
        "\n",
        "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = SystemRolePrompt(RAG_PROMPT_TEMPLATE)\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\" \\\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User Query:\n",
        "{user_query}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "user_prompt = UserRolePrompt(USER_PROMPT_TEMPLATE)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(vector_a: np.array, vector_b: np.array) -> float:\n",
        "    return np.linalg.norm(vector_a - vector_b)\n",
        "\n",
        "\n",
        "class RetrievalAugmentedQAPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "\n",
        "    def run_pipeline(self, user_query: str) -> str:\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=4, distance_measure=euclidean_distance)\n",
        "\n",
        "        context_prompt = \"\"\n",
        "        for context in context_list:\n",
        "            context_prompt += context[0] + \"\\n\"\n",
        "\n",
        "        formatted_system_prompt = rag_prompt.create_message()\n",
        "\n",
        "        formatted_user_prompt = user_prompt.create_message(user_query=user_query, context=context_prompt)\n",
        "\n",
        "        return {\"response\" : self.llm.run([formatted_system_prompt, formatted_user_prompt]), \"context\" : context_list}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZIJI19uR6yz"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response? Thing through the answer step by step.\n",
        "\n",
        "What is that strategy called? Chain of thought\n",
        "\n",
        "> NOTE: You can look through the Week 1 Day 1 \"Prompting OpenAI Like A Developer\" material for an answer to this question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "kqbE9fZ6R6yz"
      },
      "outputs": [],
      "source": [
        "retrieval_augmented_qa_pipeline = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db,\n",
        "    llm=chat_openai\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAGhaCGOR6yz",
        "outputId": "e4fb3a1b-d2bc-4e18-ec31-dc0adf767163"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': \"I don't know.\",\n",
              " 'context': [('2\\nRelated Work\\nRepresentation Learning.\\nLarge-scale datasets like ImageNet [16, 76] and JFT [85] enabled\\nthe learning of general purpose representations for computer vision [4, 98]. These representations\\nare typically learned through supervised and un/self-supervised learning paradigms. Supervised\\npretraining [29, 51, 82] casts representation learning as a multi-class/label classification problem,\\nwhile un/self-supervised learning learns representation via proxy tasks like instance classification [97]\\nand reconstruction [31, 63]. Recent advances [12, 30] in contrastive learning [27] enabled learning\\nfrom web-scale data [21] that powers large-capacity cross-modal models [18, 46, 71, 101]. Similarly,\\nnatural language applications are built [40] on large language models [8] that are pretrained [68, 75]\\nin a un/self-supervised fashion with masked language modelling [19] or autoregressive training [70].\\nMatryoshka Representation Learning (MRL) is complementary to all these setups and can be\\nadapted with minimal overhead (Section 3). MRL equips representations with multifidelity at no\\nadditional cost which enables adaptive deployment based on the data and task (Section 4).\\nEfficient Classification and Retrieval.\\nEfficiency in classification and retrieval during inference\\ncan be studied with respect to the high yet constant deep featurization costs or the search cost which\\nscales with the size of the label space and data. Efficient neural networks address the first issue\\nthrough a variety of algorithms [25, 54] and design choices [39, 53, 87]. However, with a strong\\nfeaturizer, most of the issues with scale are due to the linear dependence on number of labels (L), size\\nof the data (N) and representation size (d), stressing RAM, disk and processor all at the same time.\\nThe sub-linear complexity dependence on number of labels has been well studied in context of\\ncompute [3, 43, 69] and memory [20] using Approximate Nearest Neighbor Search (ANNS) [62] or\\nleveraging the underlying hierarchy [17, 55]. In case of the representation size, often dimensionality\\nreduction [77, 88], hashing techniques [14, 52, 78] and feature selection [64] help in alleviating\\nselective aspects of the O(d) scaling at a cost of significant drops in accuracy. Lastly, most real-world\\nsearch systems [11, 15] are often powered by large-scale embedding based retrieval [10, 66] that\\nscales in cost with the ever increasing web-data. While categorization [89, 99] clusters similar things\\ntogether, it is imperative to be equipped with retrieval capabilities that can bring forward every\\ninstance [7]. Approximate Nearest Neighbor Search (ANNS) [42] makes it feasible with efficient\\nindexing [14] and traversal [5, 6] to present the users with the most similar documents/images from\\nthe database for a requested query. Widely adopted HNSW [62] (O(d log(N))) is as accurate as\\nexact retrieval (O(dN)) at the cost of a graph-based index overhead for RAM and disk [44].\\nMRL tackles the linear dependence on embedding size,\\nd,\\nby learning multifidelity\\nMatryoshka Representations.\\nLower-dimensional Matryoshka Representations are as accu-\\nrate as independently trained counterparts without the multiple expensive forward passes.\\nMatryoshka Representations provide an intermediate abstraction between high-dimensional vec-\\ntors and their efficient ANNS indices through the adaptive embeddings nested within the original\\nrepresentation vector (Section 4). All other aforementioned efficiency techniques are complementary\\nand can be readily applied to the learned Matryoshka Representations obtained from MRL.\\nSeveral works in efficient neural network literature [9, 93, 100] aim at packing neural networks of\\nvarying capacity within the same larger network. However, the weights for each progressively smaller\\nnetwork can be different and often require distinct forward passes to isolate the final representations.\\nThis is detrimental for adaptive inference due to the need for re-encoding the entire retrieval database\\nwith expensive sub-net forward passes of varying capacities. Several works [23, 26, 65, 59] investigate\\nthe notions of intrinsic dimensionality and redundancy of representations and objective spaces pointing\\nto minimum description length [74]. Finally, ordered representations proposed by Rippel et al. [73]\\nuse nested dropout in the context of autoencoders to learn nested representations. MRL differentiates\\nitself in formulation by optimizing only for O(log(d)) nesting dimensions instead of O(d). Despite\\nthis, MRL diffuses information to intermediate dimensions interpolating between the optimized\\nMatryoshka Representation sizes accurately (Figure 5); making web-scale feasible.\\n3\\nMatryoshka Representation Learning\\nFor d ‚ààN, consider a set M ‚äÇ[d] of representation sizes. For a datapoint x in the input do-\\nmain X, our goal is to learn a d-dimensional representation vector z ‚ààRd. For every m ‚ààM,\\n3\\n',\n",
              "   np.float64(1.3418857846301961)),\n",
              "  ('References\\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis,\\nJ. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Joze-\\nfowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man√©, R. Monga, S. Moore, D. Murray,\\nC. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke,\\nV. Vasudevan, F. Vi√©gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and\\nX. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL\\nhttps://www.tensorflow.org/. Software available from tensorflow.org.\\n[2] A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz.\\nObjectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\\nmodels. Advances in neural information processing systems, 32, 2019.\\n[3] S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks.\\nAdvances in Neural Information Processing Systems, 23, 2010.\\n[4] Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In\\nProceedings of ICML workshop on unsupervised and transfer learning, pages 17‚Äì36. JMLR\\nWorkshop and Conference Proceedings, 2012.\\n[5] J. L. Bentley. K-d trees for semidynamic point sets. In Proceedings of the sixth annual\\nsymposium on Computational geometry, pages 187‚Äì197, 1990.\\n[6] A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. In Proceedings\\nof the 23rd international conference on Machine learning, pages 97‚Äì104, 2006.\\n[7] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer\\nnetworks and ISDN systems, 30(1-7):107‚Äì117, 1998.\\n[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\\nP. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in\\nneural information processing systems, 33:1877‚Äì1901, 2020.\\n[9] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han. Once-for-all: Train one network and specialize\\nit for efficient deployment. arXiv preprint arXiv:1908.09791, 2019.\\n[10] W.-C. Chang, F. X. Yu, Y.-W. Chang, Y. Yang, and S. Kumar. Pre-training tasks for embedding-\\nbased large-scale retrieval. arXiv preprint arXiv:2002.03932, 2020.\\n[11] W.-C. Chang, D. Jiang, H.-F. Yu, C. H. Teo, J. Zhang, K. Zhong, K. Kolluri, Q. Hu,\\nN. Shandilya, V. Ievgrafov, et al. Extreme multi-label learning for semantic matching in\\nproduct search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discov-\\nery & Data Mining, pages 2643‚Äì2651, 2021.\\n[12] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\\nof visual representations. In International conference on machine learning, pages 1597‚Äì1607.\\nPMLR, 2020.\\n[13] Y. Chen, Z. Liu, H. Xu, T. Darrell, and X. Wang. Meta-baseline: exploring simple meta-\\nlearning for few-shot learning. In Proceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 9062‚Äì9071, 2021.\\n[14] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based\\non p-stable distributions. In Proceedings of the twentieth annual symposium on Computational\\ngeometry, pages 253‚Äì262, 2004.\\n[15] J. Dean. Challenges in building large-scale information retrieval systems. In Keynote of the\\n2nd ACM International Conference on Web Search and Data Mining (WSDM), volume 10,\\n2009.\\n[16] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale\\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern\\nrecognition, pages 248‚Äì255. Ieee, 2009.\\n11\\n',\n",
              "   np.float64(1.3389794323882203)),\n",
              "  ('Matryoshka Representation Learning\\nAditya Kusupati‚àó‚Ä†‚ãÑ, Gantavya Bhatt‚àó‚Ä†, Aniket Rege‚àó‚Ä†,\\nMatthew Wallingford‚Ä†, Aditya Sinha‚ãÑ, Vivek Ramanujan‚Ä†, William Howard-Snyder‚Ä†,\\nKaifeng Chen‚ãÑ, Sham Kakade‚Ä°, Prateek Jain‚ãÑand Ali Farhadi‚Ä†\\n‚Ä†University of Washington, ‚ãÑGoogle Research, ‚Ä°Harvard University\\n{kusupati,ali}@cs.washington.edu, prajain@google.com\\nAbstract\\nLearned representations are a central component in modern ML systems, serv-\\ning a multitude of downstream tasks. When training such representations, it\\nis often the case that computational and statistical constraints for each down-\\nstream task are unknown. In this context, rigid fixed-capacity representations\\ncan be either over or under-accommodating to the task at hand. This leads us\\nto ask: can we design a flexible representation that can adapt to multiple down-\\nstream tasks with varying computational resources? Our main contribution is\\nMatryoshka Representation Learning (MRL) which encodes information at\\ndifferent granularities and allows a single embedding to adapt to the computational\\nconstraints of downstream tasks. MRL minimally modifies existing representation\\nlearning pipelines and imposes no additional cost during inference and deployment.\\nMRL learns coarse-to-fine representations that are at least as accurate and rich as\\nindependently trained low-dimensional representations. The flexibility within the\\nlearned Matryoshka Representations offer: (a) up to 14√ó smaller embedding\\nsize for ImageNet-1K classification at the same level of accuracy; (b) up to 14√ó\\nreal-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up\\nto 2% accuracy improvements for long-tail few-shot classification, all while being\\nas robust as the original representations. Finally, we show that MRL extends seam-\\nlessly to web-scale datasets (ImageNet, JFT) across various modalities ‚Äì vision\\n(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\\npretrained models are open-sourced at https://github.com/RAIVNLab/MRL.\\n1\\nIntroduction\\nLearned representations [57] are fundamental building blocks of real-world ML systems [66, 91].\\nTrained once and frozen, d-dimensional representations encode rich information and can be used\\nto perform multiple downstream tasks [4]. The deployment of deep representations has two steps:\\n(1) an expensive yet constant-cost forward pass to compute the representation [29] and (2) utilization\\nof the representation for downstream applications [50, 89]. Compute costs for the latter part of the\\npipeline scale with the embedding dimensionality as well as the data size (N) and label space (L).\\nAt web-scale [15, 85] this utilization cost overshadows the feature computation cost. The rigidity in\\nthese representations forces the use of high-dimensional embedding vectors across multiple tasks\\ndespite the varying resource and accuracy constraints that require flexibility.\\nHuman perception of the natural world has a naturally coarse-to-fine granularity [28, 32]. However,\\nperhaps due to the inductive bias of gradient-based training [84], deep learning models tend to diffuse\\n‚Äúinformation‚Äù across the entire representation vector. The desired elasticity is usually enabled in the\\nexisting flat and fixed representations either through training multiple low-dimensional models [29],\\njointly optimizing sub-networks of varying capacity [9, 100] or post-hoc compression [38, 60]. Each\\nof these techniques struggle to meet the requirements for adaptive large-scale deployment either\\n‚àóEqual contribution ‚Äì AK led the project with extensive support from GB and AR for experimentation.\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\\narXiv:2205.13147v4  [cs.LG]  8 Feb 2024\\n',\n",
              "   np.float64(1.3377928040351026)),\n",
              "  ('Matryoshka Representation Learning (MRL) enables each of the first m dimensions of the em-\\nbedding vector, z1:m ‚ààRm to be independently capable of being a transferable and general purpose\\nrepresentation of the datapoint x. We obtain z using a deep neural network F( ¬∑ ; Œ∏F ): X ‚ÜíRd\\nparameterized by learnable weights Œ∏F , i.e., z := F(x; Œ∏F ). The multi-granularity is captured through\\nthe set of the chosen dimensions M, that contains less than log(d) elements, i.e., |M| ‚â§‚åälog(d)‚åã.\\nThe usual set M consists of consistent halving until the representation size hits a low information\\nbottleneck. We discuss the design choices in Section 4 for each of the representation learning settings.\\nFor the ease of exposition, we present the formulation for fully supervised representation learning\\nvia multi-class classification. Matryoshka Representation Learning modifies the typical setting\\nto become a multi-scale representation learning problem on the same task. For example, we train\\nResNet50 [29] on ImageNet-1K [76] which embeds a 224 √ó 224 pixel image into a d = 2048\\nrepresentation vector and then passed through a linear classifier to make a prediction, ÀÜ\\ny among the\\nL = 1000 labels. For MRL, we choose M = {8, 16, . . . , 1024, 2048} as the nesting dimensions.\\nSuppose we are given a labelled dataset D = {(x1, y1), . . . , (xN, yN)} where xi ‚ààX is an input\\npoint and yi ‚àà[L] is the label of xi for all i ‚àà[N]. MRL optimizes the multi-class classification loss\\nfor each of the nested dimension m ‚ààM using standard empirical risk minimization using a separate\\nlinear classifier, parameterized by W(m) ‚ààRL√óm. All the losses are aggregated after scaling with\\ntheir relative importance (cm ‚â•0)m‚ààM respectively. That is, we solve\\nmin\\n{W(m)}m‚ààM, Œ∏F\\n1\\nN\\nX\\ni‚àà[N]\\nX\\nm‚ààM\\ncm ¬∑ L\\n\\x10\\nW(m) ¬∑ F(xi; Œ∏F )1:m ; yi\\n\\x11\\n,\\n(1)\\nwhere L: RL √ó [L] ‚ÜíR+ is the multi-class softmax cross-entropy loss function. This is a standard\\noptimization problem that can be solved using sub-gradient descent methods. We set all the impor-\\ntance scales, cm = 1 for all m ‚ààM; see Section 5 for ablations. Lastly, despite only optimizing\\nfor O(log(d)) nested dimensions, MRL results in accurate representations, that interpolate, for\\ndimensions that fall between the chosen granularity of the representations (Section 4.2).\\nWe call this formulation as Matryoshka Representation Learning (MRL). A natural way to make\\nthis efficient is through weight-tying across all the linear classifiers, i.e., by defining W(m) = W1:m\\nfor a set of common weights W ‚ààRL√ód. This would reduce the memory cost due to the linear\\nclassifiers by almost half, which would be crucial in cases of extremely large output spaces [89, 99].\\nThis variant is called Efficient Matryoshka Representation Learning (MRL‚ÄìE). Refer to Alg 1\\nand Alg 2 in Appendix A for the building blocks of Matryoshka Representation Learning (MRL).\\nAdaptation to Learning Frameworks.\\nMRL can be adapted seamlessly to most representation\\nlearning frameworks at web-scale with minimal modifications (Section 4.1). For example, MRL‚Äôs\\nadaptation to masked language modelling reduces to MRL‚ÄìE due to the weight-tying between the\\ninput embedding matrix and the linear classifier. For contrastive learning, both in context of vision &\\nvision + language, MRL is applied to both the embeddings that are being contrasted with each other.\\nThe presence of normalization on the representation needs to be handled independently for each of\\nthe nesting dimension for best results (see Appendix C for more details).\\n4\\nApplications\\nIn this section, we discuss Matryoshka Representation Learning (MRL) for a diverse set of ap-\\nplications along with an extensive evaluation of the learned multifidelity representations. Further,\\nwe showcase the downstream applications of the learned Matryoshka Representations for flexible\\nlarge-scale deployment through (a) Adaptive Classification (AC) and (b) Adaptive Retrieval (AR).\\n4.1\\nRepresentation Learning\\nWe adapt Matryoshka Representation Learning (MRL) to various representation learning setups\\n(a) Supervised learning for vision: ResNet50 [29] on ImageNet-1K [76] and ViT-B/16 [22] on\\nJFT-300M [85], (b) Contrastive learning for vision + language: ALIGN model with ViT-B/16 vision\\nencoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling:\\nBERT [19] on English Wikipedia and BooksCorpus [102]. Please refer to Appendices B and C for\\ndetails regarding the model architectures, datasets and training specifics.\\n4\\n',\n",
              "   np.float64(1.334030543274944))]}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline.run_pipeline(\"What is the conclusion?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üèóÔ∏è Activity #1:\n",
        "\n",
        "Enhance your RAG application in some way! \n",
        "\n",
        "Suggestions are: \n",
        "\n",
        "- Allow it to work with PDF files\n",
        "- Implement a new distance metric\n",
        "- Add metadata support to the vector database\n",
        "\n",
        "While these are suggestions, you should feel free to make whatever augmentations you desire! \n",
        "\n",
        "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# added pdf and euclidean distance implementation inside the notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "buildyourownlangchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ce393d9afcf427d9d352259c5d32678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e6efd99f7d346e485b002fb0fa85cc7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3dfb67c39958461da6071e4c19c3fa41",
            "value": 1
          }
        },
        "3a4ba348cb004f8ab7b2b1395539c81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ea5009dd16442cb5d8a0ac468e50a8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5f00135fe1044051a50ee5e841cbb8e3",
            "value": "0.018 MB of 0.018 MB uploaded\r"
          }
        },
        "3dfb67c39958461da6071e4c19c3fa41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e6efd99f7d346e485b002fb0fa85cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a8e24025594e5e9ff3b8581c344691": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f00135fe1044051a50ee5e841cbb8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb904e05ece143c79ecc4f20de482f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a4ba348cb004f8ab7b2b1395539c81b",
              "IPY_MODEL_1ce393d9afcf427d9d352259c5d32678"
            ],
            "layout": "IPY_MODEL_56a8e24025594e5e9ff3b8581c344691"
          }
        },
        "d2ea5009dd16442cb5d8a0ac468e50a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
